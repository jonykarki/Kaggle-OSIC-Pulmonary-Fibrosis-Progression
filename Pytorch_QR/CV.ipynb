{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "if os.path.abspath(os.pardir) not in sys.path:\n",
    "    sys.path.insert(0, os.path.abspath(os.pardir))\n",
    "import CONFIG\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = CONFIG.CFG.DATA.BASE\n",
    "K_FOLDS = 5\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 3e-3\n",
    "NUM_EPOCHS = 1000\n",
    "ES_PATIENCE = 20\n",
    "QUANTILES = [0.2, 0.5, 0.8]\n",
    "SCALE_COLUMNS = ['Weeks', 'FVC', 'Percent', 'Age']\n",
    "SEX_COLUMNS = ['Male', 'Female']\n",
    "SMOKING_STATUS_COLUMNS = ['Currently smokes', 'Ex-smoker', 'Never smoked']\n",
    "FV = SEX_COLUMNS + SMOKING_STATUS_COLUMNS + SCALE_COLUMNS\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = model_selection.KFold(K_FOLDS)\n",
    "MIN_MAX_SCALER = preprocessing.MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\"))\n",
    "train_df.drop_duplicates(keep=False, inplace=True, subset=['Patient', 'Weeks'])\n",
    "test_df = pd.read_csv(os.path.join(DATA_DIR, \"test.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[SCALE_COLUMNS] = MIN_MAX_SCALER.fit_transform(train_df[SCALE_COLUMNS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert categoricals into dummies\n",
    "train_df['Sex'] = pd.Categorical(train_df['Sex'], categories=SEX_COLUMNS)\n",
    "train_df['SmokingStatus'] = pd.Categorical(train_df['SmokingStatus'], categories=SMOKING_STATUS_COLUMNS)\n",
    "train_df = train_df.join(pd.get_dummies(train_df['Sex']))\n",
    "train_df = train_df.join(pd.get_dummies(train_df['SmokingStatus']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# REMOVE THE ONES FROM THE TRAIN_DF THAT ARE PRESENT IN TEST_DF AS WELL\n",
    "TEST_PATIENTS = test_df['Patient'].unique().tolist()\n",
    "valid_df = train_df[train_df['Patient'].isin(TEST_PATIENTS)]\n",
    "train_df = train_df[~train_df['Patient'].isin(TEST_PATIENTS)]\n",
    "TRAIN_PATIENTS = train_df['Patient'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Patient</th>\n      <th>Weeks</th>\n      <th>FVC</th>\n      <th>Percent</th>\n      <th>Age</th>\n      <th>Sex</th>\n      <th>SmokingStatus</th>\n      <th>Male</th>\n      <th>Female</th>\n      <th>Currently smokes</th>\n      <th>Ex-smoker</th>\n      <th>Never smoked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1504</th>\n      <td>ID00419637202311204720264</td>\n      <td>0.079710</td>\n      <td>0.393575</td>\n      <td>0.332421</td>\n      <td>0.615385</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1505</th>\n      <td>ID00419637202311204720264</td>\n      <td>0.086957</td>\n      <td>0.364681</td>\n      <td>0.302311</td>\n      <td>0.615385</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1506</th>\n      <td>ID00419637202311204720264</td>\n      <td>0.101449</td>\n      <td>0.351041</td>\n      <td>0.288097</td>\n      <td>0.615385</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1507</th>\n      <td>ID00419637202311204720264</td>\n      <td>0.108696</td>\n      <td>0.339555</td>\n      <td>0.276128</td>\n      <td>0.615385</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1508</th>\n      <td>ID00419637202311204720264</td>\n      <td>0.130435</td>\n      <td>0.342965</td>\n      <td>0.279682</td>\n      <td>0.615385</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                        Patient     Weeks  ...  Ex-smoker  Never smoked\n1504  ID00419637202311204720264  0.079710  ...          1             0\n1505  ID00419637202311204720264  0.086957  ...          1             0\n1506  ID00419637202311204720264  0.101449  ...          1             0\n1507  ID00419637202311204720264  0.108696  ...          1             0\n1508  ID00419637202311204720264  0.130435  ...          1             0\n\n[5 rows x 12 columns]"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "valid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Patient</th>\n      <th>Weeks</th>\n      <th>FVC</th>\n      <th>Percent</th>\n      <th>Age</th>\n      <th>Sex</th>\n      <th>SmokingStatus</th>\n      <th>Male</th>\n      <th>Female</th>\n      <th>Currently smokes</th>\n      <th>Ex-smoker</th>\n      <th>Never smoked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ID00007637202177411956430</td>\n      <td>0.007246</td>\n      <td>0.267050</td>\n      <td>0.236393</td>\n      <td>0.769231</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ID00007637202177411956430</td>\n      <td>0.072464</td>\n      <td>0.248923</td>\n      <td>0.215941</td>\n      <td>0.769231</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ID00007637202177411956430</td>\n      <td>0.086957</td>\n      <td>0.221464</td>\n      <td>0.184960</td>\n      <td>0.769231</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ID00007637202177411956430</td>\n      <td>0.101449</td>\n      <td>0.236360</td>\n      <td>0.201767</td>\n      <td>0.769231</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ID00007637202177411956430</td>\n      <td>0.115942</td>\n      <td>0.222900</td>\n      <td>0.186580</td>\n      <td>0.769231</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                     Patient     Weeks  ...  Ex-smoker  Never smoked\n0  ID00007637202177411956430  0.007246  ...          1             0\n1  ID00007637202177411956430  0.072464  ...          1             0\n2  ID00007637202177411956430  0.086957  ...          1             0\n3  ID00007637202177411956430  0.101449  ...          1             0\n4  ID00007637202177411956430  0.115942  ...          1             0\n\n[5 rows x 12 columns]"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Patient_Week</th>\n      <th>FVC</th>\n      <th>Confidence</th>\n      <th>Patient</th>\n      <th>Weeks</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ID00419637202311204720264_-12</td>\n      <td>2000</td>\n      <td>100</td>\n      <td>ID00419637202311204720264</td>\n      <td>-12</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ID00421637202311550012437_-12</td>\n      <td>2000</td>\n      <td>100</td>\n      <td>ID00421637202311550012437</td>\n      <td>-12</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ID00422637202311677017371_-12</td>\n      <td>2000</td>\n      <td>100</td>\n      <td>ID00422637202311677017371</td>\n      <td>-12</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ID00423637202312137826377_-12</td>\n      <td>2000</td>\n      <td>100</td>\n      <td>ID00423637202312137826377</td>\n      <td>-12</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ID00426637202313170790466_-12</td>\n      <td>2000</td>\n      <td>100</td>\n      <td>ID00426637202313170790466</td>\n      <td>-12</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                    Patient_Week   FVC  ...                    Patient Weeks\n0  ID00419637202311204720264_-12  2000  ...  ID00419637202311204720264   -12\n1  ID00421637202311550012437_-12  2000  ...  ID00421637202311550012437   -12\n2  ID00422637202311677017371_-12  2000  ...  ID00422637202311677017371   -12\n3  ID00423637202312137826377_-12  2000  ...  ID00423637202312137826377   -12\n4  ID00426637202313170790466_-12  2000  ...  ID00426637202313170790466   -12\n\n[5 rows x 5 columns]"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "sub_df = pd.read_csv(os.path.join(DATA_DIR, \"sample_submission.csv\"))\n",
    "sub_df['Patient'] = sub_df['Patient_Week'].apply(lambda x: x.split('_')[0])\n",
    "sub_df['Weeks'] = sub_df['Patient_Week'].apply(lambda x: int(x.split('_')[-1]))\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = sub_df.drop('FVC', axis=1).merge(test_df.drop('Weeks', axis=1), on='Patient')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have to make it categorical coz sub's sex column has males only\n",
    "sub_df['Sex'] = pd.Categorical(sub_df['Sex'], categories=SEX_COLUMNS)\n",
    "sub_df['SmokingStatus'] = pd.Categorical(sub_df['SmokingStatus'], categories=SMOKING_STATUS_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = sub_df.join(pd.get_dummies(sub_df['Sex']))\n",
    "sub_df = sub_df.join(pd.get_dummies(sub_df['SmokingStatus']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Patient_Week</th>\n      <th>Confidence</th>\n      <th>Patient</th>\n      <th>Weeks</th>\n      <th>FVC</th>\n      <th>Percent</th>\n      <th>Age</th>\n      <th>Sex</th>\n      <th>SmokingStatus</th>\n      <th>Male</th>\n      <th>Female</th>\n      <th>Currently smokes</th>\n      <th>Ex-smoker</th>\n      <th>Never smoked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ID00419637202311204720264_-12</td>\n      <td>100</td>\n      <td>ID00419637202311204720264</td>\n      <td>-0.050725</td>\n      <td>0.393575</td>\n      <td>0.332421</td>\n      <td>0.615385</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ID00419637202311204720264_-11</td>\n      <td>100</td>\n      <td>ID00419637202311204720264</td>\n      <td>-0.043478</td>\n      <td>0.393575</td>\n      <td>0.332421</td>\n      <td>0.615385</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ID00419637202311204720264_-10</td>\n      <td>100</td>\n      <td>ID00419637202311204720264</td>\n      <td>-0.036232</td>\n      <td>0.393575</td>\n      <td>0.332421</td>\n      <td>0.615385</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ID00419637202311204720264_-9</td>\n      <td>100</td>\n      <td>ID00419637202311204720264</td>\n      <td>-0.028986</td>\n      <td>0.393575</td>\n      <td>0.332421</td>\n      <td>0.615385</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ID00419637202311204720264_-8</td>\n      <td>100</td>\n      <td>ID00419637202311204720264</td>\n      <td>-0.021739</td>\n      <td>0.393575</td>\n      <td>0.332421</td>\n      <td>0.615385</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                    Patient_Week  Confidence  ... Ex-smoker  Never smoked\n0  ID00419637202311204720264_-12         100  ...         1             0\n1  ID00419637202311204720264_-11         100  ...         1             0\n2  ID00419637202311204720264_-10         100  ...         1             0\n3   ID00419637202311204720264_-9         100  ...         1             0\n4   ID00419637202311204720264_-8         100  ...         1             0\n\n[5 rows x 14 columns]"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "sub_df[SCALE_COLUMNS] = MIN_MAX_SCALER.transform(sub_df[SCALE_COLUMNS])\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PulmonaryDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, FV, test=False):\n",
    "        self.df = df\n",
    "        self.test = test\n",
    "        self.FV = FV\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'features': torch.tensor(self.df[self.FV].iloc[idx].values),\n",
    "            'target': torch.tensor(self.df['FVC'].iloc[idx])\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PulmonaryModel(nn.Module):\n",
    "    def __init__(self, in_features=9, out_quantiles=3):\n",
    "        super(PulmonaryModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features, 100)\n",
    "        self.fc2 = nn.Linear(100, 100)\n",
    "        self.fc3 = nn.Linear(100, out_quantiles)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_loss(preds, target, quantiles):\n",
    "    assert not target.requires_grad\n",
    "    assert preds.size(0) == target.size(0)\n",
    "    losses = []\n",
    "    for i, q in enumerate(quantiles):\n",
    "        errors = target - preds[:, i]\n",
    "        losses.append(torch.max((q - 1) * errors, q * errors).unsqueeze(1))\n",
    "    loss = torch.mean(torch.sum(torch.cat(losses, dim=1), dim=1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_data_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_losses = AverageMeter()\n",
    "\n",
    "    for i, data in enumerate(train_data_loader):\n",
    "        features = data['features']\n",
    "        targets = data['target']\n",
    "\n",
    "        features = features.to(DEVICE).float()\n",
    "        targets = targets.to(DEVICE).float()\n",
    "\n",
    "        model.zero_grad()\n",
    "        out = model(features)\n",
    "        loss = quantile_loss(out, targets, QUANTILES)\n",
    "        train_losses.update(loss, features.size(0))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch}, Loss: {train_losses.avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_one_epoch(model, valid_data_loader, valid_loss, lr_scheduler):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(valid_data_loader):\n",
    "            features = data['features']\n",
    "            targets = data['target']\n",
    "\n",
    "            features = features.to(DEVICE).float()\n",
    "            targets = targets.to(DEVICE).float()\n",
    "            \n",
    "            out = model(features)\n",
    "            loss = quantile_loss(out, targets, QUANTILES)\n",
    "            valid_loss.update(loss, features.size(0))\n",
    "    \n",
    "    if lr_scheduler is not None:\n",
    "        lr_scheduler.step(valid_loss.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 0, Loss: 0.19555068016052246\nValidation Loss improved to 0.10594289004802704 Saving model 0\nEpoch 1, Loss: 0.10113245993852615\nValidation Loss improved to 0.0942709892988205 Saving model 0\nEpoch 2, Loss: 0.06887326389551163\nValidation Loss improved to 0.08325178921222687 Saving model 0\nEpoch 3, Loss: 0.04931456223130226\nValidation Loss improved to 0.07507394254207611 Saving model 0\nEpoch 4, Loss: 0.03676648065447807\nValidation Loss improved to 0.06776890903711319 Saving model 0\nEpoch 5, Loss: 0.03279247134923935\nValidation Loss improved to 0.0609770305454731 Saving model 0\nEpoch 6, Loss: 0.023971306160092354\nValidation Loss improved to 0.055644068866968155 Saving model 0\nEpoch 7, Loss: 0.022264370694756508\nValidation Loss improved to 0.05113296955823898 Saving model 0\nEpoch 8, Loss: 0.018007317557930946\nValidation Loss improved to 0.04783182591199875 Saving model 0\nEpoch 9, Loss: 0.016149107366800308\nValidation Loss improved to 0.04475216567516327 Saving model 0\nEpoch 10, Loss: 0.01730041764676571\nValidation Loss improved to 0.04319339990615845 Saving model 0\nEpoch 11, Loss: 0.020649489015340805\nValidation Loss improved to 0.042141031473875046 Saving model 0\nEpoch 12, Loss: 0.021715901792049408\nValidation Loss improved to 0.04092041775584221 Saving model 0\nEpoch 13, Loss: 0.017592385411262512\nValidation Loss improved to 0.038869407027959824 Saving model 0\nEpoch 14, Loss: 0.01444659661501646\nValidation Loss improved to 0.03826479613780975 Saving model 0\nEpoch 15, Loss: 0.016562731936573982\nValidation Loss improved to 0.03703099861741066 Saving model 0\nEpoch 16, Loss: 0.01791776902973652\nValidation Loss improved to 0.03580173850059509 Saving model 0\nEpoch 17, Loss: 0.013377441093325615\nValidation Loss improved to 0.03459370881319046 Saving model 0\nEpoch 18, Loss: 0.01595347560942173\nValidation Loss improved to 0.03374013304710388 Saving model 0\nEpoch 19, Loss: 0.015170993283390999\nValidation Loss improved to 0.03292248770594597 Saving model 0\nEpoch 20, Loss: 0.013435043394565582\nValidation Loss improved to 0.03207774460315704 Saving model 0\nEpoch 21, Loss: 0.018267350271344185\nValidation Loss improved to 0.03151689097285271 Saving model 0\nEpoch 22, Loss: 0.01784295216202736\nValidation Loss improved to 0.031003551557660103 Saving model 0\nEpoch 23, Loss: 0.015831993892788887\nValidation Loss improved to 0.03052382729947567 Saving model 0\nEpoch 24, Loss: 0.014837964437901974\nValidation Loss improved to 0.02996271848678589 Saving model 0\nEpoch 25, Loss: 0.015355837531387806\nValidation Loss improved to 0.02935909666121006 Saving model 0\nEpoch 26, Loss: 0.018001016229391098\nValidation Loss improved to 0.02890121378004551 Saving model 0\nEpoch 27, Loss: 0.013630414381623268\nValidation Loss improved to 0.028443748131394386 Saving model 0\nEpoch 28, Loss: 0.011363513767719269\nValidation Loss improved to 0.027893079444766045 Saving model 0\nEpoch 29, Loss: 0.013417068868875504\nValidation Loss improved to 0.02741287462413311 Saving model 0\nEpoch 30, Loss: 0.010769035667181015\nValidation Loss improved to 0.027065454050898552 Saving model 0\nEpoch 31, Loss: 0.012962181121110916\nValidation Loss improved to 0.026808669790625572 Saving model 0\nEpoch 32, Loss: 0.013626936823129654\nValidation Loss improved to 0.026540156453847885 Saving model 0\nEpoch 33, Loss: 0.01137375645339489\nValidation Loss improved to 0.02613135427236557 Saving model 0\nEpoch 34, Loss: 0.009551146067678928\nValidation Loss improved to 0.025926552712917328 Saving model 0\nEpoch 35, Loss: 0.014027581550180912\nValidation Loss improved to 0.02556307055056095 Saving model 0\nEpoch 36, Loss: 0.010167085565626621\nValidation Loss improved to 0.02532024309039116 Saving model 0\nEpoch 37, Loss: 0.012631192803382874\nValidation Loss improved to 0.025096025317907333 Saving model 0\nEpoch 38, Loss: 0.01084059663116932\nValidation Loss improved to 0.024836761876940727 Saving model 0\nEpoch 39, Loss: 0.009082620032131672\nValidation Loss improved to 0.024595201015472412 Saving model 0\nEpoch 40, Loss: 0.010591189377009869\nValidation Loss improved to 0.024459773674607277 Saving model 0\nEpoch 41, Loss: 0.012520520947873592\nValidation Loss improved to 0.024167500436306 Saving model 0\nEpoch 42, Loss: 0.00901270192116499\nValidation Loss improved to 0.0239409152418375 Saving model 0\nEpoch 43, Loss: 0.00991775095462799\nValidation Loss improved to 0.02362152561545372 Saving model 0\nEpoch 44, Loss: 0.010691476985812187\nValidation Loss improved to 0.023365234956145287 Saving model 0\nEpoch 45, Loss: 0.010186090134084225\nValidation Loss improved to 0.023221779614686966 Saving model 0\nEpoch 46, Loss: 0.013961678370833397\nValidation Loss improved to 0.023021437227725983 Saving model 0\nEpoch 47, Loss: 0.00839516706764698\nValidation Loss improved to 0.02275467850267887 Saving model 0\nEpoch 48, Loss: 0.007800133898854256\nValidation Loss improved to 0.022557172924280167 Saving model 0\nEpoch 49, Loss: 0.012511120177805424\nValidation Loss improved to 0.022338973358273506 Saving model 0\nEpoch 50, Loss: 0.013442442752420902\nValidation Loss improved to 0.0222128015011549 Saving model 0\nEpoch 51, Loss: 0.013664182275533676\nValidation Loss improved to 0.02207452990114689 Saving model 0\nEpoch 52, Loss: 0.01432359404861927\nValidation Loss improved to 0.021865077316761017 Saving model 0\nEpoch 53, Loss: 0.010523861274123192\nValidation Loss improved to 0.021696042269468307 Saving model 0\nEpoch 54, Loss: 0.00877442304044962\nValidation Loss improved to 0.02147209644317627 Saving model 0\nEpoch 55, Loss: 0.013044825755059719\nValidation Loss improved to 0.021321186795830727 Saving model 0\nEpoch 56, Loss: 0.009404830634593964\nValidation Loss improved to 0.02112521603703499 Saving model 0\nEpoch 57, Loss: 0.0087452856823802\nValidation Loss improved to 0.021015482023358345 Saving model 0\nEpoch 58, Loss: 0.008894305676221848\nValidation Loss improved to 0.020934520289301872 Saving model 0\nEpoch 59, Loss: 0.008110339753329754\nEpoch 60, Loss: 0.010605133138597012\nValidation Loss improved to 0.02088167518377304 Saving model 0\nEpoch 61, Loss: 0.011698336340487003\nValidation Loss improved to 0.020852604880928993 Saving model 0\nEpoch 62, Loss: 0.0094102518633008\nValidation Loss improved to 0.020675092935562134 Saving model 0\nEpoch 63, Loss: 0.011990268714725971\nValidation Loss improved to 0.020607965067029 Saving model 0\nEpoch 64, Loss: 0.008733026683330536\nValidation Loss improved to 0.020547114312648773 Saving model 0\nEpoch 65, Loss: 0.008134321309626102\nValidation Loss improved to 0.02038867399096489 Saving model 0\nEpoch 66, Loss: 0.010597612708806992\nValidation Loss improved to 0.020257964730262756 Saving model 0\nEpoch 67, Loss: 0.0111930463463068\nValidation Loss improved to 0.020135220140218735 Saving model 0\nEpoch 68, Loss: 0.010323201306164265\nValidation Loss improved to 0.019974736496806145 Saving model 0\nEpoch 69, Loss: 0.006575168110430241\nValidation Loss improved to 0.019814234226942062 Saving model 0\nEpoch 70, Loss: 0.013205981813371181\nEpoch 71, Loss: 0.0110919289290905\nValidation Loss improved to 0.01970839500427246 Saving model 0\nEpoch 72, Loss: 0.009571511298418045\nValidation Loss improved to 0.01961582340300083 Saving model 0\nEpoch 73, Loss: 0.00825264398008585\nValidation Loss improved to 0.01946180872619152 Saving model 0\nEpoch 74, Loss: 0.006792138796299696\nValidation Loss improved to 0.01936914399266243 Saving model 0\nEpoch 75, Loss: 0.008187472820281982\nValidation Loss improved to 0.019274072721600533 Saving model 0\nEpoch 76, Loss: 0.007671097759157419\nValidation Loss improved to 0.019194060936570168 Saving model 0\nEpoch 77, Loss: 0.007977884262800217\nValidation Loss improved to 0.019066762179136276 Saving model 0\nEpoch 78, Loss: 0.007049513049423695\nValidation Loss improved to 0.018947221338748932 Saving model 0\nEpoch 79, Loss: 0.007094637490808964\nValidation Loss improved to 0.01887202076613903 Saving model 0\nEpoch 80, Loss: 0.013068082742393017\nEpoch 81, Loss: 0.016805129125714302\nEpoch 82, Loss: 0.007673292420804501\nValidation Loss improved to 0.018809793516993523 Saving model 0\nEpoch 83, Loss: 0.00721732759848237\nValidation Loss improved to 0.018769335001707077 Saving model 0\nEpoch 84, Loss: 0.007630473468452692\nValidation Loss improved to 0.01869824156165123 Saving model 0\nEpoch 85, Loss: 0.007245949003845453\nValidation Loss improved to 0.018572984263300896 Saving model 0\nEpoch 86, Loss: 0.006366991437971592\nValidation Loss improved to 0.01845776103436947 Saving model 0\nEpoch 87, Loss: 0.006924862042069435\nValidation Loss improved to 0.018401965498924255 Saving model 0\nEpoch 88, Loss: 0.00856200885027647\nValidation Loss improved to 0.018316004425287247 Saving model 0\nEpoch 89, Loss: 0.008371470496058464\nValidation Loss improved to 0.018236052244901657 Saving model 0\nEpoch 90, Loss: 0.007426020223647356\nValidation Loss improved to 0.01817883551120758 Saving model 0\nEpoch 91, Loss: 0.007343939505517483\nValidation Loss improved to 0.018093442544341087 Saving model 0\nEpoch 92, Loss: 0.006453626789152622\nValidation Loss improved to 0.017999693751335144 Saving model 0\nEpoch 93, Loss: 0.008175002411007881\nEpoch 94, Loss: 0.01092297863215208\nEpoch 95, Loss: 0.012563844211399555\nEpoch 96, Loss: 0.009019101969897747\nValidation Loss improved to 0.017950333654880524 Saving model 0\nEpoch 97, Loss: 0.007563207298517227\nValidation Loss improved to 0.017917156219482422 Saving model 0\nEpoch 98, Loss: 0.00818399153649807\nValidation Loss improved to 0.017842477187514305 Saving model 0\nEpoch 99, Loss: 0.009627344086766243\nValidation Loss improved to 0.01780993863940239 Saving model 0\nEpoch 100, Loss: 0.010817227885127068\nValidation Loss improved to 0.017720548436045647 Saving model 0\nEpoch 101, Loss: 0.007921058684587479\nValidation Loss improved to 0.017719943076372147 Saving model 0\nEpoch 102, Loss: 0.007008459884673357\nValidation Loss improved to 0.017644310370087624 Saving model 0\nEpoch 103, Loss: 0.009017709642648697\nValidation Loss improved to 0.0175846666097641 Saving model 0\nEpoch 104, Loss: 0.007229640148580074\nValidation Loss improved to 0.017512191087007523 Saving model 0\nEpoch 105, Loss: 0.006064952816814184\nValidation Loss improved to 0.017438912764191628 Saving model 0\nEpoch 106, Loss: 0.0072053903713822365\nValidation Loss improved to 0.017360344529151917 Saving model 0\nEpoch 107, Loss: 0.006165896076709032\nValidation Loss improved to 0.017265550792217255 Saving model 0\nEpoch 108, Loss: 0.005081884562969208\nValidation Loss improved to 0.017207913100719452 Saving model 0\nEpoch 109, Loss: 0.0058431378565728664\nValidation Loss improved to 0.017121927812695503 Saving model 0\nEpoch 110, Loss: 0.008420556783676147\nValidation Loss improved to 0.017058849334716797 Saving model 0\nEpoch 111, Loss: 0.00664452463388443\nValidation Loss improved to 0.017010951414704323 Saving model 0\nEpoch 112, Loss: 0.006300427485257387\nValidation Loss improved to 0.016945960000157356 Saving model 0\nEpoch 113, Loss: 0.010429047979414463\nValidation Loss improved to 0.016945455223321915 Saving model 0\nEpoch 114, Loss: 0.009228594601154327\nValidation Loss improved to 0.016912387683987617 Saving model 0\nEpoch 115, Loss: 0.006614748854190111\nValidation Loss improved to 0.016861209645867348 Saving model 0\nEpoch 116, Loss: 0.006962621118873358\nEpoch 117, Loss: 0.007433563470840454\nValidation Loss improved to 0.016811836510896683 Saving model 0\nEpoch 118, Loss: 0.007584862876683474\nValidation Loss improved to 0.016761379316449165 Saving model 0\nEpoch 119, Loss: 0.006092964671552181\nValidation Loss improved to 0.016698826104402542 Saving model 0\nEpoch 120, Loss: 0.004598172847181559\nValidation Loss improved to 0.016627132892608643 Saving model 0\nEpoch 121, Loss: 0.0057218135334551334\nValidation Loss improved to 0.016617191955447197 Saving model 0\nEpoch 122, Loss: 0.007940443232655525\nValidation Loss improved to 0.016565142199397087 Saving model 0\nEpoch 123, Loss: 0.006199135445058346\nValidation Loss improved to 0.01650114543735981 Saving model 0\nEpoch 124, Loss: 0.004896718543022871\nValidation Loss improved to 0.016474032774567604 Saving model 0\nEpoch 125, Loss: 0.009496437385678291\nValidation Loss improved to 0.016444433480501175 Saving model 0\nEpoch 126, Loss: 0.010599666275084019\nValidation Loss improved to 0.01639779843389988 Saving model 0\nEpoch 127, Loss: 0.007539920974522829\nValidation Loss improved to 0.016361763700842857 Saving model 0\nEpoch 128, Loss: 0.006795274559408426\nValidation Loss improved to 0.016307910904288292 Saving model 0\nEpoch 129, Loss: 0.004883575718849897\nValidation Loss improved to 0.016259130090475082 Saving model 0\nEpoch 130, Loss: 0.00566681707277894\nValidation Loss improved to 0.01621513068675995 Saving model 0\nEpoch 131, Loss: 0.006362618412822485\nValidation Loss improved to 0.016155054792761803 Saving model 0\nEpoch 132, Loss: 0.007371158339083195\nValidation Loss improved to 0.016115760430693626 Saving model 0\nEpoch 133, Loss: 0.007718896958976984\nValidation Loss improved to 0.016078578308224678 Saving model 0\nEpoch 134, Loss: 0.005655996035784483\nValidation Loss improved to 0.01603572629392147 Saving model 0\nEpoch 135, Loss: 0.008227048441767693\nValidation Loss improved to 0.01601487584412098 Saving model 0\nEpoch 136, Loss: 0.007389264181256294\nValidation Loss improved to 0.01595756970345974 Saving model 0\nEpoch 137, Loss: 0.0061010089702904224\nValidation Loss improved to 0.015946635976433754 Saving model 0\nEpoch 138, Loss: 0.005767705850303173\nValidation Loss improved to 0.015921616926789284 Saving model 0\nEpoch 139, Loss: 0.005974306724965572\nValidation Loss improved to 0.015905428677797318 Saving model 0\nEpoch 140, Loss: 0.009356829337775707\nValidation Loss improved to 0.015869544818997383 Saving model 0\nEpoch 141, Loss: 0.005671151448041201\nValidation Loss improved to 0.015867212787270546 Saving model 0\nEpoch 142, Loss: 0.008711603470146656\nValidation Loss improved to 0.015821069478988647 Saving model 0\nEpoch 143, Loss: 0.007837927900254726\nValidation Loss improved to 0.015779515728354454 Saving model 0\nEpoch 144, Loss: 0.012355577200651169\nValidation Loss improved to 0.015751654282212257 Saving model 0\nEpoch 145, Loss: 0.010873345658183098\nValidation Loss improved to 0.015717657282948494 Saving model 0\nEpoch 146, Loss: 0.00597988348454237\nValidation Loss improved to 0.015655163675546646 Saving model 0\nEpoch 147, Loss: 0.005279176868498325\nValidation Loss improved to 0.015639180317521095 Saving model 0\nEpoch 148, Loss: 0.004593160934746265\nValidation Loss improved to 0.015583026222884655 Saving model 0\nEpoch 149, Loss: 0.005699866451323032\nValidation Loss improved to 0.015536315739154816 Saving model 0\nEpoch 150, Loss: 0.005768067203462124\nValidation Loss improved to 0.01550764124840498 Saving model 0\nEpoch 151, Loss: 0.010245020501315594\nEpoch 152, Loss: 0.011247326619923115\nValidation Loss improved to 0.015482881106436253 Saving model 0\nEpoch 153, Loss: 0.00746186776086688\nValidation Loss improved to 0.015458541922271252 Saving model 0\nEpoch 154, Loss: 0.009185860864818096\nValidation Loss improved to 0.015429658815264702 Saving model 0\nEpoch 155, Loss: 0.008928566239774227\nValidation Loss improved to 0.015408104285597801 Saving model 0\nEpoch 156, Loss: 0.004896507598459721\nValidation Loss improved to 0.01537504605948925 Saving model 0\nEpoch 157, Loss: 0.004708720836788416\nValidation Loss improved to 0.015344778075814247 Saving model 0\nEpoch 158, Loss: 0.005263551604002714\nValidation Loss improved to 0.015326373279094696 Saving model 0\nEpoch 159, Loss: 0.00742004718631506\nValidation Loss improved to 0.015285159461200237 Saving model 0\nEpoch 160, Loss: 0.005533789284527302\nValidation Loss improved to 0.015251674689352512 Saving model 0\nEpoch 161, Loss: 0.004294030833989382\nValidation Loss improved to 0.015214761719107628 Saving model 0\nEpoch 162, Loss: 0.005865752696990967\nValidation Loss improved to 0.01518998946994543 Saving model 0\nEpoch 163, Loss: 0.0062478347681462765\nValidation Loss improved to 0.015157170593738556 Saving model 0\nEpoch 164, Loss: 0.012645388022065163\nValidation Loss improved to 0.015149697661399841 Saving model 0\nEpoch 165, Loss: 0.009388797916471958\nEpoch 166, Loss: 0.011049207299947739\nEpoch 167, Loss: 0.007563765626400709\nValidation Loss improved to 0.015131772495806217 Saving model 0\nEpoch 168, Loss: 0.0058233174495399\nValidation Loss improved to 0.015100115910172462 Saving model 0\nEpoch 169, Loss: 0.005553037393838167\nValidation Loss improved to 0.015070909634232521 Saving model 0\nEpoch 170, Loss: 0.006779327988624573\nValidation Loss improved to 0.015033280476927757 Saving model 0\nEpoch 171, Loss: 0.006535327527672052\nValidation Loss improved to 0.014994842931628227 Saving model 0\nEpoch 172, Loss: 0.006915482692420483\nValidation Loss improved to 0.014958449639379978 Saving model 0\nEpoch 173, Loss: 0.006272450089454651\nValidation Loss improved to 0.014929980970919132 Saving model 0\nEpoch 174, Loss: 0.006442264653742313\nValidation Loss improved to 0.014898049645125866 Saving model 0\nEpoch 175, Loss: 0.005882302764803171\nValidation Loss improved to 0.01485451590269804 Saving model 0\nEpoch 176, Loss: 0.005401025991886854\nValidation Loss improved to 0.014825955033302307 Saving model 0\nEpoch 177, Loss: 0.004473571665585041\nValidation Loss improved to 0.014789762906730175 Saving model 0\nEpoch 178, Loss: 0.005560541991144419\nValidation Loss improved to 0.014773789793252945 Saving model 0\nEpoch 179, Loss: 0.005869390442967415\nValidation Loss improved to 0.014742793515324593 Saving model 0\nEpoch 180, Loss: 0.005227373447269201\nValidation Loss improved to 0.014703954569995403 Saving model 0\nEpoch 181, Loss: 0.007750667165964842\nValidation Loss improved to 0.014672414399683475 Saving model 0\nEpoch 182, Loss: 0.0058859591372311115\nValidation Loss improved to 0.014641867950558662 Saving model 0\nEpoch 183, Loss: 0.004169807769358158\nValidation Loss improved to 0.01461917906999588 Saving model 0\nEpoch 184, Loss: 0.008412417955696583\nValidation Loss improved to 0.014590089209377766 Saving model 0\nEpoch 185, Loss: 0.0065671824850142\nValidation Loss improved to 0.01455877348780632 Saving model 0\nEpoch 186, Loss: 0.006223606877028942\nValidation Loss improved to 0.014538702555000782 Saving model 0\nEpoch 187, Loss: 0.007937445305287838\nValidation Loss improved to 0.014528204686939716 Saving model 0\nEpoch 188, Loss: 0.007696210406720638\nValidation Loss improved to 0.014509972184896469 Saving model 0\nEpoch 189, Loss: 0.006964680273085833\nValidation Loss improved to 0.014501344412565231 Saving model 0\nEpoch 190, Loss: 0.011508066207170486\nValidation Loss improved to 0.014476741664111614 Saving model 0\nEpoch 191, Loss: 0.008157819509506226\nEpoch 192, Loss: 0.006586982868611813\nValidation Loss improved to 0.014466719701886177 Saving model 0\nEpoch 193, Loss: 0.006015915889292955\nValidation Loss improved to 0.01445838250219822 Saving model 0\nEpoch 194, Loss: 0.006948208436369896\nValidation Loss improved to 0.014441542327404022 Saving model 0\nEpoch 195, Loss: 0.0061210268177092075\nValidation Loss improved to 0.014417307451367378 Saving model 0\nEpoch 196, Loss: 0.005808922462165356\nValidation Loss improved to 0.014392152428627014 Saving model 0\nEpoch 197, Loss: 0.006588369142264128\nEpoch 198, Loss: 0.009551949799060822\nEpoch 199, Loss: 0.007662310265004635\nValidation Loss improved to 0.014390801079571247 Saving model 0\nEpoch 200, Loss: 0.00634492002427578\nEpoch 201, Loss: 0.00833284854888916\nValidation Loss improved to 0.014385011978447437 Saving model 0\nEpoch 202, Loss: 0.004579080268740654\nValidation Loss improved to 0.01437333133071661 Saving model 0\nEpoch 203, Loss: 0.004891273099929094\nValidation Loss improved to 0.014361959882080555 Saving model 0\nEpoch 204, Loss: 0.00524782482534647\nEpoch 205, Loss: 0.006831550970673561\nValidation Loss improved to 0.014351466670632362 Saving model 0\nEpoch 206, Loss: 0.004786994773894548\nValidation Loss improved to 0.014324693940579891 Saving model 0\nEpoch 207, Loss: 0.005918966140598059\nValidation Loss improved to 0.01430414617061615 Saving model 0\nEpoch 208, Loss: 0.003634632797911763\nValidation Loss improved to 0.014293761923909187 Saving model 0\nEpoch 209, Loss: 0.005314305890351534\nValidation Loss improved to 0.014284536242485046 Saving model 0\nEpoch 210, Loss: 0.006095235701650381\nValidation Loss improved to 0.014254731126129627 Saving model 0\nEpoch 211, Loss: 0.004220625851303339\nValidation Loss improved to 0.014227272011339664 Saving model 0\nEpoch 212, Loss: 0.004275548737496138\nValidation Loss improved to 0.014204933308064938 Saving model 0\nEpoch 213, Loss: 0.0054190438240766525\nValidation Loss improved to 0.014175882562994957 Saving model 0\nEpoch 214, Loss: 0.0053844270296394825\nValidation Loss improved to 0.014153102412819862 Saving model 0\nEpoch 215, Loss: 0.008051122538745403\nValidation Loss improved to 0.014135590754449368 Saving model 0\nEpoch 216, Loss: 0.006451764609664679\nValidation Loss improved to 0.014125041663646698 Saving model 0\nEpoch 217, Loss: 0.006301338784396648\nValidation Loss improved to 0.014102120883762836 Saving model 0\nEpoch 218, Loss: 0.0044945841655135155\nValidation Loss improved to 0.014074536971747875 Saving model 0\nEpoch 219, Loss: 0.005545635242015123\nValidation Loss improved to 0.014065011404454708 Saving model 0\nEpoch 220, Loss: 0.007176529616117477\nValidation Loss improved to 0.01404623407870531 Saving model 0\nEpoch 221, Loss: 0.005539041943848133\nValidation Loss improved to 0.01403286773711443 Saving model 0\nEpoch 222, Loss: 0.00674422737210989\nValidation Loss improved to 0.014005103148519993 Saving model 0\nEpoch 223, Loss: 0.003995219711214304\nValidation Loss improved to 0.013985633850097656 Saving model 0\nEpoch 224, Loss: 0.005382191389799118\nValidation Loss improved to 0.013962788507342339 Saving model 0\nEpoch 225, Loss: 0.005235807038843632\nValidation Loss improved to 0.013936581090092659 Saving model 0\nEpoch 226, Loss: 0.005505200009793043\nValidation Loss improved to 0.013921631500124931 Saving model 0\nEpoch 227, Loss: 0.0048094578087329865\nValidation Loss improved to 0.01389198750257492 Saving model 0\nEpoch 228, Loss: 0.003618634073063731\nValidation Loss improved to 0.013861135579645634 Saving model 0\nEpoch 229, Loss: 0.005376268643885851\nValidation Loss improved to 0.013833392411470413 Saving model 0\nEpoch 230, Loss: 0.006658984813839197\nValidation Loss improved to 0.01382344402372837 Saving model 0\nEpoch 231, Loss: 0.007823722437024117\nValidation Loss improved to 0.013821907341480255 Saving model 0\nEpoch 232, Loss: 0.008454537019133568\nValidation Loss improved to 0.013817723840475082 Saving model 0\nEpoch 233, Loss: 0.00538277393206954\nValidation Loss improved to 0.013817262835800648 Saving model 0\nEpoch 234, Loss: 0.005852943751960993\nValidation Loss improved to 0.013798987492918968 Saving model 0\nEpoch 235, Loss: 0.005086622666567564\nValidation Loss improved to 0.013795938342809677 Saving model 0\nEpoch 236, Loss: 0.00852169282734394\nValidation Loss improved to 0.01378665305674076 Saving model 0\nEpoch 237, Loss: 0.004678705707192421\nValidation Loss improved to 0.013760730624198914 Saving model 0\nEpoch 238, Loss: 0.007143959868699312\nValidation Loss improved to 0.01375102810561657 Saving model 0\nEpoch 239, Loss: 0.008140885271131992\nEpoch 240, Loss: 0.008624370209872723\nEpoch 241, Loss: 0.00883353129029274\nValidation Loss improved to 0.013731791637837887 Saving model 0\nEpoch 242, Loss: 0.013523227535188198\nValidation Loss improved to 0.013714712113142014 Saving model 0\nEpoch 243, Loss: 0.008253493346273899\nValidation Loss improved to 0.013703561387956142 Saving model 0\nEpoch 244, Loss: 0.007586869411170483\nValidation Loss improved to 0.013690894469618797 Saving model 0\nEpoch 245, Loss: 0.007067074067890644\nEpoch 246, Loss: 0.006930264178663492\nValidation Loss improved to 0.013687235303223133 Saving model 0\nEpoch 247, Loss: 0.006105124019086361\nValidation Loss improved to 0.013682296499609947 Saving model 0\nEpoch 248, Loss: 0.004876710940152407\nValidation Loss improved to 0.01367171946913004 Saving model 0\nEpoch 249, Loss: 0.005124517250806093\nValidation Loss improved to 0.013653463684022427 Saving model 0\nEpoch 250, Loss: 0.005124312359839678\nValidation Loss improved to 0.013637533411383629 Saving model 0\nEpoch 251, Loss: 0.00568263279274106\nValidation Loss improved to 0.013635722920298576 Saving model 0\nEpoch 252, Loss: 0.004443997982889414\nValidation Loss improved to 0.013628540560603142 Saving model 0\nEpoch 253, Loss: 0.005163730122148991\nEpoch 254, Loss: 0.009186574257910252\nEpoch 255, Loss: 0.00538440328091383\nValidation Loss improved to 0.013617441989481449 Saving model 0\nEpoch 256, Loss: 0.005122667644172907\nEpoch 257, Loss: 0.00782051868736744\nValidation Loss improved to 0.013606997206807137 Saving model 0\nEpoch 258, Loss: 0.004449557512998581\nValidation Loss improved to 0.013576741330325603 Saving model 0\nEpoch 259, Loss: 0.006647799164056778\nEpoch 260, Loss: 0.00641526049003005\nValidation Loss improved to 0.01355968602001667 Saving model 0\nEpoch 261, Loss: 0.007163805887103081\nValidation Loss improved to 0.01355168130248785 Saving model 0\nEpoch 262, Loss: 0.003814258147031069\nValidation Loss improved to 0.013531454838812351 Saving model 0\nEpoch 263, Loss: 0.0050749098882079124\nValidation Loss improved to 0.01351728942245245 Saving model 0\nEpoch 264, Loss: 0.007649409584701061\nValidation Loss improved to 0.01350587047636509 Saving model 0\nEpoch 265, Loss: 0.00760785723105073\nValidation Loss improved to 0.013490023091435432 Saving model 0\nEpoch 266, Loss: 0.005029432475566864\nValidation Loss improved to 0.013483504764735699 Saving model 0\nEpoch 267, Loss: 0.004217455163598061\nValidation Loss improved to 0.0134597048163414 Saving model 0\nEpoch 268, Loss: 0.003774397075176239\nValidation Loss improved to 0.013447577133774757 Saving model 0\nEpoch 269, Loss: 0.004887246061116457\nValidation Loss improved to 0.01342872902750969 Saving model 0\nEpoch 270, Loss: 0.006192370317876339\nValidation Loss improved to 0.013409328646957874 Saving model 0\nEpoch 271, Loss: 0.006324738264083862\nValidation Loss improved to 0.013401087373495102 Saving model 0\nEpoch 272, Loss: 0.004186564590781927\nValidation Loss improved to 0.0133872851729393 Saving model 0\nEpoch 273, Loss: 0.005462616682052612\nValidation Loss improved to 0.013372707180678844 Saving model 0\nEpoch 274, Loss: 0.004221506416797638\nValidation Loss improved to 0.013357345946133137 Saving model 0\nEpoch 275, Loss: 0.00512199942022562\nEpoch 276, Loss: 0.00611074548214674\nValidation Loss improved to 0.01334039494395256 Saving model 0\nEpoch 277, Loss: 0.009069245308637619\nEpoch 278, Loss: 0.007010378409177065\nValidation Loss improved to 0.013327404856681824 Saving model 0\nEpoch 279, Loss: 0.004380915313959122\nValidation Loss improved to 0.013317379169166088 Saving model 0\nEpoch 280, Loss: 0.004004825372248888\nValidation Loss improved to 0.013302098959684372 Saving model 0\nEpoch 281, Loss: 0.0056017315946519375\nValidation Loss improved to 0.013289463706314564 Saving model 0\nEpoch 282, Loss: 0.00510259298607707\nValidation Loss improved to 0.013282125815749168 Saving model 0\nEpoch 283, Loss: 0.007005998399108648\nValidation Loss improved to 0.013271583244204521 Saving model 0\nEpoch 284, Loss: 0.00757752638310194\nEpoch 285, Loss: 0.005389423109591007\nValidation Loss improved to 0.013259322382509708 Saving model 0\nEpoch 286, Loss: 0.0052479105070233345\nValidation Loss improved to 0.013256176374852657 Saving model 0\nEpoch 287, Loss: 0.0058637079782783985\nValidation Loss improved to 0.013243358582258224 Saving model 0\nEpoch 288, Loss: 0.004055241122841835\nValidation Loss improved to 0.013228127732872963 Saving model 0\nEpoch 289, Loss: 0.0038956899661570787\nEpoch 290, Loss: 0.006328917574137449\nEpoch 291, Loss: 0.006439084652811289\nEpoch 292, Loss: 0.006461790297180414\nValidation Loss improved to 0.013212104327976704 Saving model 0\nEpoch 293, Loss: 0.0062834760174155235\nValidation Loss improved to 0.013201076537370682 Saving model 0\nEpoch 294, Loss: 0.005537467077374458\nValidation Loss improved to 0.013200760819017887 Saving model 0\nEpoch 295, Loss: 0.007965240627527237\nValidation Loss improved to 0.01319111231714487 Saving model 0\nEpoch 296, Loss: 0.006656622048467398\nValidation Loss improved to 0.013176397420465946 Saving model 0\nEpoch 297, Loss: 0.004736687056720257\nValidation Loss improved to 0.013165689073503017 Saving model 0\nEpoch 298, Loss: 0.005014367867261171\nValidation Loss improved to 0.013157644309103489 Saving model 0\nEpoch 299, Loss: 0.005756790284067392\nValidation Loss improved to 0.013143826276063919 Saving model 0\nEpoch 300, Loss: 0.006208482198417187\nValidation Loss improved to 0.013139933347702026 Saving model 0\nEpoch 301, Loss: 0.007299588527530432\nValidation Loss improved to 0.013124438934028149 Saving model 0\nEpoch 302, Loss: 0.006994260940700769\nEpoch 303, Loss: 0.006210532505065203\nValidation Loss improved to 0.013120441697537899 Saving model 0\nEpoch 304, Loss: 0.005430163349956274\nValidation Loss improved to 0.013108580373227596 Saving model 0\nEpoch 305, Loss: 0.007049177307635546\nValidation Loss improved to 0.013102022930979729 Saving model 0\nEpoch 306, Loss: 0.006630989257246256\nEpoch 307, Loss: 0.013789957389235497\nEpoch 308, Loss: 0.009769254364073277\nEpoch 309, Loss: 0.006257043685764074\nEpoch 310, Loss: 0.006105651147663593\nEpoch 311, Loss: 0.005101403687149286\nEpoch 312, Loss: 0.005845279898494482\nEpoch 313, Loss: 0.007599394302815199\nValidation Loss improved to 0.013092081062495708 Saving model 0\nEpoch 314, Loss: 0.0051086656749248505\nEpoch 315, Loss: 0.006117768120020628\nValidation Loss improved to 0.013079635798931122 Saving model 0\nEpoch 316, Loss: 0.0070150261744856834\nEpoch 317, Loss: 0.006664587184786797\nValidation Loss improved to 0.013075977563858032 Saving model 0\nEpoch 318, Loss: 0.0035707568749785423\nEpoch 319, Loss: 0.003909214865416288\nValidation Loss improved to 0.013066509738564491 Saving model 0\nEpoch 320, Loss: 0.0031092707067728043\nValidation Loss improved to 0.013053876347839832 Saving model 0\nEpoch 321, Loss: 0.004312427714467049\nValidation Loss improved to 0.013050186447799206 Saving model 0\nEpoch 322, Loss: 0.004277403000742197\nValidation Loss improved to 0.013036997057497501 Saving model 0\nEpoch 323, Loss: 0.0060118986293673515\nValidation Loss improved to 0.013033637776970863 Saving model 0\nEpoch 324, Loss: 0.00331651559099555\nValidation Loss improved to 0.013023501262068748 Saving model 0\nEpoch 325, Loss: 0.0036597440484911203\nValidation Loss improved to 0.013017125427722931 Saving model 0\nEpoch 326, Loss: 0.003453996730968356\nValidation Loss improved to 0.013004519045352936 Saving model 0\nEpoch 327, Loss: 0.0033219612669199705\nValidation Loss improved to 0.012986491434276104 Saving model 0\nEpoch 328, Loss: 0.004481268115341663\nValidation Loss improved to 0.012972419150173664 Saving model 0\nEpoch 329, Loss: 0.005187137983739376\nValidation Loss improved to 0.012965233065187931 Saving model 0\nEpoch 330, Loss: 0.006306357681751251\nValidation Loss improved to 0.012963014654815197 Saving model 0\nEpoch 331, Loss: 0.005566026084125042\nValidation Loss improved to 0.012958952225744724 Saving model 0\nEpoch 332, Loss: 0.006006041541695595\nEpoch 333, Loss: 0.004187708720564842\nEpoch 334, Loss: 0.00599972577765584\nValidation Loss improved to 0.012958148494362831 Saving model 0\nEpoch 335, Loss: 0.004111689515411854\nValidation Loss improved to 0.012951628305017948 Saving model 0\nEpoch 336, Loss: 0.0036035163793712854\nEpoch 337, Loss: 0.005053482484072447\nEpoch 338, Loss: 0.005688019096851349\nEpoch 339, Loss: 0.009524605236947536\nEpoch 340, Loss: 0.004400865640491247\nEpoch 341, Loss: 0.0028047312516719103\nEpoch 342, Loss: 0.004694252274930477\nEpoch 343, Loss: 0.003529868321493268\nEpoch 344, Loss: 0.003241592552512884\nValidation Loss improved to 0.01294998824596405 Saving model 0\nEpoch 345, Loss: 0.00363218504935503\nValidation Loss improved to 0.012941454537212849 Saving model 0\nEpoch 346, Loss: 0.0039258720353245735\nValidation Loss improved to 0.012930687516927719 Saving model 0\nEpoch 347, Loss: 0.0048827119171619415\nEpoch 348, Loss: 0.005641903728246689\nEpoch 349, Loss: 0.009908408857882023\nEpoch 350, Loss: 0.004097409080713987\nEpoch 351, Loss: 0.003727730130776763\nEpoch 352, Loss: 0.004579593893140554\nEpoch 353, Loss: 0.0059199524112045765\nEpoch 354, Loss: 0.00836270209401846\nEpoch 355, Loss: 0.00714963348582387\nEpoch 356, Loss: 0.004946350120007992\nEpoch 357, Loss: 0.004642891231924295\nValidation Loss improved to 0.012928798794746399 Saving model 0\nEpoch 358, Loss: 0.0032372986897826195\nValidation Loss improved to 0.01291960570961237 Saving model 0\nEpoch 359, Loss: 0.007219790015369654\nValidation Loss improved to 0.012913125567138195 Saving model 0\nEpoch 360, Loss: 0.0063047753646969795\nValidation Loss improved to 0.012909548357129097 Saving model 0\nEpoch 361, Loss: 0.00529429130256176\nEpoch 362, Loss: 0.004478975664824247\nEpoch 363, Loss: 0.004825660027563572\nValidation Loss improved to 0.012903645634651184 Saving model 0\nEpoch 364, Loss: 0.006021692883223295\nValidation Loss improved to 0.01289918553084135 Saving model 0\nEpoch 365, Loss: 0.006388568319380283\nValidation Loss improved to 0.012896474450826645 Saving model 0\nEpoch 366, Loss: 0.005445604678243399\nValidation Loss improved to 0.012883346527814865 Saving model 0\nEpoch 367, Loss: 0.0035158086102455854\nEpoch 368, Loss: 0.0051350463181734085\nValidation Loss improved to 0.012880866415798664 Saving model 0\nEpoch 369, Loss: 0.004676756449043751\nValidation Loss improved to 0.012878102250397205 Saving model 0\nEpoch 370, Loss: 0.0037469903472810984\nValidation Loss improved to 0.012873128987848759 Saving model 0\nEpoch 371, Loss: 0.005027253180742264\nValidation Loss improved to 0.012867624871432781 Saving model 0\nEpoch 372, Loss: 0.006061222869902849\nEpoch 373, Loss: 0.007306952495127916\nEpoch 374, Loss: 0.013081986457109451\nEpoch 375, Loss: 0.009661736898124218\nEpoch 376, Loss: 0.007684519048780203\nEpoch 377, Loss: 0.006959541700780392\nEpoch 378, Loss: 0.00390982162207365\nEpoch 379, Loss: 0.007220557425171137\nEpoch 380, Loss: 0.005876001436263323\nEpoch 381, Loss: 0.004127655178308487\nEpoch 382, Loss: 0.0033097181003540754\nEpoch   383: reducing learning rate of group 0 to 1.5000e-04.\nEpoch 383, Loss: 0.0022193582262843847\nEpoch 384, Loss: 0.0016420594183728099\nEpoch 385, Loss: 0.0013907253742218018\nEpoch 386, Loss: 0.0012549733510240912\nEpoch 387, Loss: 0.0015270323492586613\nValidation Loss improved to 0.012866943143308163 Saving model 0\nEpoch 388, Loss: 0.0014285012148320675\nValidation Loss improved to 0.012859095819294453 Saving model 0\nEpoch 389, Loss: 0.0012353782076388597\nValidation Loss improved to 0.012851515784859657 Saving model 0\nEpoch 390, Loss: 0.0012079356238245964\nValidation Loss improved to 0.012843762524425983 Saving model 0\nEpoch 391, Loss: 0.0011057046940550208\nValidation Loss improved to 0.012835985980927944 Saving model 0\nEpoch 392, Loss: 0.0010809998493641615\nValidation Loss improved to 0.012828554026782513 Saving model 0\nEpoch 393, Loss: 0.001169710187241435\nValidation Loss improved to 0.012820644304156303 Saving model 0\nEpoch 394, Loss: 0.0011813284363597631\nValidation Loss improved to 0.012813356705009937 Saving model 0\nEpoch 395, Loss: 0.0010436378652229905\nValidation Loss improved to 0.012806239537894726 Saving model 0\nEpoch 396, Loss: 0.0010070588905364275\nValidation Loss improved to 0.01279878243803978 Saving model 0\nEpoch 397, Loss: 0.0010295864194631577\nValidation Loss improved to 0.012791079469025135 Saving model 0\nEpoch 398, Loss: 0.0009830498602241278\nValidation Loss improved to 0.012784036807715893 Saving model 0\nEpoch 399, Loss: 0.0012297125067561865\nValidation Loss improved to 0.012775602750480175 Saving model 0\nEpoch 400, Loss: 0.0012170798145234585\nValidation Loss improved to 0.012767908163368702 Saving model 0\nEpoch 401, Loss: 0.0010524226818233728\nValidation Loss improved to 0.012760238721966743 Saving model 0\nEpoch 402, Loss: 0.0009154760628007352\nValidation Loss improved to 0.012752451002597809 Saving model 0\nEpoch 403, Loss: 0.0009082749020308256\nValidation Loss improved to 0.012744867242872715 Saving model 0\nEpoch 404, Loss: 0.0009037380805239081\nValidation Loss improved to 0.012737088836729527 Saving model 0\nEpoch 405, Loss: 0.0009890416404232383\nValidation Loss improved to 0.012729229405522346 Saving model 0\nEpoch 406, Loss: 0.0009561243350617588\nValidation Loss improved to 0.012721537612378597 Saving model 0\nEpoch 407, Loss: 0.0009313033078797162\nValidation Loss improved to 0.01271351333707571 Saving model 0\nEpoch 408, Loss: 0.0008652232936583459\nValidation Loss improved to 0.012705517001450062 Saving model 0\nEpoch 409, Loss: 0.0008190162479877472\nValidation Loss improved to 0.012697882950305939 Saving model 0\nEpoch 410, Loss: 0.0009016388212330639\nValidation Loss improved to 0.012690444476902485 Saving model 0\nEpoch 411, Loss: 0.000925083237234503\nValidation Loss improved to 0.01268264651298523 Saving model 0\nEpoch 412, Loss: 0.0009632579749450088\nValidation Loss improved to 0.012675183825194836 Saving model 0\nEpoch 413, Loss: 0.0009045980405062437\nValidation Loss improved to 0.012667112983763218 Saving model 0\nEpoch 414, Loss: 0.0008520772680640221\nValidation Loss improved to 0.01265947986394167 Saving model 0\nEpoch 415, Loss: 0.0008031835313886404\nValidation Loss improved to 0.012651417404413223 Saving model 0\nEpoch 416, Loss: 0.000936092808842659\nValidation Loss improved to 0.012644178234040737 Saving model 0\nEpoch 417, Loss: 0.0007901638746261597\nValidation Loss improved to 0.012636144645512104 Saving model 0\nEpoch 418, Loss: 0.0008707513334229589\nValidation Loss improved to 0.012628793716430664 Saving model 0\nEpoch 419, Loss: 0.0008065323927439749\nValidation Loss improved to 0.012621080502867699 Saving model 0\nEpoch 420, Loss: 0.0008494534413330257\nValidation Loss improved to 0.012613166123628616 Saving model 0\nEpoch 421, Loss: 0.0007760482258163393\nValidation Loss improved to 0.012605651281774044 Saving model 0\nEpoch 422, Loss: 0.0008288165554404259\nValidation Loss improved to 0.012597703374922276 Saving model 0\nEpoch 423, Loss: 0.0008215986308641732\nValidation Loss improved to 0.012590144760906696 Saving model 0\nEpoch 424, Loss: 0.0007929775165393949\nValidation Loss improved to 0.012582090683281422 Saving model 0\nEpoch 425, Loss: 0.0008096155943349004\nValidation Loss improved to 0.012574260123074055 Saving model 0\nEpoch 426, Loss: 0.0008128525223582983\nValidation Loss improved to 0.012566957622766495 Saving model 0\nEpoch 427, Loss: 0.0007388431113213301\nValidation Loss improved to 0.012559705413877964 Saving model 0\nEpoch 428, Loss: 0.0007946168188937008\nValidation Loss improved to 0.0125524140894413 Saving model 0\nEpoch 429, Loss: 0.0007512597949244082\nValidation Loss improved to 0.012544672004878521 Saving model 0\nEpoch 430, Loss: 0.000812243262771517\nValidation Loss improved to 0.012536793947219849 Saving model 0\nEpoch 431, Loss: 0.0007422956405207515\nValidation Loss improved to 0.012529018335044384 Saving model 0\nEpoch 432, Loss: 0.0008703319472260773\nValidation Loss improved to 0.01252119243144989 Saving model 0\nEpoch 433, Loss: 0.0009497863356955349\nValidation Loss improved to 0.012514397501945496 Saving model 0\nEpoch 434, Loss: 0.00093607057351619\nValidation Loss improved to 0.012506543658673763 Saving model 0\nEpoch 435, Loss: 0.000716567796189338\nValidation Loss improved to 0.012498889118432999 Saving model 0\nEpoch 436, Loss: 0.0007033928413875401\nValidation Loss improved to 0.012491934932768345 Saving model 0\nEpoch 437, Loss: 0.0007847927627153695\nValidation Loss improved to 0.012484438717365265 Saving model 0\nEpoch 438, Loss: 0.0007415146683342755\nValidation Loss improved to 0.012477421201765537 Saving model 0\nEpoch 439, Loss: 0.0008107072208076715\nValidation Loss improved to 0.012469994835555553 Saving model 0\nEpoch 440, Loss: 0.0007025558734312654\nValidation Loss improved to 0.012462412007153034 Saving model 0\nEpoch 441, Loss: 0.0007707230397500098\nValidation Loss improved to 0.012454686686396599 Saving model 0\nEpoch 442, Loss: 0.000780650123488158\nValidation Loss improved to 0.012447033077478409 Saving model 0\nEpoch 443, Loss: 0.0009287490393035114\nValidation Loss improved to 0.012438997626304626 Saving model 0\nEpoch 444, Loss: 0.000888399372342974\nValidation Loss improved to 0.01243206113576889 Saving model 0\nEpoch 445, Loss: 0.0009306255378760397\nValidation Loss improved to 0.0124242277815938 Saving model 0\nEpoch 446, Loss: 0.0007319438154809177\nValidation Loss improved to 0.012416590936481953 Saving model 0\nEpoch 447, Loss: 0.0008118933765217662\nValidation Loss improved to 0.012409147806465626 Saving model 0\nEpoch 448, Loss: 0.0007001133053563535\nValidation Loss improved to 0.012401866726577282 Saving model 0\nEpoch 449, Loss: 0.0007125359843485057\nValidation Loss improved to 0.012394292280077934 Saving model 0\nEpoch 450, Loss: 0.0007102811359800398\nValidation Loss improved to 0.012386726215481758 Saving model 0\nEpoch 451, Loss: 0.0007437370950356126\nValidation Loss improved to 0.012379266321659088 Saving model 0\nEpoch 452, Loss: 0.0006924552144482732\nValidation Loss improved to 0.0123717300593853 Saving model 0\nEpoch 453, Loss: 0.0008753048605285585\nValidation Loss improved to 0.012364412657916546 Saving model 0\nEpoch 454, Loss: 0.0007461995701305568\nValidation Loss improved to 0.01235753484070301 Saving model 0\nEpoch 455, Loss: 0.000788621255196631\nValidation Loss improved to 0.012350055389106274 Saving model 0\nEpoch 456, Loss: 0.0006825273158028722\nValidation Loss improved to 0.01234263926744461 Saving model 0\nEpoch 457, Loss: 0.0007100373040884733\nValidation Loss improved to 0.012335387989878654 Saving model 0\nEpoch 458, Loss: 0.0009259239886887372\nValidation Loss improved to 0.012328644283115864 Saving model 0\nEpoch 459, Loss: 0.000722105149179697\nValidation Loss improved to 0.012321356683969498 Saving model 0\nEpoch 460, Loss: 0.0006836793618276715\nValidation Loss improved to 0.01231419574469328 Saving model 0\nEpoch 461, Loss: 0.0006619074265472591\nValidation Loss improved to 0.01230713166296482 Saving model 0\nEpoch 462, Loss: 0.0006917543360032141\nValidation Loss improved to 0.012300197966396809 Saving model 0\nEpoch 463, Loss: 0.0006955814897082746\nValidation Loss improved to 0.012293911539018154 Saving model 0\nEpoch 464, Loss: 0.0008368325070478022\nValidation Loss improved to 0.012287038378417492 Saving model 0\nEpoch 465, Loss: 0.000692251545842737\nValidation Loss improved to 0.012280345894396305 Saving model 0\nEpoch 466, Loss: 0.000762429554015398\nValidation Loss improved to 0.012273234315216541 Saving model 0\nEpoch 467, Loss: 0.0007584678241983056\nValidation Loss improved to 0.012266382575035095 Saving model 0\nEpoch 468, Loss: 0.0006753277266398072\nValidation Loss improved to 0.012259241193532944 Saving model 0\nEpoch 469, Loss: 0.0007013754802756011\nValidation Loss improved to 0.012252063490450382 Saving model 0\nEpoch 470, Loss: 0.000903018401004374\nValidation Loss improved to 0.012245185673236847 Saving model 0\nEpoch 471, Loss: 0.0006922725006006658\nValidation Loss improved to 0.01223850529640913 Saving model 0\nEpoch 472, Loss: 0.0006330170435830951\nValidation Loss improved to 0.01223148312419653 Saving model 0\nEpoch 473, Loss: 0.0007033527363091707\nValidation Loss improved to 0.012224515900015831 Saving model 0\nEpoch 474, Loss: 0.0005808990681543946\nValidation Loss improved to 0.012217719107866287 Saving model 0\nEpoch 475, Loss: 0.0006831713835708797\nValidation Loss improved to 0.012210582382977009 Saving model 0\nEpoch 476, Loss: 0.0006799637922085822\nValidation Loss improved to 0.012203597463667393 Saving model 0\nEpoch 477, Loss: 0.0007230542250908911\nValidation Loss improved to 0.012196504510939121 Saving model 0\nEpoch 478, Loss: 0.0006674400065094233\nValidation Loss improved to 0.012189329601824284 Saving model 0\nEpoch 479, Loss: 0.0006469358340837061\nValidation Loss improved to 0.012182225473225117 Saving model 0\nEpoch 480, Loss: 0.0006807340541854501\nValidation Loss improved to 0.012174863368272781 Saving model 0\nEpoch 481, Loss: 0.0006033400422893465\nValidation Loss improved to 0.012167830020189285 Saving model 0\nEpoch 482, Loss: 0.0006366312736645341\nValidation Loss improved to 0.012160695157945156 Saving model 0\nEpoch 483, Loss: 0.0006697648786939681\nValidation Loss improved to 0.012154088355600834 Saving model 0\nEpoch 484, Loss: 0.0007567640859633684\nValidation Loss improved to 0.012147238478064537 Saving model 0\nEpoch 485, Loss: 0.0007676400127820671\nValidation Loss improved to 0.012140308506786823 Saving model 0\nEpoch 486, Loss: 0.0006755716749466956\nValidation Loss improved to 0.012133278883993626 Saving model 0\nEpoch 487, Loss: 0.0006330822943709791\nValidation Loss improved to 0.012126363813877106 Saving model 0\nEpoch 488, Loss: 0.0005904565332457423\nValidation Loss improved to 0.012119792401790619 Saving model 0\nEpoch 489, Loss: 0.0006743748672306538\nValidation Loss improved to 0.012112929485738277 Saving model 0\nEpoch 490, Loss: 0.0006607131799682975\nValidation Loss improved to 0.01210584957152605 Saving model 0\nEpoch 491, Loss: 0.0006131323170848191\nValidation Loss improved to 0.01209917664527893 Saving model 0\nEpoch 492, Loss: 0.0006387089961208403\nValidation Loss improved to 0.012092343531548977 Saving model 0\nEpoch 493, Loss: 0.0007614749483764172\nValidation Loss improved to 0.012085884809494019 Saving model 0\nEpoch 494, Loss: 0.0010605469578877091\nValidation Loss improved to 0.012078993022441864 Saving model 0\nEpoch 495, Loss: 0.0007814579294063151\nValidation Loss improved to 0.012072338722646236 Saving model 0\nEpoch 496, Loss: 0.0006840482819825411\nValidation Loss improved to 0.012065927498042583 Saving model 0\nEpoch 497, Loss: 0.0006417963886633515\nValidation Loss improved to 0.012059795670211315 Saving model 0\nEpoch 498, Loss: 0.0007503602537326515\nValidation Loss improved to 0.012053042650222778 Saving model 0\nEpoch 499, Loss: 0.0007976656779646873\nValidation Loss improved to 0.012046350166201591 Saving model 0\nEpoch 500, Loss: 0.0008400716469623148\nValidation Loss improved to 0.01203984022140503 Saving model 0\nEpoch 501, Loss: 0.0007200686377473176\nValidation Loss improved to 0.012033236213028431 Saving model 0\nEpoch 502, Loss: 0.0005878495285287499\nValidation Loss improved to 0.012026719748973846 Saving model 0\nEpoch 503, Loss: 0.0005758922197856009\nValidation Loss improved to 0.012020023539662361 Saving model 0\nEpoch 504, Loss: 0.0005854926421307027\nValidation Loss improved to 0.012013448402285576 Saving model 0\nEpoch 505, Loss: 0.0006777034723199904\nValidation Loss improved to 0.012006956152617931 Saving model 0\nEpoch 506, Loss: 0.0007248088368214667\nValidation Loss improved to 0.012001488357782364 Saving model 0\nEpoch 507, Loss: 0.0008339505875483155\nValidation Loss improved to 0.01199512928724289 Saving model 0\nEpoch 508, Loss: 0.0006719580269418657\nValidation Loss improved to 0.011988632380962372 Saving model 0\nEpoch 509, Loss: 0.0007005698862485588\nValidation Loss improved to 0.011981780640780926 Saving model 0\nEpoch 510, Loss: 0.0007421053014695644\nValidation Loss improved to 0.011975264176726341 Saving model 0\nEpoch 511, Loss: 0.0006446524639613926\nValidation Loss improved to 0.011968866921961308 Saving model 0\nEpoch 512, Loss: 0.000570128031540662\nValidation Loss improved to 0.01196256186813116 Saving model 0\nEpoch 513, Loss: 0.0005661066970787942\nValidation Loss improved to 0.011956152506172657 Saving model 0\nEpoch 514, Loss: 0.0005450081080198288\nValidation Loss improved to 0.011949813924729824 Saving model 0\nEpoch 515, Loss: 0.0005451024626381695\nValidation Loss improved to 0.011943256482481956 Saving model 0\nEpoch 516, Loss: 0.0006140843033790588\nValidation Loss improved to 0.011936773546040058 Saving model 0\nEpoch 517, Loss: 0.0005774418241344392\nValidation Loss improved to 0.011930175125598907 Saving model 0\nEpoch 518, Loss: 0.0006059087463654578\nValidation Loss improved to 0.011923584155738354 Saving model 0\nEpoch 519, Loss: 0.0006151321576908231\nValidation Loss improved to 0.011917284689843655 Saving model 0\nEpoch 520, Loss: 0.0005144233582541347\nValidation Loss improved to 0.011910981498658657 Saving model 0\nEpoch 521, Loss: 0.0005215718410909176\nValidation Loss improved to 0.01190502755343914 Saving model 0\nEpoch 522, Loss: 0.0006913677207194269\nValidation Loss improved to 0.011898631229996681 Saving model 0\nEpoch 523, Loss: 0.0007788040093146265\nValidation Loss improved to 0.011892640963196754 Saving model 0\nEpoch 524, Loss: 0.0007725637406110764\nValidation Loss improved to 0.011886071413755417 Saving model 0\nEpoch 525, Loss: 0.0007475644815713167\nValidation Loss improved to 0.011879866942763329 Saving model 0\nEpoch 526, Loss: 0.0006257746717892587\nValidation Loss improved to 0.011873687617480755 Saving model 0\nEpoch 527, Loss: 0.0006661433144472539\nValidation Loss improved to 0.01186779048293829 Saving model 0\nEpoch 528, Loss: 0.0005260498146526515\nValidation Loss improved to 0.011861859820783138 Saving model 0\nEpoch 529, Loss: 0.0005853850743733346\nValidation Loss improved to 0.011855474673211575 Saving model 0\nEpoch 530, Loss: 0.0005402855458669364\nValidation Loss improved to 0.01184939220547676 Saving model 0\nEpoch 531, Loss: 0.0005608369829133153\nValidation Loss improved to 0.011843381449580193 Saving model 0\nEpoch 532, Loss: 0.0005549094639718533\nValidation Loss improved to 0.011836900375783443 Saving model 0\nEpoch 533, Loss: 0.0007364985649473965\nValidation Loss improved to 0.011831168085336685 Saving model 0\nEpoch 534, Loss: 0.0006694714538753033\nValidation Loss improved to 0.011825239285826683 Saving model 0\nEpoch 535, Loss: 0.0005688774981535971\nValidation Loss improved to 0.011819391511380672 Saving model 0\nEpoch 536, Loss: 0.0005695418803952634\nValidation Loss improved to 0.011813641525804996 Saving model 0\nEpoch 537, Loss: 0.0005087321042083204\nValidation Loss improved to 0.011807766743004322 Saving model 0\nEpoch 538, Loss: 0.000558525905944407\nValidation Loss improved to 0.011801717802882195 Saving model 0\nEpoch 539, Loss: 0.0005400499794632196\nValidation Loss improved to 0.01179602649062872 Saving model 0\nEpoch 540, Loss: 0.0006000484572723508\nValidation Loss improved to 0.011790257878601551 Saving model 0\nEpoch 541, Loss: 0.0006096463766880333\nValidation Loss improved to 0.011784499511122704 Saving model 0\nEpoch 542, Loss: 0.0005365270189940929\nValidation Loss improved to 0.011778908781707287 Saving model 0\nEpoch 543, Loss: 0.0004893714794889092\nValidation Loss improved to 0.011772981844842434 Saving model 0\nEpoch 544, Loss: 0.0005856705829501152\nValidation Loss improved to 0.01176778506487608 Saving model 0\nEpoch 545, Loss: 0.0008024906273931265\nValidation Loss improved to 0.011761937290430069 Saving model 0\nEpoch 546, Loss: 0.0006519241724163294\nValidation Loss improved to 0.011756032705307007 Saving model 0\nEpoch 547, Loss: 0.0005349339335225523\nValidation Loss improved to 0.011749888770282269 Saving model 0\nEpoch 548, Loss: 0.0006026377668604255\nValidation Loss improved to 0.011744342744350433 Saving model 0\nEpoch 549, Loss: 0.0005895584472455084\nValidation Loss improved to 0.011738856323063374 Saving model 0\nEpoch 550, Loss: 0.0005292861023917794\nValidation Loss improved to 0.011733113788068295 Saving model 0\nEpoch 551, Loss: 0.0006187963881529868\nValidation Loss improved to 0.011727424338459969 Saving model 0\nEpoch 552, Loss: 0.0005530210910364985\nValidation Loss improved to 0.011721701361238956 Saving model 0\nEpoch 553, Loss: 0.0006458263378590345\nValidation Loss improved to 0.011716390028595924 Saving model 0\nEpoch 554, Loss: 0.000847667979542166\nValidation Loss improved to 0.011710881255567074 Saving model 0\nEpoch 555, Loss: 0.0006192700820975006\nValidation Loss improved to 0.011705007404088974 Saving model 0\nEpoch 556, Loss: 0.0006401460850611329\nValidation Loss improved to 0.011699332855641842 Saving model 0\nEpoch 557, Loss: 0.0005331381689757109\nValidation Loss improved to 0.011693966574966908 Saving model 0\nEpoch 558, Loss: 0.0005149204516783357\nValidation Loss improved to 0.011688580736517906 Saving model 0\nEpoch 559, Loss: 0.000573501514736563\nValidation Loss improved to 0.011682990938425064 Saving model 0\nEpoch 560, Loss: 0.0005201291642151773\nValidation Loss improved to 0.011677446775138378 Saving model 0\nEpoch 561, Loss: 0.0005294833681546152\nValidation Loss improved to 0.01167195476591587 Saving model 0\nEpoch 562, Loss: 0.0005407807766459882\nValidation Loss improved to 0.011666582897305489 Saving model 0\nEpoch 563, Loss: 0.0006867951597087085\nValidation Loss improved to 0.011660846881568432 Saving model 0\nEpoch 564, Loss: 0.0005336745525710285\nValidation Loss improved to 0.011655636131763458 Saving model 0\nEpoch 565, Loss: 0.0005625553312711418\nValidation Loss improved to 0.01165024098008871 Saving model 0\nEpoch 566, Loss: 0.0006795377703383565\nValidation Loss improved to 0.011644558049738407 Saving model 0\nEpoch 567, Loss: 0.0009909201180562377\nValidation Loss improved to 0.011639265343546867 Saving model 0\nEpoch 568, Loss: 0.0008404530235566199\nValidation Loss improved to 0.011634177528321743 Saving model 0\nEpoch 569, Loss: 0.0008732039714232087\nValidation Loss improved to 0.011628986336290836 Saving model 0\nEpoch 570, Loss: 0.0007973093306645751\nValidation Loss improved to 0.011623740196228027 Saving model 0\nEpoch 571, Loss: 0.0006176092429086566\nValidation Loss improved to 0.011618178337812424 Saving model 0\nEpoch 572, Loss: 0.0005751964636147022\nValidation Loss improved to 0.011613041162490845 Saving model 0\nEpoch 573, Loss: 0.0005438353982754052\nValidation Loss improved to 0.011607609689235687 Saving model 0\nEpoch 574, Loss: 0.0005676544387824833\nValidation Loss improved to 0.011602258309721947 Saving model 0\nEpoch 575, Loss: 0.0006774894427508116\nValidation Loss improved to 0.011596758849918842 Saving model 0\nEpoch 576, Loss: 0.0006801785784773529\nValidation Loss improved to 0.011591981165111065 Saving model 0\nEpoch 577, Loss: 0.0007107567507773638\nValidation Loss improved to 0.011586806736886501 Saving model 0\nEpoch 578, Loss: 0.000566172122489661\nValidation Loss improved to 0.011581619270145893 Saving model 0\nEpoch 579, Loss: 0.0006732191541232169\nValidation Loss improved to 0.011576632037758827 Saving model 0\nEpoch 580, Loss: 0.0005345186800695956\nValidation Loss improved to 0.011571300216019154 Saving model 0\nEpoch 581, Loss: 0.0006560351466760039\nValidation Loss improved to 0.011566187255084515 Saving model 0\nEpoch 582, Loss: 0.000556649814825505\nValidation Loss improved to 0.011560829356312752 Saving model 0\nEpoch 583, Loss: 0.0005327648250386119\nValidation Loss improved to 0.01155563909560442 Saving model 0\nEpoch 584, Loss: 0.0005220864550210536\nValidation Loss improved to 0.01155055034905672 Saving model 0\nEpoch 585, Loss: 0.0005376135231927037\nValidation Loss improved to 0.011545440182089806 Saving model 0\nEpoch 586, Loss: 0.0005209989030845463\nValidation Loss improved to 0.01154046505689621 Saving model 0\nEpoch 587, Loss: 0.0005561329307965934\nValidation Loss improved to 0.011535309255123138 Saving model 0\nEpoch 588, Loss: 0.0006835795938968658\nValidation Loss improved to 0.011529883369803429 Saving model 0\nEpoch 589, Loss: 0.0006559623288922012\nValidation Loss improved to 0.011525124311447144 Saving model 0\nEpoch 590, Loss: 0.0006590223638340831\nValidation Loss improved to 0.011520029976963997 Saving model 0\nEpoch 591, Loss: 0.000533921062014997\nValidation Loss improved to 0.011515128426253796 Saving model 0\nEpoch 592, Loss: 0.00049361283890903\nValidation Loss improved to 0.011510270647704601 Saving model 0\nEpoch 593, Loss: 0.0005492942873388529\nValidation Loss improved to 0.01150545384734869 Saving model 0\nEpoch 594, Loss: 0.0006515290588140488\nValidation Loss improved to 0.011500622145831585 Saving model 0\nEpoch 595, Loss: 0.0005794651224277914\nValidation Loss improved to 0.011495793238282204 Saving model 0\nEpoch 596, Loss: 0.0004911578726023436\nValidation Loss improved to 0.011491063982248306 Saving model 0\nEpoch 597, Loss: 0.000473195017548278\nValidation Loss improved to 0.011486323550343513 Saving model 0\nEpoch 598, Loss: 0.0004783518088515848\nValidation Loss improved to 0.011481767520308495 Saving model 0\nEpoch 599, Loss: 0.0007753191166557372\nValidation Loss improved to 0.011477172374725342 Saving model 0\nEpoch 600, Loss: 0.0005662517505697906\nValidation Loss improved to 0.011472532525658607 Saving model 0\nEpoch 601, Loss: 0.0005981444264762104\nValidation Loss improved to 0.011467843316495419 Saving model 0\nEpoch 602, Loss: 0.0005388841964304447\nValidation Loss improved to 0.011463351547718048 Saving model 0\nEpoch 603, Loss: 0.0004552449390757829\nValidation Loss improved to 0.011458459310233593 Saving model 0\nEpoch 604, Loss: 0.000490456004627049\nValidation Loss improved to 0.011453844606876373 Saving model 0\nEpoch 605, Loss: 0.00047200446715578437\nValidation Loss improved to 0.01144920289516449 Saving model 0\nEpoch 606, Loss: 0.0006584315560758114\nValidation Loss improved to 0.011444698087871075 Saving model 0\nEpoch 607, Loss: 0.00047307199565693736\nValidation Loss improved to 0.011439903639256954 Saving model 0\nEpoch 608, Loss: 0.0006507226498797536\nValidation Loss improved to 0.01143505610525608 Saving model 0\nEpoch 609, Loss: 0.0006096597062423825\nValidation Loss improved to 0.011430269107222557 Saving model 0\nEpoch 610, Loss: 0.0004928529378958046\nValidation Loss improved to 0.011425502598285675 Saving model 0\nEpoch 611, Loss: 0.0006518036825582385\nValidation Loss improved to 0.011421097442507744 Saving model 0\nEpoch 612, Loss: 0.0006908613140694797\nValidation Loss improved to 0.011416058987379074 Saving model 0\nEpoch 613, Loss: 0.0007368020014837384\nValidation Loss improved to 0.011411510407924652 Saving model 0\nEpoch 614, Loss: 0.0006852816441096365\nValidation Loss improved to 0.011406856589019299 Saving model 0\nEpoch 615, Loss: 0.0005988004268147051\nValidation Loss improved to 0.011402404867112637 Saving model 0\nEpoch 616, Loss: 0.0005497889942489564\nValidation Loss improved to 0.011397753842175007 Saving model 0\nEpoch 617, Loss: 0.0005749328411184251\nValidation Loss improved to 0.01139306090772152 Saving model 0\nEpoch 618, Loss: 0.0006771035259589553\nValidation Loss improved to 0.011388896964490414 Saving model 0\nEpoch 619, Loss: 0.0007148974691517651\nValidation Loss improved to 0.011384161189198494 Saving model 0\nEpoch 620, Loss: 0.0008491986081935465\nValidation Loss improved to 0.01137970294803381 Saving model 0\nEpoch 621, Loss: 0.0008454545168206096\nValidation Loss improved to 0.01137539092451334 Saving model 0\nEpoch 622, Loss: 0.0007055209134705365\nValidation Loss improved to 0.011371126398444176 Saving model 0\nEpoch 623, Loss: 0.00048159025027416646\nValidation Loss improved to 0.01136655081063509 Saving model 0\nEpoch 624, Loss: 0.000528553151525557\nValidation Loss improved to 0.011362140998244286 Saving model 0\nEpoch 625, Loss: 0.0004795454442501068\nValidation Loss improved to 0.011357988230884075 Saving model 0\nEpoch 626, Loss: 0.0005018675001338124\nValidation Loss improved to 0.011353523470461369 Saving model 0\nEpoch 627, Loss: 0.00048742361832410097\nValidation Loss improved to 0.01134908851236105 Saving model 0\nEpoch 628, Loss: 0.0005401283269748092\nValidation Loss improved to 0.011344498954713345 Saving model 0\nEpoch 629, Loss: 0.0005934590008109808\nValidation Loss improved to 0.011339613236486912 Saving model 0\nEpoch 630, Loss: 0.0006331020849756896\nValidation Loss improved to 0.011335212737321854 Saving model 0\nEpoch 631, Loss: 0.0005249997484497726\nValidation Loss improved to 0.011330695822834969 Saving model 0\nEpoch 632, Loss: 0.0005371735896915197\nValidation Loss improved to 0.011326499283313751 Saving model 0\nEpoch 633, Loss: 0.000520023051649332\nValidation Loss improved to 0.011322245001792908 Saving model 0\nEpoch 634, Loss: 0.0005126254982315004\nValidation Loss improved to 0.011318023316562176 Saving model 0\nEpoch 635, Loss: 0.00043606810504570603\nValidation Loss improved to 0.011313843540847301 Saving model 0\nEpoch 636, Loss: 0.000406384002417326\nValidation Loss improved to 0.011309499852359295 Saving model 0\nEpoch 637, Loss: 0.0004375460557639599\nValidation Loss improved to 0.011305119842290878 Saving model 0\nEpoch 638, Loss: 0.0004932042793370783\nValidation Loss improved to 0.011300676502287388 Saving model 0\nEpoch 639, Loss: 0.00043603332596831024\nValidation Loss improved to 0.011296514421701431 Saving model 0\nEpoch 640, Loss: 0.0005010595778003335\nValidation Loss improved to 0.011292298324406147 Saving model 0\nEpoch 641, Loss: 0.0005552605725824833\nValidation Loss improved to 0.011288070119917393 Saving model 0\nEpoch 642, Loss: 0.00090499606449157\nValidation Loss improved to 0.011283709667623043 Saving model 0\nEpoch 643, Loss: 0.0008169363718479872\nValidation Loss improved to 0.011279157362878323 Saving model 0\nEpoch 644, Loss: 0.0008742973441258073\nValidation Loss improved to 0.011275166645646095 Saving model 0\nEpoch 645, Loss: 0.0006034670514054596\nValidation Loss improved to 0.011270943097770214 Saving model 0\nEpoch 646, Loss: 0.0005170984659343958\nValidation Loss improved to 0.011266708374023438 Saving model 0\nEpoch 647, Loss: 0.0005385632975958288\nValidation Loss improved to 0.011262590065598488 Saving model 0\nEpoch 648, Loss: 0.0004621937987394631\nValidation Loss improved to 0.011258323676884174 Saving model 0\nEpoch 649, Loss: 0.0005928907776251435\nValidation Loss improved to 0.011253966018557549 Saving model 0\nEpoch 650, Loss: 0.0006092251860536635\nValidation Loss improved to 0.0112496642395854 Saving model 0\nEpoch 651, Loss: 0.0005562846781685948\nValidation Loss improved to 0.01124524138867855 Saving model 0\nEpoch 652, Loss: 0.0005823351675644517\nValidation Loss improved to 0.011241281405091286 Saving model 0\nEpoch 653, Loss: 0.0005769634153693914\nValidation Loss improved to 0.01123723667114973 Saving model 0\nEpoch 654, Loss: 0.0005932573112659156\nValidation Loss improved to 0.011233125813305378 Saving model 0\nEpoch 655, Loss: 0.000505071016959846\nValidation Loss improved to 0.011228833347558975 Saving model 0\nEpoch 656, Loss: 0.00043203699169680476\nValidation Loss improved to 0.011224850080907345 Saving model 0\nEpoch 657, Loss: 0.00044360297033563256\nValidation Loss improved to 0.01122078113257885 Saving model 0\nEpoch 658, Loss: 0.0005310708074830472\nValidation Loss improved to 0.011216627433896065 Saving model 0\nEpoch 659, Loss: 0.0004226566234137863\nValidation Loss improved to 0.011212648823857307 Saving model 0\nEpoch 660, Loss: 0.0005957522662356496\nValidation Loss improved to 0.011208653450012207 Saving model 0\nEpoch 661, Loss: 0.000508153170812875\nValidation Loss improved to 0.011204536072909832 Saving model 0\nEpoch 662, Loss: 0.0005794530152343214\nValidation Loss improved to 0.01120045781135559 Saving model 0\nEpoch 663, Loss: 0.0005860818782821298\nValidation Loss improved to 0.011196437291800976 Saving model 0\nEpoch 664, Loss: 0.0007067096303217113\nValidation Loss improved to 0.011192514561116695 Saving model 0\nEpoch 665, Loss: 0.0004780516028404236\nValidation Loss improved to 0.01118852011859417 Saving model 0\nEpoch 666, Loss: 0.0004648383182939142\nValidation Loss improved to 0.011184613220393658 Saving model 0\nEpoch 667, Loss: 0.00041787399095483124\nValidation Loss improved to 0.01118050143122673 Saving model 0\nEpoch 668, Loss: 0.0004514620522968471\nValidation Loss improved to 0.011176584288477898 Saving model 0\nEpoch 669, Loss: 0.0004534051986411214\nValidation Loss improved to 0.011172556318342686 Saving model 0\nEpoch 670, Loss: 0.0006202995427884161\nValidation Loss improved to 0.011168343015015125 Saving model 0\nEpoch 671, Loss: 0.0005602582241408527\nValidation Loss improved to 0.011164329014718533 Saving model 0\nEpoch 672, Loss: 0.0005605542683042586\nValidation Loss improved to 0.01116027869284153 Saving model 0\nEpoch 673, Loss: 0.0005021461984142661\nValidation Loss improved to 0.011156249791383743 Saving model 0\nEpoch 674, Loss: 0.0006013424135744572\nValidation Loss improved to 0.011152496561408043 Saving model 0\nEpoch 675, Loss: 0.0006235387409105897\nValidation Loss improved to 0.011148530058562756 Saving model 0\nEpoch 676, Loss: 0.0004804797354154289\nValidation Loss improved to 0.011144631542265415 Saving model 0\nEpoch 677, Loss: 0.00048344911192543805\nValidation Loss improved to 0.011141117662191391 Saving model 0\nEpoch 678, Loss: 0.00048356596380472183\nValidation Loss improved to 0.011137436144053936 Saving model 0\nEpoch 679, Loss: 0.0006675184704363346\nValidation Loss improved to 0.011133924126625061 Saving model 0\nEpoch 680, Loss: 0.0006531292456202209\nValidation Loss improved to 0.011129945516586304 Saving model 0\nEpoch 681, Loss: 0.0006355607183650136\nValidation Loss improved to 0.011126188561320305 Saving model 0\nEpoch 682, Loss: 0.0004756418929900974\nValidation Loss improved to 0.011122632771730423 Saving model 0\nEpoch 683, Loss: 0.0005537018878385425\nValidation Loss improved to 0.011118916794657707 Saving model 0\nEpoch 684, Loss: 0.00045894848881289363\nValidation Loss improved to 0.0111150611191988 Saving model 0\nEpoch 685, Loss: 0.0005295759183354676\nValidation Loss improved to 0.011111491359770298 Saving model 0\nEpoch 686, Loss: 0.0006398241384886205\nValidation Loss improved to 0.011107704602181911 Saving model 0\nEpoch 687, Loss: 0.0006438795244321227\nValidation Loss improved to 0.01110388245433569 Saving model 0\nEpoch 688, Loss: 0.0005643470212817192\nValidation Loss improved to 0.011099742725491524 Saving model 0\nEpoch 689, Loss: 0.0005665033822879195\nValidation Loss improved to 0.01109602116048336 Saving model 0\nEpoch 690, Loss: 0.000590172247029841\nValidation Loss improved to 0.011092345230281353 Saving model 0\nEpoch 691, Loss: 0.0004328873474150896\nValidation Loss improved to 0.011088486760854721 Saving model 0\nEpoch 692, Loss: 0.0005353968590497971\nValidation Loss improved to 0.01108487881720066 Saving model 0\nEpoch 693, Loss: 0.0005919407703913748\nValidation Loss improved to 0.011081135831773281 Saving model 0\nEpoch 694, Loss: 0.0005011953180655837\nValidation Loss improved to 0.011077512986958027 Saving model 0\nEpoch 695, Loss: 0.00047405820805579424\nValidation Loss improved to 0.01107401680201292 Saving model 0\nEpoch 696, Loss: 0.0006368683534674346\nValidation Loss improved to 0.011070522479712963 Saving model 0\nEpoch 697, Loss: 0.000484664662508294\nValidation Loss improved to 0.011066892184317112 Saving model 0\nEpoch 698, Loss: 0.0005273889983072877\nValidation Loss improved to 0.01106312032788992 Saving model 0\nEpoch 699, Loss: 0.0005711892154067755\nValidation Loss improved to 0.011059751734137535 Saving model 0\nEpoch 700, Loss: 0.0006082626641727984\nValidation Loss improved to 0.011055951938033104 Saving model 0\nEpoch 701, Loss: 0.000501899397931993\nValidation Loss improved to 0.01105221826583147 Saving model 0\nEpoch 702, Loss: 0.00047910070861689746\nValidation Loss improved to 0.011048555374145508 Saving model 0\nEpoch 703, Loss: 0.0004824727075174451\nValidation Loss improved to 0.011044822633266449 Saving model 0\nEpoch 704, Loss: 0.00048707184032537043\nValidation Loss improved to 0.011041147634387016 Saving model 0\nEpoch 705, Loss: 0.00046733362250961363\nValidation Loss improved to 0.011037446558475494 Saving model 0\nEpoch 706, Loss: 0.00048597503337077796\nValidation Loss improved to 0.011033848859369755 Saving model 0\nEpoch 707, Loss: 0.0005571229849010706\nValidation Loss improved to 0.01103015337139368 Saving model 0\nEpoch 708, Loss: 0.0004639427352230996\nValidation Loss improved to 0.011026584543287754 Saving model 0\nEpoch 709, Loss: 0.0005927341990172863\nValidation Loss improved to 0.011022890917956829 Saving model 0\nEpoch 710, Loss: 0.0004670575726777315\nValidation Loss improved to 0.011019407771527767 Saving model 0\nEpoch 711, Loss: 0.000458411785075441\nValidation Loss improved to 0.011015902273356915 Saving model 0\nEpoch 712, Loss: 0.00043662558891810477\nValidation Loss improved to 0.011012417264282703 Saving model 0\nEpoch 713, Loss: 0.000522694259416312\nValidation Loss improved to 0.011008858680725098 Saving model 0\nEpoch 714, Loss: 0.0005101963179185987\nValidation Loss improved to 0.011005491018295288 Saving model 0\nEpoch 715, Loss: 0.0004884875379502773\nValidation Loss improved to 0.011001970618963242 Saving model 0\nEpoch 716, Loss: 0.0004846903320867568\nValidation Loss improved to 0.010998510755598545 Saving model 0\nEpoch 717, Loss: 0.00042298255721107125\nValidation Loss improved to 0.01099502295255661 Saving model 0\nEpoch 718, Loss: 0.0004326097550801933\nValidation Loss improved to 0.010991337709128857 Saving model 0\nEpoch 719, Loss: 0.0004855504084844142\nValidation Loss improved to 0.010987905785441399 Saving model 0\nEpoch 720, Loss: 0.0004550863814074546\nValidation Loss improved to 0.0109843285754323 Saving model 0\nEpoch 721, Loss: 0.0003710552991833538\nValidation Loss improved to 0.010980852879583836 Saving model 0\nEpoch 722, Loss: 0.00043869781075045466\nValidation Loss improved to 0.010977664962410927 Saving model 0\nEpoch 723, Loss: 0.000573166529648006\nValidation Loss improved to 0.010974113829433918 Saving model 0\nEpoch 724, Loss: 0.00048083049478009343\nValidation Loss improved to 0.0109707061201334 Saving model 0\nEpoch 725, Loss: 0.00048045162111520767\nValidation Loss improved to 0.010967225767672062 Saving model 0\nEpoch 726, Loss: 0.0003861401346512139\nValidation Loss improved to 0.010963782668113708 Saving model 0\nEpoch 727, Loss: 0.00042276576277799904\nValidation Loss improved to 0.010960369370877743 Saving model 0\nEpoch 728, Loss: 0.0005073611391708255\nValidation Loss improved to 0.010956835001707077 Saving model 0\nEpoch 729, Loss: 0.0004924032837152481\nValidation Loss improved to 0.010953732766211033 Saving model 0\nEpoch 730, Loss: 0.0005222088075242937\nValidation Loss improved to 0.01095031388103962 Saving model 0\nEpoch 731, Loss: 0.0006144549697637558\nValidation Loss improved to 0.01094684936106205 Saving model 0\nEpoch 732, Loss: 0.0007425213698297739\nValidation Loss improved to 0.010943425819277763 Saving model 0\nEpoch 733, Loss: 0.0005822241073474288\nValidation Loss improved to 0.010940172709524632 Saving model 0\nEpoch 734, Loss: 0.000556357903406024\nValidation Loss improved to 0.010937108658254147 Saving model 0\nEpoch 735, Loss: 0.0006497125141322613\nValidation Loss improved to 0.01093380432575941 Saving model 0\nEpoch 736, Loss: 0.0004932330921292305\nValidation Loss improved to 0.010930582880973816 Saving model 0\nEpoch 737, Loss: 0.00043119656038470566\nValidation Loss improved to 0.010927087627351284 Saving model 0\nEpoch 738, Loss: 0.0004148035659454763\nValidation Loss improved to 0.010924044996500015 Saving model 0\nEpoch 739, Loss: 0.00046066101640462875\nValidation Loss improved to 0.010920806787908077 Saving model 0\nEpoch 740, Loss: 0.0005387119599618018\nValidation Loss improved to 0.010917584411799908 Saving model 0\nEpoch 741, Loss: 0.0005771266296505928\nValidation Loss improved to 0.010914710350334644 Saving model 0\nEpoch 742, Loss: 0.0007653094362467527\nValidation Loss improved to 0.010911456309258938 Saving model 0\nEpoch 743, Loss: 0.0006507594953291118\nValidation Loss improved to 0.01090831309556961 Saving model 0\nEpoch 744, Loss: 0.0006027764757163823\nValidation Loss improved to 0.010905301198363304 Saving model 0\nEpoch 745, Loss: 0.0007197338854894042\nValidation Loss improved to 0.010902012698352337 Saving model 0\nEpoch 746, Loss: 0.0006243567913770676\nValidation Loss improved to 0.010898825712502003 Saving model 0\nEpoch 747, Loss: 0.0006523505435325205\nValidation Loss improved to 0.010895511135458946 Saving model 0\nEpoch 748, Loss: 0.0004703650192823261\nValidation Loss improved to 0.010892314836382866 Saving model 0\nEpoch 749, Loss: 0.00040176804759539664\nValidation Loss improved to 0.010889124125242233 Saving model 0\nEpoch 750, Loss: 0.0005651254905387759\nValidation Loss improved to 0.010885770432651043 Saving model 0\nEpoch 751, Loss: 0.0005296203889884055\nValidation Loss improved to 0.010882571339607239 Saving model 0\nEpoch 752, Loss: 0.00043155980529263616\nValidation Loss improved to 0.010879436507821083 Saving model 0\nEpoch 753, Loss: 0.00041041665826924145\nValidation Loss improved to 0.010876333341002464 Saving model 0\nEpoch 754, Loss: 0.0003996099694631994\nValidation Loss improved to 0.010873169638216496 Saving model 0\nEpoch 755, Loss: 0.0004765718476846814\nValidation Loss improved to 0.010869848541915417 Saving model 0\nEpoch 756, Loss: 0.0004296366241760552\nValidation Loss improved to 0.010866831056773663 Saving model 0\nEpoch 757, Loss: 0.0004973041359335184\nValidation Loss improved to 0.01086343452334404 Saving model 0\nEpoch 758, Loss: 0.0005812548333778977\nValidation Loss improved to 0.010860410518944263 Saving model 0\nEpoch 759, Loss: 0.00043365315650589764\nValidation Loss improved to 0.010857440531253815 Saving model 0\nEpoch 760, Loss: 0.0004252448561601341\nValidation Loss improved to 0.010854225605726242 Saving model 0\nEpoch 761, Loss: 0.00040175655158236623\nValidation Loss improved to 0.010851090773940086 Saving model 0\nEpoch 762, Loss: 0.000494130770675838\nValidation Loss improved to 0.010848039761185646 Saving model 0\nEpoch 763, Loss: 0.00047011146671138704\nValidation Loss improved to 0.010844970121979713 Saving model 0\nEpoch 764, Loss: 0.00046360789565369487\nValidation Loss improved to 0.010842099785804749 Saving model 0\nEpoch 765, Loss: 0.0005754627636633813\nValidation Loss improved to 0.010839208029210567 Saving model 0\nEpoch 766, Loss: 0.0005376186454668641\nValidation Loss improved to 0.010836116969585419 Saving model 0\nEpoch 767, Loss: 0.0005720573826692998\nValidation Loss improved to 0.010833106003701687 Saving model 0\nEpoch 768, Loss: 0.0005630180821754038\nValidation Loss improved to 0.010830042883753777 Saving model 0\nEpoch 769, Loss: 0.0008338402258232236\nValidation Loss improved to 0.010826955549418926 Saving model 0\nEpoch 770, Loss: 0.0005209462833590806\nValidation Loss improved to 0.010823680087924004 Saving model 0\nEpoch 771, Loss: 0.0005909508327022195\nValidation Loss improved to 0.010820661671459675 Saving model 0\nEpoch 772, Loss: 0.0005387155106291175\nValidation Loss improved to 0.010817766189575195 Saving model 0\nEpoch 773, Loss: 0.0005093223298899829\nValidation Loss improved to 0.01081477478146553 Saving model 0\nEpoch 774, Loss: 0.000463903124909848\nValidation Loss improved to 0.010812029242515564 Saving model 0\nEpoch 775, Loss: 0.0006851893267594278\nValidation Loss improved to 0.010808833874762058 Saving model 0\nEpoch 776, Loss: 0.0005409725708886981\nValidation Loss improved to 0.010805678553879261 Saving model 0\nEpoch 777, Loss: 0.0005273435381241143\nValidation Loss improved to 0.01080253068357706 Saving model 0\nEpoch 778, Loss: 0.0005958747933618724\nValidation Loss improved to 0.010799400508403778 Saving model 0\nEpoch 779, Loss: 0.0004779619921464473\nValidation Loss improved to 0.010796424001455307 Saving model 0\nEpoch 780, Loss: 0.0004576494684442878\nValidation Loss improved to 0.010793374851346016 Saving model 0\nEpoch 781, Loss: 0.0005977521068416536\nValidation Loss improved to 0.010790378786623478 Saving model 0\nEpoch 782, Loss: 0.0004949062131345272\nValidation Loss improved to 0.010787510313093662 Saving model 0\nEpoch 783, Loss: 0.00048275795415975153\nValidation Loss improved to 0.010784531012177467 Saving model 0\nEpoch 784, Loss: 0.0003967167576774955\nValidation Loss improved to 0.010781553573906422 Saving model 0\nEpoch 785, Loss: 0.0003900862648151815\nValidation Loss improved to 0.010778396390378475 Saving model 0\nEpoch 786, Loss: 0.0004561821115203202\nValidation Loss improved to 0.010775374248623848 Saving model 0\nEpoch 787, Loss: 0.0004321910091675818\nValidation Loss improved to 0.010772420093417168 Saving model 0\nEpoch 788, Loss: 0.00041129274177365005\nValidation Loss improved to 0.010769433341920376 Saving model 0\nEpoch 789, Loss: 0.0004142064426559955\nValidation Loss improved to 0.010766549035906792 Saving model 0\nEpoch 790, Loss: 0.00041253212839365005\nValidation Loss improved to 0.010763573460280895 Saving model 0\nEpoch 791, Loss: 0.0004429692926350981\nValidation Loss improved to 0.01076066680252552 Saving model 0\nEpoch 792, Loss: 0.0003847965563181788\nValidation Loss improved to 0.010757758282124996 Saving model 0\nEpoch 793, Loss: 0.0003855330287478864\nValidation Loss improved to 0.010754656046628952 Saving model 0\nEpoch 794, Loss: 0.00047980633098632097\nValidation Loss improved to 0.010751687921583652 Saving model 0\nEpoch 795, Loss: 0.0004884374793618917\nValidation Loss improved to 0.010748674161732197 Saving model 0\nEpoch 796, Loss: 0.0006735803326591849\nValidation Loss improved to 0.010745944455265999 Saving model 0\nEpoch 797, Loss: 0.0006422804435715079\nValidation Loss improved to 0.010743102990090847 Saving model 0\nEpoch 798, Loss: 0.0005209884839132428\nValidation Loss improved to 0.010740361176431179 Saving model 0\nEpoch 799, Loss: 0.0004549817240331322\nValidation Loss improved to 0.010737706907093525 Saving model 0\nEpoch 800, Loss: 0.00048002906260080636\nValidation Loss improved to 0.010734771378338337 Saving model 0\nEpoch 801, Loss: 0.0004917926853522658\nValidation Loss improved to 0.010731743648648262 Saving model 0\nEpoch 802, Loss: 0.0004930983413942158\nValidation Loss improved to 0.010729094967246056 Saving model 0\nEpoch 803, Loss: 0.0004874739679507911\nValidation Loss improved to 0.010726449079811573 Saving model 0\nEpoch 804, Loss: 0.0006149241235107183\nValidation Loss improved to 0.010723471641540527 Saving model 0\nEpoch 805, Loss: 0.00045496688107959926\nValidation Loss improved to 0.010720949620008469 Saving model 0\nEpoch 806, Loss: 0.0005311091081239283\nValidation Loss improved to 0.010718068107962608 Saving model 0\nEpoch 807, Loss: 0.00044539279770106077\nValidation Loss improved to 0.010715305805206299 Saving model 0\nEpoch 808, Loss: 0.0004375397984404117\nValidation Loss improved to 0.01071250345557928 Saving model 0\nEpoch 809, Loss: 0.00043898061267100275\nValidation Loss improved to 0.010709872469305992 Saving model 0\nEpoch 810, Loss: 0.0004491587169468403\nValidation Loss improved to 0.010707203298807144 Saving model 0\nEpoch 811, Loss: 0.000467292353278026\nValidation Loss improved to 0.010704545304179192 Saving model 0\nEpoch 812, Loss: 0.0004500129434745759\nValidation Loss improved to 0.010701789520680904 Saving model 0\nEpoch 813, Loss: 0.00039046109304763377\nValidation Loss improved to 0.010698981583118439 Saving model 0\nEpoch 814, Loss: 0.000511718331836164\nValidation Loss improved to 0.010696270503103733 Saving model 0\nEpoch 815, Loss: 0.000791724247392267\nValidation Loss improved to 0.010693749412894249 Saving model 0\nEpoch 816, Loss: 0.0005114763625897467\nValidation Loss improved to 0.010690947994589806 Saving model 0\nEpoch 817, Loss: 0.0005149257485754788\nValidation Loss improved to 0.010688208043575287 Saving model 0\nEpoch 818, Loss: 0.0005448261508718133\nValidation Loss improved to 0.010685492306947708 Saving model 0\nEpoch 819, Loss: 0.0003833460505120456\nValidation Loss improved to 0.01068283710628748 Saving model 0\nEpoch 820, Loss: 0.00045660577598027885\nValidation Loss improved to 0.01067989319562912 Saving model 0\nEpoch 821, Loss: 0.0005344094824977219\nValidation Loss improved to 0.010677347891032696 Saving model 0\nEpoch 822, Loss: 0.0005591531516984105\nValidation Loss improved to 0.010674522258341312 Saving model 0\nEpoch 823, Loss: 0.0005436346982605755\nValidation Loss improved to 0.010671781376004219 Saving model 0\nEpoch 824, Loss: 0.00043412618106231093\nValidation Loss improved to 0.010669098235666752 Saving model 0\nEpoch 825, Loss: 0.00043348586768843234\nValidation Loss improved to 0.01066640391945839 Saving model 0\nEpoch 826, Loss: 0.0003663326206151396\nValidation Loss improved to 0.010663744062185287 Saving model 0\nEpoch 827, Loss: 0.0005993575905449688\nValidation Loss improved to 0.010661100968718529 Saving model 0\nEpoch 828, Loss: 0.0005562108126468956\nValidation Loss improved to 0.010658466257154942 Saving model 0\nEpoch 829, Loss: 0.0005585321341641247\nValidation Loss improved to 0.010655866004526615 Saving model 0\nEpoch 830, Loss: 0.00043064579949714243\nValidation Loss improved to 0.010653266683220863 Saving model 0\nEpoch 831, Loss: 0.0004333771939855069\nValidation Loss improved to 0.010650666430592537 Saving model 0\nEpoch 832, Loss: 0.0008463513804599643\nValidation Loss improved to 0.010647849179804325 Saving model 0\nEpoch 833, Loss: 0.0005509248585440218\nValidation Loss improved to 0.010645236819982529 Saving model 0\nEpoch 834, Loss: 0.00038420830969698727\nValidation Loss improved to 0.010642524808645248 Saving model 0\nEpoch 835, Loss: 0.00038194682565517724\nValidation Loss improved to 0.010639987885951996 Saving model 0\nEpoch 836, Loss: 0.00042049892363138497\nValidation Loss improved to 0.01063752081245184 Saving model 0\nEpoch 837, Loss: 0.0004154451016802341\nValidation Loss improved to 0.010635029524564743 Saving model 0\nEpoch 838, Loss: 0.0004287205811124295\nValidation Loss improved to 0.010632502846419811 Saving model 0\nEpoch 839, Loss: 0.0003588422550819814\nValidation Loss improved to 0.010630081407725811 Saving model 0\nEpoch 840, Loss: 0.0004183021083008498\nValidation Loss improved to 0.010627522133290768 Saving model 0\nEpoch 841, Loss: 0.0004122653917875141\nValidation Loss improved to 0.01062482688575983 Saving model 0\nEpoch 842, Loss: 0.00042971130460500717\nValidation Loss improved to 0.010622153989970684 Saving model 0\nEpoch 843, Loss: 0.00045200882595963776\nValidation Loss improved to 0.010619624517858028 Saving model 0\nEpoch 844, Loss: 0.0005005258717574179\nValidation Loss improved to 0.010616952553391457 Saving model 0\nEpoch 845, Loss: 0.0004511874576564878\nValidation Loss improved to 0.010614377446472645 Saving model 0\nEpoch 846, Loss: 0.0004181014664936811\nValidation Loss improved to 0.010611746460199356 Saving model 0\nEpoch 847, Loss: 0.00045140893780626357\nValidation Loss improved to 0.01060904748737812 Saving model 0\nEpoch 848, Loss: 0.00040365997119806707\nValidation Loss improved to 0.01060661394149065 Saving model 0\nEpoch 849, Loss: 0.00035978216328658164\nValidation Loss improved to 0.010603970848023891 Saving model 0\nEpoch 850, Loss: 0.000403060665121302\nValidation Loss improved to 0.010601448826491833 Saving model 0\nEpoch 851, Loss: 0.0007256182143464684\nValidation Loss improved to 0.010598660446703434 Saving model 0\nEpoch 852, Loss: 0.0005188180948607624\nValidation Loss improved to 0.010596116073429585 Saving model 0\nEpoch 853, Loss: 0.00047884625382721424\nValidation Loss improved to 0.010593423619866371 Saving model 0\nEpoch 854, Loss: 0.00045933062210679054\nValidation Loss improved to 0.010590841993689537 Saving model 0\nEpoch 855, Loss: 0.0003580011543817818\nValidation Loss improved to 0.010588346049189568 Saving model 0\nEpoch 856, Loss: 0.000546407769434154\nValidation Loss improved to 0.010585666634142399 Saving model 0\nEpoch 857, Loss: 0.0003773868374992162\nValidation Loss improved to 0.010583239607512951 Saving model 0\nEpoch 858, Loss: 0.00040556341991759837\nValidation Loss improved to 0.010580788366496563 Saving model 0\nEpoch 859, Loss: 0.00046115071745589375\nValidation Loss improved to 0.010578159242868423 Saving model 0\nEpoch 860, Loss: 0.0005177786806598306\nValidation Loss improved to 0.01057574711740017 Saving model 0\nEpoch 861, Loss: 0.000570627860724926\nValidation Loss improved to 0.010573047213256359 Saving model 0\nEpoch 862, Loss: 0.0004797307192347944\nValidation Loss improved to 0.010570499114692211 Saving model 0\nEpoch 863, Loss: 0.00040374096715822816\nValidation Loss improved to 0.010568005032837391 Saving model 0\nEpoch 864, Loss: 0.0003941427858080715\nValidation Loss improved to 0.010565459728240967 Saving model 0\nEpoch 865, Loss: 0.00038407614920288324\nValidation Loss improved to 0.010562852025032043 Saving model 0\nEpoch 866, Loss: 0.0005324575467966497\nValidation Loss improved to 0.01056033093482256 Saving model 0\nEpoch 867, Loss: 0.0003867044870276004\nValidation Loss improved to 0.010558047331869602 Saving model 0\nEpoch 868, Loss: 0.0005297892494127154\nValidation Loss improved to 0.010555426590144634 Saving model 0\nEpoch 869, Loss: 0.0005344563978724182\nValidation Loss improved to 0.010552960447967052 Saving model 0\nEpoch 870, Loss: 0.00044193811481818557\nValidation Loss improved to 0.010550552047789097 Saving model 0\nEpoch 871, Loss: 0.0004819439200218767\nValidation Loss improved to 0.010548007674515247 Saving model 0\nEpoch 872, Loss: 0.0004506981640588492\nValidation Loss improved to 0.010545634664595127 Saving model 0\nEpoch 873, Loss: 0.0006029843352735043\nValidation Loss improved to 0.010543077252805233 Saving model 0\nEpoch 874, Loss: 0.0004387809894979\nValidation Loss improved to 0.010540547780692577 Saving model 0\nEpoch 875, Loss: 0.0004124612605664879\nValidation Loss improved to 0.010538110509514809 Saving model 0\nEpoch 876, Loss: 0.00039358914364129305\nValidation Loss improved to 0.010535640642046928 Saving model 0\nEpoch 877, Loss: 0.0003773009520955384\nValidation Loss improved to 0.010533316992223263 Saving model 0\nEpoch 878, Loss: 0.00041139620589092374\nValidation Loss improved to 0.010530872270464897 Saving model 0\nEpoch 879, Loss: 0.00039711323915980756\nValidation Loss improved to 0.01052861288189888 Saving model 0\nEpoch 880, Loss: 0.00044190630433149636\nValidation Loss improved to 0.010526350699365139 Saving model 0\nEpoch 881, Loss: 0.0005564876482822001\nValidation Loss improved to 0.010524031706154346 Saving model 0\nEpoch 882, Loss: 0.000411553104640916\nValidation Loss improved to 0.010521751828491688 Saving model 0\nEpoch 883, Loss: 0.0005747946561314166\nValidation Loss improved to 0.01051933504641056 Saving model 0\nEpoch 884, Loss: 0.00047046493273228407\nValidation Loss improved to 0.010517029091715813 Saving model 0\nEpoch 885, Loss: 0.0005642054020427167\nValidation Loss improved to 0.010514844208955765 Saving model 0\nEpoch 886, Loss: 0.0005503988359123468\nValidation Loss improved to 0.010512267239391804 Saving model 0\nEpoch 887, Loss: 0.0006086629582569003\nValidation Loss improved to 0.010509849525988102 Saving model 0\nEpoch 888, Loss: 0.0006762988050468266\nValidation Loss improved to 0.010507344268262386 Saving model 0\nEpoch 889, Loss: 0.0006000321009196341\nValidation Loss improved to 0.010504947043955326 Saving model 0\nEpoch 890, Loss: 0.0005745444213971496\nValidation Loss improved to 0.010502705350518227 Saving model 0\nEpoch 891, Loss: 0.0005113306688144803\nValidation Loss improved to 0.010500350035727024 Saving model 0\nEpoch 892, Loss: 0.0004987035063095391\nValidation Loss improved to 0.010498066432774067 Saving model 0\nEpoch 893, Loss: 0.0004124956321902573\nValidation Loss improved to 0.01049572043120861 Saving model 0\nEpoch 894, Loss: 0.00043791657662950456\nValidation Loss improved to 0.010493521578609943 Saving model 0\nEpoch 895, Loss: 0.0005180526641197503\nValidation Loss improved to 0.010491209104657173 Saving model 0\nEpoch 896, Loss: 0.00045970367500558496\nValidation Loss improved to 0.010488896630704403 Saving model 0\nEpoch 897, Loss: 0.0003557461022865027\nValidation Loss improved to 0.01048657950013876 Saving model 0\nEpoch 898, Loss: 0.0003934892301913351\nValidation Loss improved to 0.010484324768185616 Saving model 0\nEpoch 899, Loss: 0.0005043178098276258\nValidation Loss improved to 0.010481974110007286 Saving model 0\nEpoch 900, Loss: 0.000472310115583241\nValidation Loss improved to 0.01047962810844183 Saving model 0\nEpoch 901, Loss: 0.00043356072274036705\nValidation Loss improved to 0.010477366857230663 Saving model 0\nEpoch 902, Loss: 0.00040942372288554907\nValidation Loss improved to 0.010475053451955318 Saving model 0\nEpoch 903, Loss: 0.0004830146790482104\nValidation Loss improved to 0.010472851805388927 Saving model 0\nEpoch 904, Loss: 0.0004990283050574362\nValidation Loss improved to 0.010470618493855 Saving model 0\nEpoch 905, Loss: 0.0004128052678424865\nValidation Loss improved to 0.010468457825481892 Saving model 0\nEpoch 906, Loss: 0.0004946260014548898\nValidation Loss improved to 0.010466251522302628 Saving model 0\nEpoch 907, Loss: 0.0003670237201731652\nValidation Loss improved to 0.010464224964380264 Saving model 0\nEpoch 908, Loss: 0.0005011744797229767\nValidation Loss improved to 0.010462154634296894 Saving model 0\nEpoch 909, Loss: 0.0004910207353532314\nValidation Loss improved to 0.01045982725918293 Saving model 0\nEpoch 910, Loss: 0.0004450109554454684\nValidation Loss improved to 0.010457584634423256 Saving model 0\nEpoch 911, Loss: 0.00045203324407339096\nValidation Loss improved to 0.010455334559082985 Saving model 0\nEpoch 912, Loss: 0.00036021077539771795\nValidation Loss improved to 0.01045314222574234 Saving model 0\nEpoch 913, Loss: 0.0005097208195365965\nValidation Loss improved to 0.010450921021401882 Saving model 0\nEpoch 914, Loss: 0.00045846105786040425\nValidation Loss improved to 0.010448700748383999 Saving model 0\nEpoch 915, Loss: 0.0005898879026062787\nValidation Loss improved to 0.010446527972817421 Saving model 0\nEpoch 916, Loss: 0.0004173013148829341\nValidation Loss improved to 0.010444323532283306 Saving model 0\nEpoch 917, Loss: 0.00042509305058047175\nValidation Loss improved to 0.010442116297781467 Saving model 0\nEpoch 918, Loss: 0.00046179164201021194\nValidation Loss improved to 0.01044006273150444 Saving model 0\nEpoch 919, Loss: 0.0005012117908336222\nValidation Loss improved to 0.010437707416713238 Saving model 0\nEpoch 920, Loss: 0.00044825326767750084\nValidation Loss improved to 0.01043557096272707 Saving model 0\nEpoch 921, Loss: 0.0004629405157174915\nValidation Loss improved to 0.010433318093419075 Saving model 0\nEpoch 922, Loss: 0.0004381959151942283\nValidation Loss improved to 0.010431138798594475 Saving model 0\nEpoch 923, Loss: 0.0005004609120078385\nValidation Loss improved to 0.01042875275015831 Saving model 0\nEpoch 924, Loss: 0.0005624625482596457\nValidation Loss improved to 0.010426587425172329 Saving model 0\nEpoch 925, Loss: 0.0005126327741891146\nValidation Loss improved to 0.010424227453768253 Saving model 0\nEpoch 926, Loss: 0.0004475016030482948\nValidation Loss improved to 0.010422058403491974 Saving model 0\nEpoch 927, Loss: 0.0005583144375123084\nValidation Loss improved to 0.010419909842312336 Saving model 0\nEpoch 928, Loss: 0.0006760589312762022\nValidation Loss improved to 0.010418307036161423 Saving model 0\nEpoch 929, Loss: 0.0007950154249556363\nValidation Loss improved to 0.010416585952043533 Saving model 0\nEpoch 930, Loss: 0.0005123402224853635\nValidation Loss improved to 0.010414378717541695 Saving model 0\nEpoch 931, Loss: 0.0004059523926116526\nValidation Loss improved to 0.01041227113455534 Saving model 0\nEpoch 932, Loss: 0.0004752483800984919\nValidation Loss improved to 0.010410194285213947 Saving model 0\nEpoch 933, Loss: 0.0006907735951244831\nValidation Loss improved to 0.010408093221485615 Saving model 0\nEpoch 934, Loss: 0.0005331597640179098\nValidation Loss improved to 0.010405945591628551 Saving model 0\nEpoch 935, Loss: 0.0005184544716030359\nValidation Loss improved to 0.010403662919998169 Saving model 0\nEpoch 936, Loss: 0.0005187916685827076\nValidation Loss improved to 0.0104015888646245 Saving model 0\nEpoch 937, Loss: 0.0004342686152085662\nValidation Loss improved to 0.010399391874670982 Saving model 0\nEpoch 938, Loss: 0.0004050190909765661\nValidation Loss improved to 0.010397215373814106 Saving model 0\nEpoch 939, Loss: 0.00044755800627171993\nValidation Loss improved to 0.010394979268312454 Saving model 0\nEpoch 940, Loss: 0.0003838254197034985\nValidation Loss improved to 0.010392877273261547 Saving model 0\nEpoch 941, Loss: 0.0004172596090938896\nValidation Loss improved to 0.010390850715339184 Saving model 0\nEpoch 942, Loss: 0.0005176383419893682\nValidation Loss improved to 0.010388830676674843 Saving model 0\nEpoch 943, Loss: 0.00051816989434883\nValidation Loss improved to 0.010386700741946697 Saving model 0\nEpoch 944, Loss: 0.00042743506492115557\nValidation Loss improved to 0.010384644381701946 Saving model 0\nEpoch 945, Loss: 0.000371391826774925\nValidation Loss improved to 0.010382543317973614 Saving model 0\nEpoch 946, Loss: 0.000386560132028535\nValidation Loss improved to 0.01038034725934267 Saving model 0\nEpoch 947, Loss: 0.00037197303026914597\nValidation Loss improved to 0.010378222912549973 Saving model 0\nEpoch 948, Loss: 0.000566273694857955\nValidation Loss improved to 0.010376262478530407 Saving model 0\nEpoch 949, Loss: 0.0006822746363468468\nValidation Loss improved to 0.0103741604834795 Saving model 0\nEpoch 950, Loss: 0.00041128916200250387\nValidation Loss improved to 0.0103721022605896 Saving model 0\nEpoch 951, Loss: 0.0003919930022675544\nValidation Loss improved to 0.010370040312409401 Saving model 0\nEpoch 952, Loss: 0.00048026416334323585\nValidation Loss improved to 0.010368200950324535 Saving model 0\nEpoch 953, Loss: 0.0005844530533067882\nValidation Loss improved to 0.010366314090788364 Saving model 0\nEpoch 954, Loss: 0.00045542942825704813\nValidation Loss improved to 0.01036426518112421 Saving model 0\nEpoch 955, Loss: 0.0005028372979722917\nValidation Loss improved to 0.010362287051975727 Saving model 0\nEpoch 956, Loss: 0.0005521401180885732\nValidation Loss improved to 0.010360310785472393 Saving model 0\nEpoch 957, Loss: 0.0005154396058060229\nValidation Loss improved to 0.010358292609453201 Saving model 0\nEpoch 958, Loss: 0.00038770140963606536\nValidation Loss improved to 0.010356370359659195 Saving model 0\nEpoch 959, Loss: 0.0003857187693938613\nValidation Loss improved to 0.01035438384860754 Saving model 0\nEpoch 960, Loss: 0.0003524480271153152\nValidation Loss improved to 0.010352431796491146 Saving model 0\nEpoch 961, Loss: 0.0005486024892888963\nValidation Loss improved to 0.010350318625569344 Saving model 0\nEpoch 962, Loss: 0.0004357571597211063\nValidation Loss improved to 0.010348256677389145 Saving model 0\nEpoch 963, Loss: 0.0004860787303186953\nValidation Loss improved to 0.01034619752317667 Saving model 0\nEpoch 964, Loss: 0.0005915741203352809\nValidation Loss improved to 0.010344046168029308 Saving model 0\nEpoch 965, Loss: 0.0005726456874981523\nValidation Loss improved to 0.010342004708945751 Saving model 0\nEpoch 966, Loss: 0.0004839340690523386\nValidation Loss improved to 0.010339940898120403 Saving model 0\nEpoch 967, Loss: 0.00047728916979394853\nValidation Loss improved to 0.01033783983439207 Saving model 0\nEpoch 968, Loss: 0.0004969096044078469\nValidation Loss improved to 0.010335840284824371 Saving model 0\nEpoch 969, Loss: 0.0005377241177484393\nValidation Loss improved to 0.010333946906030178 Saving model 0\nEpoch 970, Loss: 0.0005900096148252487\nValidation Loss improved to 0.01033209078013897 Saving model 0\nEpoch 971, Loss: 0.0004054366727359593\nValidation Loss improved to 0.010330154560506344 Saving model 0\nEpoch 972, Loss: 0.000517933745868504\nValidation Loss improved to 0.010328082367777824 Saving model 0\nEpoch 973, Loss: 0.0005465339054353535\nValidation Loss improved to 0.010326250456273556 Saving model 0\nEpoch 974, Loss: 0.0005425448180176318\nValidation Loss improved to 0.010324304923415184 Saving model 0\nEpoch 975, Loss: 0.0004713006201200187\nValidation Loss improved to 0.010322378017008305 Saving model 0\nEpoch 976, Loss: 0.0004296645929571241\nValidation Loss improved to 0.010320360772311687 Saving model 0\nEpoch 977, Loss: 0.000473312713438645\nValidation Loss improved to 0.010318408720195293 Saving model 0\nEpoch 978, Loss: 0.00044237973634153605\nValidation Loss improved to 0.010316372849047184 Saving model 0\nEpoch 979, Loss: 0.0004902246873825788\nValidation Loss improved to 0.010314571671187878 Saving model 0\nEpoch 980, Loss: 0.0005527902976609766\nValidation Loss improved to 0.010312684811651707 Saving model 0\nEpoch 981, Loss: 0.00046279665548354387\nValidation Loss improved to 0.010310735553503036 Saving model 0\nEpoch 982, Loss: 0.00035741287865675986\nValidation Loss improved to 0.01030903123319149 Saving model 0\nEpoch 983, Loss: 0.0005989336641505361\nValidation Loss improved to 0.010307232849299908 Saving model 0\nEpoch 984, Loss: 0.00043902252218686044\nValidation Loss improved to 0.010305151343345642 Saving model 0\nEpoch 985, Loss: 0.00042613863479346037\nValidation Loss improved to 0.010303224436938763 Saving model 0\nEpoch 986, Loss: 0.0005904211429879069\nValidation Loss improved to 0.010301281698048115 Saving model 0\nEpoch 987, Loss: 0.000723285716958344\nValidation Loss improved to 0.01029956340789795 Saving model 0\nEpoch 988, Loss: 0.0004777119029313326\nValidation Loss improved to 0.010297711007297039 Saving model 0\nEpoch 989, Loss: 0.0003427455958444625\nValidation Loss improved to 0.010295804589986801 Saving model 0\nEpoch 990, Loss: 0.00044088094728067517\nValidation Loss improved to 0.010293941013514996 Saving model 0\nEpoch 991, Loss: 0.0003890529042109847\nValidation Loss improved to 0.010292031802237034 Saving model 0\nEpoch 992, Loss: 0.00047877957695163786\nValidation Loss improved to 0.010290184058248997 Saving model 0\nEpoch 993, Loss: 0.0005785679677501321\nValidation Loss improved to 0.010288402438163757 Saving model 0\nEpoch 994, Loss: 0.00047401321353390813\nValidation Loss improved to 0.010286571457982063 Saving model 0\nEpoch 995, Loss: 0.0004714015231002122\nValidation Loss improved to 0.010284578427672386 Saving model 0\nEpoch 996, Loss: 0.00047179849934764206\nValidation Loss improved to 0.010282756760716438 Saving model 0\nEpoch 997, Loss: 0.00046781188575550914\nValidation Loss improved to 0.010281037539243698 Saving model 0\nEpoch 998, Loss: 0.00046304502757266164\nValidation Loss improved to 0.010279141366481781 Saving model 0\nEpoch 999, Loss: 0.00047306300257332623\nValidation Loss improved to 0.010277383029460907 Saving model 0\nEpoch 0, Loss: 0.22313421964645386\nValidation Loss improved to 0.10054945945739746 Saving model 1\nEpoch 1, Loss: 0.10642844438552856\nValidation Loss improved to 0.0800362303853035 Saving model 1\nEpoch 2, Loss: 0.06907029449939728\nValidation Loss improved to 0.0776335597038269 Saving model 1\nEpoch 3, Loss: 0.04084207862615585\nValidation Loss improved to 0.06737930327653885 Saving model 1\nEpoch 4, Loss: 0.024507174268364906\nValidation Loss improved to 0.058192163705825806 Saving model 1\nEpoch 5, Loss: 0.01870790868997574\nValidation Loss improved to 0.055298615247011185 Saving model 1\nEpoch 6, Loss: 0.01498748455196619\nValidation Loss improved to 0.05024827644228935 Saving model 1\nEpoch 7, Loss: 0.016803739592432976\nValidation Loss improved to 0.04733356460928917 Saving model 1\nEpoch 8, Loss: 0.023936308920383453\nValidation Loss improved to 0.04555227607488632 Saving model 1\nEpoch 9, Loss: 0.021472252905368805\nValidation Loss improved to 0.043660007417201996 Saving model 1\nEpoch 10, Loss: 0.017342079430818558\nValidation Loss improved to 0.04214191809296608 Saving model 1\nEpoch 11, Loss: 0.013945077545940876\nValidation Loss improved to 0.04050168767571449 Saving model 1\nEpoch 12, Loss: 0.011922569945454597\nValidation Loss improved to 0.038836054503917694 Saving model 1\nEpoch 13, Loss: 0.019407352432608604\nValidation Loss improved to 0.03785257786512375 Saving model 1\nEpoch 14, Loss: 0.01768031343817711\nValidation Loss improved to 0.03689339756965637 Saving model 1\nEpoch 15, Loss: 0.015233438462018967\nEpoch 16, Loss: 0.018114695325493813\nValidation Loss improved to 0.03598544001579285 Saving model 1\nEpoch 17, Loss: 0.013228103518486023\nValidation Loss improved to 0.03528982400894165 Saving model 1\nEpoch 18, Loss: 0.014210294932126999\nValidation Loss improved to 0.03463418036699295 Saving model 1\nEpoch 19, Loss: 0.015581237152218819\nValidation Loss improved to 0.033769551664590836 Saving model 1\nEpoch 20, Loss: 0.013351713307201862\nValidation Loss improved to 0.03316181153059006 Saving model 1\nEpoch 21, Loss: 0.011630956083536148\nValidation Loss improved to 0.032406195998191833 Saving model 1\nEpoch 22, Loss: 0.011969823390245438\nValidation Loss improved to 0.03204828500747681 Saving model 1\nEpoch 23, Loss: 0.011908426880836487\nValidation Loss improved to 0.03152014687657356 Saving model 1\nEpoch 24, Loss: 0.0101225720718503\nValidation Loss improved to 0.031166015192866325 Saving model 1\nEpoch 25, Loss: 0.010564063675701618\nValidation Loss improved to 0.030655020847916603 Saving model 1\nEpoch 26, Loss: 0.010016456246376038\nValidation Loss improved to 0.03036171942949295 Saving model 1\nEpoch 27, Loss: 0.009189676493406296\nValidation Loss improved to 0.029948661103844643 Saving model 1\nEpoch 28, Loss: 0.020187631249427795\nValidation Loss improved to 0.029581576585769653 Saving model 1\nEpoch 29, Loss: 0.012452257797122002\nValidation Loss improved to 0.029104074463248253 Saving model 1\nEpoch 30, Loss: 0.00953608937561512\nValidation Loss improved to 0.02863459102809429 Saving model 1\nEpoch 31, Loss: 0.010351133532822132\nValidation Loss improved to 0.028559884056448936 Saving model 1\nEpoch 32, Loss: 0.00913860835134983\nValidation Loss improved to 0.028184950351715088 Saving model 1\nEpoch 33, Loss: 0.008579077199101448\nValidation Loss improved to 0.027876954525709152 Saving model 1\nEpoch 34, Loss: 0.009251268580555916\nValidation Loss improved to 0.02773800492286682 Saving model 1\nEpoch 35, Loss: 0.01261228509247303\nEpoch 36, Loss: 0.013106304220855236\nValidation Loss improved to 0.02771463431417942 Saving model 1\nEpoch 37, Loss: 0.01061372458934784\nValidation Loss improved to 0.02749679423868656 Saving model 1\nEpoch 38, Loss: 0.006753520108759403\nValidation Loss improved to 0.027323292568325996 Saving model 1\nEpoch 39, Loss: 0.007937947288155556\nValidation Loss improved to 0.027064671739935875 Saving model 1\nEpoch 40, Loss: 0.009853418916463852\nValidation Loss improved to 0.026834838092327118 Saving model 1\nEpoch 41, Loss: 0.007973775267601013\nValidation Loss improved to 0.026567505672574043 Saving model 1\nEpoch 42, Loss: 0.014301365241408348\nValidation Loss improved to 0.02617223933339119 Saving model 1\nEpoch 43, Loss: 0.009264063090085983\nValidation Loss improved to 0.026003185659646988 Saving model 1\nEpoch 44, Loss: 0.009420768357813358\nValidation Loss improved to 0.025668567046523094 Saving model 1\nEpoch 45, Loss: 0.009261033497750759\nValidation Loss improved to 0.025525741279125214 Saving model 1\nEpoch 46, Loss: 0.011113499291241169\nValidation Loss improved to 0.0251951664686203 Saving model 1\nEpoch 47, Loss: 0.012702086009085178\nEpoch 48, Loss: 0.020446140319108963\nEpoch 49, Loss: 0.01520907785743475\nEpoch 50, Loss: 0.008748003281652927\nValidation Loss improved to 0.025124138221144676 Saving model 1\nEpoch 51, Loss: 0.010143894702196121\nEpoch 52, Loss: 0.010254857130348682\nValidation Loss improved to 0.024832721799612045 Saving model 1\nEpoch 53, Loss: 0.007366063538938761\nValidation Loss improved to 0.024630805477499962 Saving model 1\nEpoch 54, Loss: 0.007629885338246822\nValidation Loss improved to 0.02439500018954277 Saving model 1\nEpoch 55, Loss: 0.009515241719782352\nEpoch 56, Loss: 0.015034008771181107\nEpoch 57, Loss: 0.009258253499865532\nValidation Loss improved to 0.02434651181101799 Saving model 1\nEpoch 58, Loss: 0.005751654971390963\nValidation Loss improved to 0.024275775998830795 Saving model 1\nEpoch 59, Loss: 0.007586224935948849\nValidation Loss improved to 0.024087173864245415 Saving model 1\nEpoch 60, Loss: 0.008310046046972275\nValidation Loss improved to 0.0238350797444582 Saving model 1\nEpoch 61, Loss: 0.010513816960155964\nEpoch 62, Loss: 0.00931573286652565\nValidation Loss improved to 0.023737354204058647 Saving model 1\nEpoch 63, Loss: 0.007913624867796898\nValidation Loss improved to 0.023563552647829056 Saving model 1\nEpoch 64, Loss: 0.007296200841665268\nValidation Loss improved to 0.023382574319839478 Saving model 1\nEpoch 65, Loss: 0.008154983632266521\nValidation Loss improved to 0.023255998268723488 Saving model 1\nEpoch 66, Loss: 0.006956757046282291\nValidation Loss improved to 0.023026160895824432 Saving model 1\nEpoch 67, Loss: 0.006594611797481775\nValidation Loss improved to 0.022886140272021294 Saving model 1\nEpoch 68, Loss: 0.005693286191672087\nValidation Loss improved to 0.022759823128581047 Saving model 1\nEpoch 69, Loss: 0.01120903342962265\nValidation Loss improved to 0.02267545461654663 Saving model 1\nEpoch 70, Loss: 0.009035348892211914\nValidation Loss improved to 0.02251588925719261 Saving model 1\nEpoch 71, Loss: 0.008035904727876186\nValidation Loss improved to 0.022376421838998795 Saving model 1\nEpoch 72, Loss: 0.009758521802723408\nValidation Loss improved to 0.02227536216378212 Saving model 1\nEpoch 73, Loss: 0.008585471659898758\nValidation Loss improved to 0.022134121507406235 Saving model 1\nEpoch 74, Loss: 0.006038254126906395\nValidation Loss improved to 0.02206440456211567 Saving model 1\nEpoch 75, Loss: 0.006977301090955734\nValidation Loss improved to 0.021919459104537964 Saving model 1\nEpoch 76, Loss: 0.011120649985969067\nValidation Loss improved to 0.0218803808093071 Saving model 1\nEpoch 77, Loss: 0.010781467892229557\nValidation Loss improved to 0.0217279139906168 Saving model 1\nEpoch 78, Loss: 0.008705909363925457\nValidation Loss improved to 0.021623767912387848 Saving model 1\nEpoch 79, Loss: 0.0074517978355288506\nValidation Loss improved to 0.021538205444812775 Saving model 1\nEpoch 80, Loss: 0.006809084676206112\nValidation Loss improved to 0.02150990255177021 Saving model 1\nEpoch 81, Loss: 0.005871674511581659\nValidation Loss improved to 0.021436311304569244 Saving model 1\nEpoch 82, Loss: 0.005663692485541105\nValidation Loss improved to 0.021345017477869987 Saving model 1\nEpoch 83, Loss: 0.0077603403478860855\nValidation Loss improved to 0.021314745768904686 Saving model 1\nEpoch 84, Loss: 0.008186747319996357\nValidation Loss improved to 0.02124950848519802 Saving model 1\nEpoch 85, Loss: 0.007158149499446154\nValidation Loss improved to 0.021100550889968872 Saving model 1\nEpoch 86, Loss: 0.004890487063676119\nValidation Loss improved to 0.021010058000683784 Saving model 1\nEpoch 87, Loss: 0.005746637005358934\nValidation Loss improved to 0.02098611555993557 Saving model 1\nEpoch 88, Loss: 0.0069093541242182255\nValidation Loss improved to 0.020889565348625183 Saving model 1\nEpoch 89, Loss: 0.008149312809109688\nValidation Loss improved to 0.020804153755307198 Saving model 1\nEpoch 90, Loss: 0.007768718991428614\nValidation Loss improved to 0.02072492055594921 Saving model 1\nEpoch 91, Loss: 0.008945876732468605\nValidation Loss improved to 0.020617729052901268 Saving model 1\nEpoch 92, Loss: 0.01132649090141058\nValidation Loss improved to 0.020533788949251175 Saving model 1\nEpoch 93, Loss: 0.006202711258083582\nValidation Loss improved to 0.020465657114982605 Saving model 1\nEpoch 94, Loss: 0.0072128078900277615\nValidation Loss improved to 0.020361410453915596 Saving model 1\nEpoch 95, Loss: 0.006941663101315498\nEpoch 96, Loss: 0.00719979964196682\nValidation Loss improved to 0.020265843719244003 Saving model 1\nEpoch 97, Loss: 0.008911415934562683\nValidation Loss improved to 0.020239194855093956 Saving model 1\nEpoch 98, Loss: 0.006132572889328003\nValidation Loss improved to 0.02020532265305519 Saving model 1\nEpoch 99, Loss: 0.007918601855635643\nValidation Loss improved to 0.020072130486369133 Saving model 1\nEpoch 100, Loss: 0.008179337717592716\nValidation Loss improved to 0.01997189410030842 Saving model 1\nEpoch 101, Loss: 0.006522091571241617\nValidation Loss improved to 0.019922524690628052 Saving model 1\nEpoch 102, Loss: 0.006029194686561823\nValidation Loss improved to 0.01981363445520401 Saving model 1\nEpoch 103, Loss: 0.005305144470185041\nValidation Loss improved to 0.019739076495170593 Saving model 1\nEpoch 104, Loss: 0.009388655424118042\nValidation Loss improved to 0.019704660400748253 Saving model 1\nEpoch 105, Loss: 0.006875091698020697\nValidation Loss improved to 0.019664060324430466 Saving model 1\nEpoch 106, Loss: 0.005218476988375187\nValidation Loss improved to 0.01961572654545307 Saving model 1\nEpoch 107, Loss: 0.0049011679366230965\nValidation Loss improved to 0.019566986709833145 Saving model 1\nEpoch 108, Loss: 0.010836738161742687\nValidation Loss improved to 0.019487803801894188 Saving model 1\nEpoch 109, Loss: 0.008936861529946327\nValidation Loss improved to 0.01947084628045559 Saving model 1\nEpoch 110, Loss: 0.0066148750483989716\nValidation Loss improved to 0.019457343965768814 Saving model 1\nEpoch 111, Loss: 0.0071721370331943035\nEpoch 112, Loss: 0.0064770132303237915\nValidation Loss improved to 0.0194243174046278 Saving model 1\nEpoch 113, Loss: 0.006599480286240578\nValidation Loss improved to 0.019374992698431015 Saving model 1\nEpoch 114, Loss: 0.004953152034431696\nValidation Loss improved to 0.01928885467350483 Saving model 1\nEpoch 115, Loss: 0.00563009362667799\nValidation Loss improved to 0.019226130098104477 Saving model 1\nEpoch 116, Loss: 0.005864715203642845\nValidation Loss improved to 0.01920822076499462 Saving model 1\nEpoch 117, Loss: 0.006709577050060034\nValidation Loss improved to 0.019154099747538567 Saving model 1\nEpoch 118, Loss: 0.0036685005761682987\nValidation Loss improved to 0.019116276875138283 Saving model 1\nEpoch 119, Loss: 0.005653472151607275\nValidation Loss improved to 0.019027110189199448 Saving model 1\nEpoch 120, Loss: 0.010195687413215637\nValidation Loss improved to 0.01895381696522236 Saving model 1\nEpoch 121, Loss: 0.00579562783241272\nValidation Loss improved to 0.018928123638033867 Saving model 1\nEpoch 122, Loss: 0.004088145215064287\nValidation Loss improved to 0.018889140337705612 Saving model 1\nEpoch 123, Loss: 0.005288243293762207\nValidation Loss improved to 0.018832944333553314 Saving model 1\nEpoch 124, Loss: 0.005508963949978352\nValidation Loss improved to 0.018752891570329666 Saving model 1\nEpoch 125, Loss: 0.007366955745965242\nValidation Loss improved to 0.018716085702180862 Saving model 1\nEpoch 126, Loss: 0.006288962904363871\nValidation Loss improved to 0.018661223351955414 Saving model 1\nEpoch 127, Loss: 0.008033393882215023\nValidation Loss improved to 0.01860225945711136 Saving model 1\nEpoch 128, Loss: 0.006034740712493658\nValidation Loss improved to 0.018552375957369804 Saving model 1\nEpoch 129, Loss: 0.004685575142502785\nValidation Loss improved to 0.018481338396668434 Saving model 1\nEpoch 130, Loss: 0.005524851847440004\nValidation Loss improved to 0.01844845898449421 Saving model 1\nEpoch 131, Loss: 0.0060943965800106525\nValidation Loss improved to 0.018408717587590218 Saving model 1\nEpoch 132, Loss: 0.004837898537516594\nValidation Loss improved to 0.018388794735074043 Saving model 1\nEpoch 133, Loss: 0.007691909093409777\nEpoch 134, Loss: 0.006887857802212238\nValidation Loss improved to 0.01834096945822239 Saving model 1\nEpoch 135, Loss: 0.008195298723876476\nValidation Loss improved to 0.018299158662557602 Saving model 1\nEpoch 136, Loss: 0.006115433294326067\nValidation Loss improved to 0.0182357020676136 Saving model 1\nEpoch 137, Loss: 0.004873211495578289\nValidation Loss improved to 0.01819034107029438 Saving model 1\nEpoch 138, Loss: 0.004824409261345863\nValidation Loss improved to 0.018165959045290947 Saving model 1\nEpoch 139, Loss: 0.0053812917321920395\nValidation Loss improved to 0.018117142841219902 Saving model 1\nEpoch 140, Loss: 0.0088557293638587\nValidation Loss improved to 0.018080491572618484 Saving model 1\nEpoch 141, Loss: 0.00843302346765995\nValidation Loss improved to 0.018053853884339333 Saving model 1\nEpoch 142, Loss: 0.008737917058169842\nValidation Loss improved to 0.01803945004940033 Saving model 1\nEpoch 143, Loss: 0.006765568163245916\nValidation Loss improved to 0.01796635054051876 Saving model 1\nEpoch 144, Loss: 0.005267236847430468\nValidation Loss improved to 0.017942043021321297 Saving model 1\nEpoch 145, Loss: 0.006220054347068071\nValidation Loss improved to 0.017908578738570213 Saving model 1\nEpoch 146, Loss: 0.005235374439507723\nValidation Loss improved to 0.01787562482059002 Saving model 1\nEpoch 147, Loss: 0.01005420833826065\nValidation Loss improved to 0.017805222421884537 Saving model 1\nEpoch 148, Loss: 0.00898167584091425\nValidation Loss improved to 0.01775280013680458 Saving model 1\nEpoch 149, Loss: 0.007547104265540838\nValidation Loss improved to 0.017699308693408966 Saving model 1\nEpoch 150, Loss: 0.005794374737888575\nValidation Loss improved to 0.017657620832324028 Saving model 1\nEpoch 151, Loss: 0.00528775854036212\nValidation Loss improved to 0.01761370152235031 Saving model 1\nEpoch 152, Loss: 0.0044843703508377075\nValidation Loss improved to 0.0175628699362278 Saving model 1\nEpoch 153, Loss: 0.006054845172911882\nValidation Loss improved to 0.017510943114757538 Saving model 1\nEpoch 154, Loss: 0.00691510085016489\nValidation Loss improved to 0.01746409945189953 Saving model 1\nEpoch 155, Loss: 0.007512243930250406\nValidation Loss improved to 0.017394203692674637 Saving model 1\nEpoch 156, Loss: 0.0073162405751645565\nValidation Loss improved to 0.017373859882354736 Saving model 1\nEpoch 157, Loss: 0.006387534085661173\nValidation Loss improved to 0.017312122508883476 Saving model 1\nEpoch 158, Loss: 0.005210000555962324\nValidation Loss improved to 0.017261996865272522 Saving model 1\nEpoch 159, Loss: 0.004046530928462744\nValidation Loss improved to 0.017218226566910744 Saving model 1\nEpoch 160, Loss: 0.005854672286659479\nValidation Loss improved to 0.017190931364893913 Saving model 1\nEpoch 161, Loss: 0.006006055977195501\nValidation Loss improved to 0.017153650522232056 Saving model 1\nEpoch 162, Loss: 0.0053894901648163795\nValidation Loss improved to 0.017112433910369873 Saving model 1\nEpoch 163, Loss: 0.004760604817420244\nValidation Loss improved to 0.017080139368772507 Saving model 1\nEpoch 164, Loss: 0.00716521218419075\nValidation Loss improved to 0.01704438216984272 Saving model 1\nEpoch 165, Loss: 0.004812871105968952\nValidation Loss improved to 0.017010800540447235 Saving model 1\nEpoch 166, Loss: 0.004513703286647797\nValidation Loss improved to 0.01696261204779148 Saving model 1\nEpoch 167, Loss: 0.004987620748579502\nValidation Loss improved to 0.016897521913051605 Saving model 1\nEpoch 168, Loss: 0.004526711069047451\nValidation Loss improved to 0.016865765675902367 Saving model 1\nEpoch 169, Loss: 0.0060082715936005116\nValidation Loss improved to 0.016852742061018944 Saving model 1\nEpoch 170, Loss: 0.005227802786976099\nValidation Loss improved to 0.01680239848792553 Saving model 1\nEpoch 171, Loss: 0.005350934341549873\nValidation Loss improved to 0.016781259328126907 Saving model 1\nEpoch 172, Loss: 0.00830854568630457\nValidation Loss improved to 0.01674443855881691 Saving model 1\nEpoch 173, Loss: 0.00787700992077589\nValidation Loss improved to 0.016706205904483795 Saving model 1\nEpoch 174, Loss: 0.006771606858819723\nValidation Loss improved to 0.01665332540869713 Saving model 1\nEpoch 175, Loss: 0.005582624115049839\nValidation Loss improved to 0.016633864492177963 Saving model 1\nEpoch 176, Loss: 0.005584105383604765\nValidation Loss improved to 0.016596607863903046 Saving model 1\nEpoch 177, Loss: 0.005909838248044252\nValidation Loss improved to 0.016577476635575294 Saving model 1\nEpoch 178, Loss: 0.006997266784310341\nValidation Loss improved to 0.016539068892598152 Saving model 1\nEpoch 179, Loss: 0.0051649753004312515\nValidation Loss improved to 0.01649215631186962 Saving model 1\nEpoch 180, Loss: 0.0043472894467413425\nValidation Loss improved to 0.016477622091770172 Saving model 1\nEpoch 181, Loss: 0.0034135954920202494\nValidation Loss improved to 0.016442114487290382 Saving model 1\nEpoch 182, Loss: 0.0038513541221618652\nValidation Loss improved to 0.01639929786324501 Saving model 1\nEpoch 183, Loss: 0.006637285463511944\nValidation Loss improved to 0.016349822282791138 Saving model 1\nEpoch 184, Loss: 0.008901895955204964\nValidation Loss improved to 0.01634722575545311 Saving model 1\nEpoch 185, Loss: 0.006743347272276878\nValidation Loss improved to 0.016304947435855865 Saving model 1\nEpoch 186, Loss: 0.0064524454064667225\nValidation Loss improved to 0.016259722411632538 Saving model 1\nEpoch 187, Loss: 0.0058299521915614605\nValidation Loss improved to 0.016223210841417313 Saving model 1\nEpoch 188, Loss: 0.005988942924886942\nValidation Loss improved to 0.01617656648159027 Saving model 1\nEpoch 189, Loss: 0.007908009923994541\nValidation Loss improved to 0.016170684248209 Saving model 1\nEpoch 190, Loss: 0.007716734893620014\nValidation Loss improved to 0.016153668984770775 Saving model 1\nEpoch 191, Loss: 0.0038618971593677998\nValidation Loss improved to 0.016115199774503708 Saving model 1\nEpoch 192, Loss: 0.003465014975517988\nValidation Loss improved to 0.01606924459338188 Saving model 1\nEpoch 193, Loss: 0.004522297065705061\nValidation Loss improved to 0.016016755253076553 Saving model 1\nEpoch 194, Loss: 0.005042906850576401\nValidation Loss improved to 0.015978753566741943 Saving model 1\nEpoch 195, Loss: 0.0041412292048335075\nValidation Loss improved to 0.01594422571361065 Saving model 1\nEpoch 196, Loss: 0.00530750909820199\nValidation Loss improved to 0.015901191160082817 Saving model 1\nEpoch 197, Loss: 0.005824992898851633\nValidation Loss improved to 0.01587178371846676 Saving model 1\nEpoch 198, Loss: 0.007733812090009451\nValidation Loss improved to 0.0158479493111372 Saving model 1\nEpoch 199, Loss: 0.006224385462701321\nValidation Loss improved to 0.015825515612959862 Saving model 1\nEpoch 200, Loss: 0.004854913335293531\nValidation Loss improved to 0.015797384083271027 Saving model 1\nEpoch 201, Loss: 0.004944640211760998\nValidation Loss improved to 0.015773823484778404 Saving model 1\nEpoch 202, Loss: 0.0038167908787727356\nValidation Loss improved to 0.01575865037739277 Saving model 1\nEpoch 203, Loss: 0.00695439986884594\nValidation Loss improved to 0.015754472464323044 Saving model 1\nEpoch 204, Loss: 0.009198346175253391\nEpoch 205, Loss: 0.010821260511875153\nEpoch 206, Loss: 0.007058791350573301\nEpoch 207, Loss: 0.005478649400174618\nValidation Loss improved to 0.015722382813692093 Saving model 1\nEpoch 208, Loss: 0.007959775626659393\nValidation Loss improved to 0.015721846371889114 Saving model 1\nEpoch 209, Loss: 0.007851227186620235\nValidation Loss improved to 0.015696048736572266 Saving model 1\nEpoch 210, Loss: 0.004146536346524954\nValidation Loss improved to 0.01568486914038658 Saving model 1\nEpoch 211, Loss: 0.004296682775020599\nValidation Loss improved to 0.01564803346991539 Saving model 1\nEpoch 212, Loss: 0.006140077020972967\nValidation Loss improved to 0.015614877454936504 Saving model 1\nEpoch 213, Loss: 0.007101274095475674\nValidation Loss improved to 0.0156077416613698 Saving model 1\nEpoch 214, Loss: 0.0062063210643827915\nValidation Loss improved to 0.01557534746825695 Saving model 1\nEpoch 215, Loss: 0.005759966094046831\nValidation Loss improved to 0.015539183281362057 Saving model 1\nEpoch 216, Loss: 0.01369081437587738\nValidation Loss improved to 0.015538429841399193 Saving model 1\nEpoch 217, Loss: 0.007984282448887825\nValidation Loss improved to 0.015500704757869244 Saving model 1\nEpoch 218, Loss: 0.00846413616091013\nValidation Loss improved to 0.015494388528168201 Saving model 1\nEpoch 219, Loss: 0.00859888456761837\nValidation Loss improved to 0.015488569624722004 Saving model 1\nEpoch 220, Loss: 0.005938371643424034\nValidation Loss improved to 0.015474813990294933 Saving model 1\nEpoch 221, Loss: 0.004754301160573959\nValidation Loss improved to 0.015456491149961948 Saving model 1\nEpoch 222, Loss: 0.0041530984453856945\nValidation Loss improved to 0.01543848030269146 Saving model 1\nEpoch 223, Loss: 0.006461698561906815\nValidation Loss improved to 0.015425427816808224 Saving model 1\nEpoch 224, Loss: 0.005103047005832195\nValidation Loss improved to 0.015398329123854637 Saving model 1\nEpoch 225, Loss: 0.005065421108156443\nValidation Loss improved to 0.015380539931356907 Saving model 1\nEpoch 226, Loss: 0.005422674585133791\nEpoch 227, Loss: 0.008736390620470047\nEpoch 228, Loss: 0.005990702658891678\nEpoch 229, Loss: 0.004660532809793949\nEpoch 230, Loss: 0.004859302192926407\nEpoch 231, Loss: 0.005454757250845432\nEpoch 232, Loss: 0.0048884013667702675\nValidation Loss improved to 0.015360853634774685 Saving model 1\nEpoch 233, Loss: 0.004753706511110067\nValidation Loss improved to 0.015345013700425625 Saving model 1\nEpoch 234, Loss: 0.008515776135027409\nEpoch 235, Loss: 0.009021297097206116\nValidation Loss improved to 0.015341686084866524 Saving model 1\nEpoch 236, Loss: 0.007306783460080624\nValidation Loss improved to 0.015321471728384495 Saving model 1\nEpoch 237, Loss: 0.006508173421025276\nEpoch 238, Loss: 0.005515078082680702\nValidation Loss improved to 0.015305263921618462 Saving model 1\nEpoch 239, Loss: 0.0060117593966424465\nValidation Loss improved to 0.015286627225577831 Saving model 1\nEpoch 240, Loss: 0.004010289907455444\nValidation Loss improved to 0.015274863690137863 Saving model 1\nEpoch 241, Loss: 0.0030623935163021088\nValidation Loss improved to 0.015265097841620445 Saving model 1\nEpoch 242, Loss: 0.003679848974570632\nValidation Loss improved to 0.0152484942227602 Saving model 1\nEpoch 243, Loss: 0.0067282565869390965\nValidation Loss improved to 0.015245983377099037 Saving model 1\nEpoch 244, Loss: 0.009898112155497074\nValidation Loss improved to 0.01522720418870449 Saving model 1\nEpoch 245, Loss: 0.012108609080314636\nEpoch 246, Loss: 0.007705591153353453\nValidation Loss improved to 0.015222489833831787 Saving model 1\nEpoch 247, Loss: 0.007708471268415451\nValidation Loss improved to 0.015213081613183022 Saving model 1\nEpoch 248, Loss: 0.004653292242437601\nValidation Loss improved to 0.015209893696010113 Saving model 1\nEpoch 249, Loss: 0.006693603936582804\nValidation Loss improved to 0.01519754994660616 Saving model 1\nEpoch 250, Loss: 0.005227555520832539\nValidation Loss improved to 0.015184711664915085 Saving model 1\nEpoch 251, Loss: 0.006446673069149256\nValidation Loss improved to 0.015165522694587708 Saving model 1\nEpoch 252, Loss: 0.008213946595788002\nEpoch 253, Loss: 0.010391553863883018\nEpoch 254, Loss: 0.0050322613678872585\nValidation Loss improved to 0.015157355926930904 Saving model 1\nEpoch 255, Loss: 0.00533519359305501\nValidation Loss improved to 0.015157200396060944 Saving model 1\nEpoch 256, Loss: 0.005957045126706362\nValidation Loss improved to 0.015137812122702599 Saving model 1\nEpoch 257, Loss: 0.008765090256929398\nEpoch 258, Loss: 0.005479282699525356\nValidation Loss improved to 0.01513593178242445 Saving model 1\nEpoch 259, Loss: 0.004898655228316784\nEpoch 260, Loss: 0.0038296696729958057\nValidation Loss improved to 0.015125857666134834 Saving model 1\nEpoch 261, Loss: 0.0037538190372288227\nEpoch 262, Loss: 0.003461691318079829\nValidation Loss improved to 0.015111482702195644 Saving model 1\nEpoch 263, Loss: 0.005433427635580301\nValidation Loss improved to 0.01510527078062296 Saving model 1\nEpoch 264, Loss: 0.003792472882196307\nValidation Loss improved to 0.015087679028511047 Saving model 1\nEpoch 265, Loss: 0.004293813370168209\nValidation Loss improved to 0.015075321309268475 Saving model 1\nEpoch 266, Loss: 0.004932969342917204\nValidation Loss improved to 0.01505864318460226 Saving model 1\nEpoch 267, Loss: 0.004527291748672724\nEpoch 268, Loss: 0.0032543912529945374\nEpoch 269, Loss: 0.003782668150961399\nValidation Loss improved to 0.015056992881000042 Saving model 1\nEpoch 270, Loss: 0.0055747558362782\nEpoch 271, Loss: 0.007958692498505116\nEpoch 272, Loss: 0.008603944443166256\nValidation Loss improved to 0.01504612434655428 Saving model 1\nEpoch 273, Loss: 0.006824829149991274\nValidation Loss improved to 0.015039181336760521 Saving model 1\nEpoch 274, Loss: 0.006551423575729132\nValidation Loss improved to 0.015037053264677525 Saving model 1\nEpoch 275, Loss: 0.008214994333684444\nValidation Loss improved to 0.015021102502942085 Saving model 1\nEpoch 276, Loss: 0.005743005778640509\nValidation Loss improved to 0.015004826709628105 Saving model 1\nEpoch 277, Loss: 0.006733445450663567\nValidation Loss improved to 0.014994307421147823 Saving model 1\nEpoch 278, Loss: 0.006457043346017599\nValidation Loss improved to 0.014974930323660374 Saving model 1\nEpoch 279, Loss: 0.007285524159669876\nValidation Loss improved to 0.014973037876188755 Saving model 1\nEpoch 280, Loss: 0.005561391822993755\nValidation Loss improved to 0.014953249134123325 Saving model 1\nEpoch 281, Loss: 0.004623208660632372\nValidation Loss improved to 0.014939909800887108 Saving model 1\nEpoch 282, Loss: 0.006708766799420118\nEpoch 283, Loss: 0.004629131406545639\nValidation Loss improved to 0.01492379792034626 Saving model 1\nEpoch 284, Loss: 0.003975983709096909\nEpoch 285, Loss: 0.0054988679476082325\nEpoch 286, Loss: 0.005289279390126467\nValidation Loss improved to 0.014921555295586586 Saving model 1\nEpoch 287, Loss: 0.003841619472950697\nValidation Loss improved to 0.014909482561051846 Saving model 1\nEpoch 288, Loss: 0.005304059945046902\nValidation Loss improved to 0.0148917306214571 Saving model 1\nEpoch 289, Loss: 0.0037180345971137285\nValidation Loss improved to 0.014869647100567818 Saving model 1\nEpoch 290, Loss: 0.005405270494520664\nValidation Loss improved to 0.014858962967991829 Saving model 1\nEpoch 291, Loss: 0.006220834795385599\nValidation Loss improved to 0.014839956536889076 Saving model 1\nEpoch 292, Loss: 0.004194031469523907\nEpoch 293, Loss: 0.004495556931942701\nEpoch 294, Loss: 0.0036763709504157305\nEpoch 295, Loss: 0.004709814675152302\nValidation Loss improved to 0.014839377254247665 Saving model 1\nEpoch 296, Loss: 0.005481081549078226\nValidation Loss improved to 0.014824059791862965 Saving model 1\nEpoch 297, Loss: 0.005947955884039402\nValidation Loss improved to 0.014810801483690739 Saving model 1\nEpoch 298, Loss: 0.004482213407754898\nValidation Loss improved to 0.014803643338382244 Saving model 1\nEpoch 299, Loss: 0.0033787398133426905\nValidation Loss improved to 0.014794173650443554 Saving model 1\nEpoch 300, Loss: 0.0046907514333724976\nValidation Loss improved to 0.01479343045502901 Saving model 1\nEpoch 301, Loss: 0.0053061312064528465\nValidation Loss improved to 0.014772249385714531 Saving model 1\nEpoch 302, Loss: 0.006980829406529665\nValidation Loss improved to 0.01476241648197174 Saving model 1\nEpoch 303, Loss: 0.005708071403205395\nValidation Loss improved to 0.014742929488420486 Saving model 1\nEpoch 304, Loss: 0.007133168634027243\nValidation Loss improved to 0.014741276390850544 Saving model 1\nEpoch 305, Loss: 0.005296381656080484\nValidation Loss improved to 0.014727303758263588 Saving model 1\nEpoch 306, Loss: 0.005228007212281227\nValidation Loss improved to 0.014720335602760315 Saving model 1\nEpoch 307, Loss: 0.005185435991734266\nValidation Loss improved to 0.014714927412569523 Saving model 1\nEpoch 308, Loss: 0.005019994918256998\nEpoch 309, Loss: 0.008367420174181461\nEpoch 310, Loss: 0.006502239033579826\nValidation Loss improved to 0.014712769538164139 Saving model 1\nEpoch 311, Loss: 0.007496541365981102\nEpoch 312, Loss: 0.009924089536070824\nEpoch 313, Loss: 0.006262745708227158\nEpoch 314, Loss: 0.005508330184966326\nEpoch 315, Loss: 0.007646054960787296\nEpoch 316, Loss: 0.005605997052043676\nEpoch 317, Loss: 0.005210737232118845\nEpoch 318, Loss: 0.005562482867389917\nValidation Loss improved to 0.014706797897815704 Saving model 1\nEpoch 319, Loss: 0.0054586478509008884\nValidation Loss improved to 0.014697154983878136 Saving model 1\nEpoch 320, Loss: 0.004049086011946201\nValidation Loss improved to 0.014682347886264324 Saving model 1\nEpoch 321, Loss: 0.006688325200229883\nValidation Loss improved to 0.014677059836685658 Saving model 1\nEpoch 322, Loss: 0.009776917286217213\nEpoch 323, Loss: 0.005018183961510658\nValidation Loss improved to 0.014662775211036205 Saving model 1\nEpoch 324, Loss: 0.00391044607385993\nValidation Loss improved to 0.01464719045907259 Saving model 1\nEpoch 325, Loss: 0.0057975007221102715\nValidation Loss improved to 0.014645656570792198 Saving model 1\nEpoch 326, Loss: 0.010574279353022575\nValidation Loss improved to 0.014638416469097137 Saving model 1\nEpoch 327, Loss: 0.008830703794956207\nEpoch 328, Loss: 0.004127155523747206\nValidation Loss improved to 0.01463486161082983 Saving model 1\nEpoch 329, Loss: 0.004526328295469284\nValidation Loss improved to 0.01462454255670309 Saving model 1\nEpoch 330, Loss: 0.004925187677145004\nValidation Loss improved to 0.014617422595620155 Saving model 1\nEpoch 331, Loss: 0.004640142433345318\nValidation Loss improved to 0.014616264961659908 Saving model 1\nEpoch 332, Loss: 0.0045697567984461784\nValidation Loss improved to 0.014607283286750317 Saving model 1\nEpoch 333, Loss: 0.006334418896585703\nValidation Loss improved to 0.014591510407626629 Saving model 1\nEpoch 334, Loss: 0.004925128538161516\nValidation Loss improved to 0.014588962309062481 Saving model 1\nEpoch 335, Loss: 0.004203257616609335\nValidation Loss improved to 0.014585473574697971 Saving model 1\nEpoch 336, Loss: 0.006563111674040556\nValidation Loss improved to 0.014584115706384182 Saving model 1\nEpoch 337, Loss: 0.004659163299947977\nValidation Loss improved to 0.014583032578229904 Saving model 1\nEpoch 338, Loss: 0.004501731134951115\nValidation Loss improved to 0.014572793617844582 Saving model 1\nEpoch 339, Loss: 0.004459029529243708\nEpoch 340, Loss: 0.006455426104366779\nValidation Loss improved to 0.014569795690476894 Saving model 1\nEpoch 341, Loss: 0.0034208232536911964\nValidation Loss improved to 0.014564108103513718 Saving model 1\nEpoch 342, Loss: 0.003060221439227462\nValidation Loss improved to 0.014553559012711048 Saving model 1\nEpoch 343, Loss: 0.004018053878098726\nValidation Loss improved to 0.014543999917805195 Saving model 1\nEpoch 344, Loss: 0.0031121494248509407\nValidation Loss improved to 0.014530528336763382 Saving model 1\nEpoch 345, Loss: 0.005329717881977558\nValidation Loss improved to 0.014529482461512089 Saving model 1\nEpoch 346, Loss: 0.004196091089397669\nValidation Loss improved to 0.01451656874269247 Saving model 1\nEpoch 347, Loss: 0.0028808482456952333\nValidation Loss improved to 0.0145014813169837 Saving model 1\nEpoch 348, Loss: 0.0036524678580462933\nValidation Loss improved to 0.014494852162897587 Saving model 1\nEpoch 349, Loss: 0.004225175827741623\nEpoch 350, Loss: 0.005629483610391617\nValidation Loss improved to 0.014486955478787422 Saving model 1\nEpoch 351, Loss: 0.005564331542700529\nValidation Loss improved to 0.014485079795122147 Saving model 1\nEpoch 352, Loss: 0.005042885895818472\nValidation Loss improved to 0.014470841735601425 Saving model 1\nEpoch 353, Loss: 0.003994285129010677\nValidation Loss improved to 0.014451439492404461 Saving model 1\nEpoch 354, Loss: 0.003794914111495018\nValidation Loss improved to 0.01444972213357687 Saving model 1\nEpoch 355, Loss: 0.0033950642682611942\nValidation Loss improved to 0.01444578543305397 Saving model 1\nEpoch 356, Loss: 0.0055521451868116856\nValidation Loss improved to 0.014440551400184631 Saving model 1\nEpoch 357, Loss: 0.007604301441460848\nValidation Loss improved to 0.014434194192290306 Saving model 1\nEpoch 358, Loss: 0.003971801605075598\nValidation Loss improved to 0.014427589252591133 Saving model 1\nEpoch 359, Loss: 0.006653450895100832\nValidation Loss improved to 0.014421872794628143 Saving model 1\nEpoch 360, Loss: 0.00832670833915472\nEpoch 361, Loss: 0.003564571961760521\nValidation Loss improved to 0.014417941682040691 Saving model 1\nEpoch 362, Loss: 0.0036910916678607464\nValidation Loss improved to 0.014416978694498539 Saving model 1\nEpoch 363, Loss: 0.005069860722869635\nValidation Loss improved to 0.014411800540983677 Saving model 1\nEpoch 364, Loss: 0.005776349455118179\nEpoch 365, Loss: 0.005825479980558157\nValidation Loss improved to 0.014406034722924232 Saving model 1\nEpoch 366, Loss: 0.004823204129934311\nValidation Loss improved to 0.014401664026081562 Saving model 1\nEpoch 367, Loss: 0.00466475635766983\nValidation Loss improved to 0.01439628191292286 Saving model 1\nEpoch 368, Loss: 0.004676125477999449\nValidation Loss improved to 0.01438723225146532 Saving model 1\nEpoch 369, Loss: 0.003364131785929203\nValidation Loss improved to 0.014379539526998997 Saving model 1\nEpoch 370, Loss: 0.0034210574813187122\nValidation Loss improved to 0.014362421818077564 Saving model 1\nEpoch 371, Loss: 0.0029269156511873007\nEpoch 372, Loss: 0.0034346734173595905\nEpoch 373, Loss: 0.0037507875822484493\nValidation Loss improved to 0.01435976941138506 Saving model 1\nEpoch 374, Loss: 0.003994354512542486\nValidation Loss improved to 0.014356175437569618 Saving model 1\nEpoch 375, Loss: 0.003087870078161359\nValidation Loss improved to 0.01434157695621252 Saving model 1\nEpoch 376, Loss: 0.0044586085714399815\nValidation Loss improved to 0.014329479075968266 Saving model 1\nEpoch 377, Loss: 0.005237904377281666\nValidation Loss improved to 0.014323536306619644 Saving model 1\nEpoch 378, Loss: 0.0032954479102045298\nEpoch 379, Loss: 0.0033502739388495684\nValidation Loss improved to 0.014321778900921345 Saving model 1\nEpoch 380, Loss: 0.0038201753050088882\nValidation Loss improved to 0.014312905259430408 Saving model 1\nEpoch 381, Loss: 0.004798299167305231\nValidation Loss improved to 0.014303771778941154 Saving model 1\nEpoch 382, Loss: 0.003667375771328807\nValidation Loss improved to 0.014293208718299866 Saving model 1\nEpoch 383, Loss: 0.005549170542508364\nValidation Loss improved to 0.014280867762863636 Saving model 1\nEpoch 384, Loss: 0.005896407179534435\nValidation Loss improved to 0.014273963868618011 Saving model 1\nEpoch 385, Loss: 0.003097282024100423\nValidation Loss improved to 0.014272970147430897 Saving model 1\nEpoch 386, Loss: 0.003857406321913004\nValidation Loss improved to 0.014264130964875221 Saving model 1\nEpoch 387, Loss: 0.00465448247268796\nValidation Loss improved to 0.014259153977036476 Saving model 1\nEpoch 388, Loss: 0.004429632797837257\nValidation Loss improved to 0.014246168546378613 Saving model 1\nEpoch 389, Loss: 0.008278392255306244\nValidation Loss improved to 0.014241919852793217 Saving model 1\nEpoch 390, Loss: 0.013627013191580772\nValidation Loss improved to 0.014238080009818077 Saving model 1\nEpoch 391, Loss: 0.007576752919703722\nValidation Loss improved to 0.01423264853656292 Saving model 1\nEpoch 392, Loss: 0.005606487859040499\nValidation Loss improved to 0.014227529056370258 Saving model 1\nEpoch 393, Loss: 0.007182175759226084\nEpoch 394, Loss: 0.005766158923506737\nEpoch 395, Loss: 0.004560654051601887\nValidation Loss improved to 0.014226673170924187 Saving model 1\nEpoch 396, Loss: 0.004065799992531538\nValidation Loss improved to 0.014214188791811466 Saving model 1\nEpoch 397, Loss: 0.004521558061242104\nValidation Loss improved to 0.014210959896445274 Saving model 1\nEpoch 398, Loss: 0.005943669937551022\nEpoch 399, Loss: 0.005272563546895981\nValidation Loss improved to 0.014209011569619179 Saving model 1\nEpoch 400, Loss: 0.00843859277665615\nEpoch 401, Loss: 0.007551269605755806\nEpoch 402, Loss: 0.003711903700605035\nValidation Loss improved to 0.014208544977009296 Saving model 1\nEpoch 403, Loss: 0.004199203569442034\nEpoch 404, Loss: 0.004730555694550276\nEpoch 405, Loss: 0.005006111226975918\nValidation Loss improved to 0.014207075349986553 Saving model 1\nEpoch 406, Loss: 0.006377568934112787\nEpoch 407, Loss: 0.010023847222328186\nEpoch 408, Loss: 0.006498138885945082\nValidation Loss improved to 0.014204726554453373 Saving model 1\nEpoch 409, Loss: 0.002978699514642358\nValidation Loss improved to 0.014202115125954151 Saving model 1\nEpoch 410, Loss: 0.004882233217358589\nValidation Loss improved to 0.014189651235938072 Saving model 1\nEpoch 411, Loss: 0.003508901922032237\nValidation Loss improved to 0.014186960645020008 Saving model 1\nEpoch 412, Loss: 0.0036803309340029955\nEpoch 413, Loss: 0.003810813883319497\nValidation Loss improved to 0.014185893349349499 Saving model 1\nEpoch 414, Loss: 0.0031702714040875435\nValidation Loss improved to 0.014179225079715252 Saving model 1\nEpoch 415, Loss: 0.003697039792314172\nEpoch 416, Loss: 0.0032623931765556335\nValidation Loss improved to 0.014172486029565334 Saving model 1\nEpoch 417, Loss: 0.004009781405329704\nValidation Loss improved to 0.014165574684739113 Saving model 1\nEpoch 418, Loss: 0.00421323673799634\nValidation Loss improved to 0.014157301746308804 Saving model 1\nEpoch 419, Loss: 0.006785858888179064\nValidation Loss improved to 0.014150699600577354 Saving model 1\nEpoch 420, Loss: 0.00771614583209157\nValidation Loss improved to 0.014145136810839176 Saving model 1\nEpoch 421, Loss: 0.004429772030562162\nValidation Loss improved to 0.01413999404758215 Saving model 1\nEpoch 422, Loss: 0.0034854647237807512\nValidation Loss improved to 0.014136802405118942 Saving model 1\nEpoch 423, Loss: 0.0037173249293118715\nValidation Loss improved to 0.014128582552075386 Saving model 1\nEpoch 424, Loss: 0.0063493698835372925\nValidation Loss improved to 0.014127705246210098 Saving model 1\nEpoch 425, Loss: 0.0037214800249785185\nValidation Loss improved to 0.014124555513262749 Saving model 1\nEpoch 426, Loss: 0.0042589521035552025\nEpoch 427, Loss: 0.005467806477099657\nEpoch 428, Loss: 0.004002782516181469\nValidation Loss improved to 0.014116061851382256 Saving model 1\nEpoch 429, Loss: 0.004805768374353647\nEpoch 430, Loss: 0.004552698228508234\nEpoch 431, Loss: 0.00568181648850441\nValidation Loss improved to 0.014114734716713428 Saving model 1\nEpoch 432, Loss: 0.005307010840624571\nValidation Loss improved to 0.01411137543618679 Saving model 1\nEpoch 433, Loss: 0.00606362521648407\nEpoch 434, Loss: 0.003973841667175293\nValidation Loss improved to 0.014110908843576908 Saving model 1\nEpoch 435, Loss: 0.003521565580740571\nEpoch 436, Loss: 0.0037997481413185596\nValidation Loss improved to 0.01410958543419838 Saving model 1\nEpoch 437, Loss: 0.004101121798157692\nValidation Loss improved to 0.014107726514339447 Saving model 1\nEpoch 438, Loss: 0.005471337586641312\nValidation Loss improved to 0.014106424525380135 Saving model 1\nEpoch 439, Loss: 0.0039765797555446625\nEpoch 440, Loss: 0.00613787304610014\nEpoch 441, Loss: 0.01143337320536375\nEpoch 442, Loss: 0.008582090958952904\nEpoch 443, Loss: 0.0046731228940188885\nEpoch 444, Loss: 0.0038970683235675097\nEpoch 445, Loss: 0.003492671065032482\nEpoch 446, Loss: 0.0038988073356449604\nEpoch 447, Loss: 0.004622296430170536\nEpoch 448, Loss: 0.004644176457077265\nEpoch   449: reducing learning rate of group 0 to 1.5000e-04.\nEpoch 449, Loss: 0.0021160005126148462\nEpoch 450, Loss: 0.001443094457499683\nEpoch 451, Loss: 0.001341983792372048\nEpoch 452, Loss: 0.0010668834438547492\nValidation Loss improved to 0.014105468988418579 Saving model 1\nEpoch 453, Loss: 0.0008557417313568294\nValidation Loss improved to 0.014104212634265423 Saving model 1\nEpoch 454, Loss: 0.0008302309433929622\nValidation Loss improved to 0.014102458953857422 Saving model 1\nEpoch 455, Loss: 0.0008264433708973229\nValidation Loss improved to 0.014100931584835052 Saving model 1\nEpoch 456, Loss: 0.0007573229959234595\nValidation Loss improved to 0.014099120162427425 Saving model 1\nEpoch 457, Loss: 0.0007960128714330494\nValidation Loss improved to 0.014097411185503006 Saving model 1\nEpoch 458, Loss: 0.0007863495848141611\nValidation Loss improved to 0.01409514807164669 Saving model 1\nEpoch 459, Loss: 0.0007535487529821694\nValidation Loss improved to 0.014093763194978237 Saving model 1\nEpoch 460, Loss: 0.0006841922295279801\nValidation Loss improved to 0.014091739431023598 Saving model 1\nEpoch 461, Loss: 0.0006700706435367465\nValidation Loss improved to 0.014089688658714294 Saving model 1\nEpoch 462, Loss: 0.0006750090979039669\nValidation Loss improved to 0.014087614603340626 Saving model 1\nEpoch 463, Loss: 0.0006789679173380136\nValidation Loss improved to 0.014085754752159119 Saving model 1\nEpoch 464, Loss: 0.0006580132758244872\nValidation Loss improved to 0.014084212481975555 Saving model 1\nEpoch 465, Loss: 0.000652326038107276\nValidation Loss improved to 0.01408260315656662 Saving model 1\nEpoch 466, Loss: 0.0006608407711610198\nValidation Loss improved to 0.014080720953643322 Saving model 1\nEpoch 467, Loss: 0.0006642489461228251\nValidation Loss improved to 0.014078387059271336 Saving model 1\nEpoch 468, Loss: 0.0008243730990216136\nValidation Loss improved to 0.014075620099902153 Saving model 1\nEpoch 469, Loss: 0.0007605701684951782\nValidation Loss improved to 0.014073720201849937 Saving model 1\nEpoch 470, Loss: 0.0006134143332019448\nValidation Loss improved to 0.014071968384087086 Saving model 1\nEpoch 471, Loss: 0.0006207551341503859\nValidation Loss improved to 0.014070084318518639 Saving model 1\nEpoch 472, Loss: 0.0006225573597475886\nValidation Loss improved to 0.014067990705370903 Saving model 1\nEpoch 473, Loss: 0.0007389374077320099\nValidation Loss improved to 0.014065968804061413 Saving model 1\nEpoch 474, Loss: 0.0007132574683055282\nValidation Loss improved to 0.014064217917621136 Saving model 1\nEpoch 475, Loss: 0.0006398214027285576\nValidation Loss improved to 0.014062496833503246 Saving model 1\nEpoch 476, Loss: 0.0005881279357708991\nValidation Loss improved to 0.014060777612030506 Saving model 1\nEpoch 477, Loss: 0.0006748521700501442\nValidation Loss improved to 0.014058761298656464 Saving model 1\nEpoch 478, Loss: 0.0006391776842065156\nValidation Loss improved to 0.014057070948183537 Saving model 1\nEpoch 479, Loss: 0.0006597406463697553\nValidation Loss improved to 0.014055062085390091 Saving model 1\nEpoch 480, Loss: 0.0007196525111794472\nValidation Loss improved to 0.014052974991500378 Saving model 1\nEpoch 481, Loss: 0.0006103373016230762\nValidation Loss improved to 0.014051305130124092 Saving model 1\nEpoch 482, Loss: 0.0006273940089158714\nValidation Loss improved to 0.014049265533685684 Saving model 1\nEpoch 483, Loss: 0.0006882373709231615\nValidation Loss improved to 0.014047536998987198 Saving model 1\nEpoch 484, Loss: 0.0006604036898352206\nValidation Loss improved to 0.014044912531971931 Saving model 1\nEpoch 485, Loss: 0.00060974684311077\nValidation Loss improved to 0.01404310017824173 Saving model 1\nEpoch 486, Loss: 0.000669505272526294\nValidation Loss improved to 0.014040772803127766 Saving model 1\nEpoch 487, Loss: 0.0005646561621688306\nValidation Loss improved to 0.014038668014109135 Saving model 1\nEpoch 488, Loss: 0.0005668684025295079\nValidation Loss improved to 0.014036937616765499 Saving model 1\nEpoch 489, Loss: 0.0005092403152957559\nValidation Loss improved to 0.014035109430551529 Saving model 1\nEpoch 490, Loss: 0.0005605949554592371\nValidation Loss improved to 0.014033487997949123 Saving model 1\nEpoch 491, Loss: 0.0005742062930949032\nValidation Loss improved to 0.014031526632606983 Saving model 1\nEpoch 492, Loss: 0.0004950860748067498\nValidation Loss improved to 0.014029876329004765 Saving model 1\nEpoch 493, Loss: 0.0005535862874239683\nValidation Loss improved to 0.014027809724211693 Saving model 1\nEpoch 494, Loss: 0.0005219585145823658\nValidation Loss improved to 0.014026380144059658 Saving model 1\nEpoch 495, Loss: 0.000584817782510072\nValidation Loss improved to 0.014024445787072182 Saving model 1\nEpoch 496, Loss: 0.0005864030099473894\nValidation Loss improved to 0.014022549614310265 Saving model 1\nEpoch 497, Loss: 0.0004784542543347925\nValidation Loss improved to 0.014020698145031929 Saving model 1\nEpoch 498, Loss: 0.0005629885708913207\nValidation Loss improved to 0.014018859714269638 Saving model 1\nEpoch 499, Loss: 0.00045467924792319536\nValidation Loss improved to 0.014016875065863132 Saving model 1\nEpoch 500, Loss: 0.00047407022793777287\nValidation Loss improved to 0.014015069231390953 Saving model 1\nEpoch 501, Loss: 0.000542301859240979\nValidation Loss improved to 0.014013264328241348 Saving model 1\nEpoch 502, Loss: 0.0005643246695399284\nValidation Loss improved to 0.014011271297931671 Saving model 1\nEpoch 503, Loss: 0.0005042001139372587\nValidation Loss improved to 0.014008700847625732 Saving model 1\nEpoch 504, Loss: 0.0006051263771951199\nValidation Loss improved to 0.01400698907673359 Saving model 1\nEpoch 505, Loss: 0.0005697188316844404\nValidation Loss improved to 0.014005160890519619 Saving model 1\nEpoch 506, Loss: 0.0005615694099105895\nValidation Loss improved to 0.014003213495016098 Saving model 1\nEpoch 507, Loss: 0.0005096827517263591\nValidation Loss improved to 0.01400161162018776 Saving model 1\nEpoch 508, Loss: 0.000497856002766639\nValidation Loss improved to 0.013999603688716888 Saving model 1\nEpoch 509, Loss: 0.00044691457878798246\nValidation Loss improved to 0.013998094014823437 Saving model 1\nEpoch 510, Loss: 0.0004597536171786487\nValidation Loss improved to 0.013996395282447338 Saving model 1\nEpoch 511, Loss: 0.0005211808602325618\nValidation Loss improved to 0.013994620181620121 Saving model 1\nEpoch 512, Loss: 0.0005134642706252635\nValidation Loss improved to 0.01399235613644123 Saving model 1\nEpoch 513, Loss: 0.0005504207802005112\nValidation Loss improved to 0.013989423401653767 Saving model 1\nEpoch 514, Loss: 0.0006138940807431936\nValidation Loss improved to 0.01398761011660099 Saving model 1\nEpoch 515, Loss: 0.0006763245910406113\nValidation Loss improved to 0.01398547738790512 Saving model 1\nEpoch 516, Loss: 0.00046524356002919376\nValidation Loss improved to 0.013983498327434063 Saving model 1\nEpoch 517, Loss: 0.00044848606921732426\nValidation Loss improved to 0.013981616124510765 Saving model 1\nEpoch 518, Loss: 0.0004654137883335352\nValidation Loss improved to 0.013979426585137844 Saving model 1\nEpoch 519, Loss: 0.00045717268949374557\nValidation Loss improved to 0.01397737581282854 Saving model 1\nEpoch 520, Loss: 0.0004615876532625407\nValidation Loss improved to 0.013975278474390507 Saving model 1\nEpoch 521, Loss: 0.0004223907890263945\nValidation Loss improved to 0.013973093591630459 Saving model 1\nEpoch 522, Loss: 0.00043313842616043985\nValidation Loss improved to 0.013971023261547089 Saving model 1\nEpoch 523, Loss: 0.0004972599563188851\nValidation Loss improved to 0.01396858412772417 Saving model 1\nEpoch 524, Loss: 0.0005071068881079555\nValidation Loss improved to 0.013966298662126064 Saving model 1\nEpoch 525, Loss: 0.000427520222729072\nValidation Loss improved to 0.013964621350169182 Saving model 1\nEpoch 526, Loss: 0.0004338111320976168\nValidation Loss improved to 0.01396236289292574 Saving model 1\nEpoch 527, Loss: 0.0004239016561768949\nValidation Loss improved to 0.013959720730781555 Saving model 1\nEpoch 528, Loss: 0.0006437458214350045\nValidation Loss improved to 0.013957615941762924 Saving model 1\nEpoch 529, Loss: 0.0005326839745976031\nValidation Loss improved to 0.013955684378743172 Saving model 1\nEpoch 530, Loss: 0.00046169196139089763\nValidation Loss improved to 0.013953073881566525 Saving model 1\nEpoch 531, Loss: 0.0004188001621514559\nValidation Loss improved to 0.013950915075838566 Saving model 1\nEpoch 532, Loss: 0.00043409361387602985\nValidation Loss improved to 0.013948990032076836 Saving model 1\nEpoch 533, Loss: 0.0004472901055123657\nValidation Loss improved to 0.01394676323980093 Saving model 1\nEpoch 534, Loss: 0.00042715808376669884\nValidation Loss improved to 0.013944352976977825 Saving model 1\nEpoch 535, Loss: 0.0004174070490989834\nValidation Loss improved to 0.013942012563347816 Saving model 1\nEpoch 536, Loss: 0.000409401924116537\nValidation Loss improved to 0.013939514756202698 Saving model 1\nEpoch 537, Loss: 0.0004491214349400252\nValidation Loss improved to 0.013937670737504959 Saving model 1\nEpoch 538, Loss: 0.0005612992681562901\nValidation Loss improved to 0.01393571775406599 Saving model 1\nEpoch 539, Loss: 0.00043849117355421185\nValidation Loss improved to 0.01393340528011322 Saving model 1\nEpoch 540, Loss: 0.00044675215031020343\nValidation Loss improved to 0.013930730521678925 Saving model 1\nEpoch 541, Loss: 0.0004782773903571069\nValidation Loss improved to 0.013928456231951714 Saving model 1\nEpoch 542, Loss: 0.0006424661260098219\nValidation Loss improved to 0.013926206156611443 Saving model 1\nEpoch 543, Loss: 0.0005714904982596636\nValidation Loss improved to 0.013923505321145058 Saving model 1\nEpoch 544, Loss: 0.0005403810646384954\nValidation Loss improved to 0.013921419158577919 Saving model 1\nEpoch 545, Loss: 0.0004922043299302459\nValidation Loss improved to 0.013918754644691944 Saving model 1\nEpoch 546, Loss: 0.0005806612898595631\nValidation Loss improved to 0.013916341587901115 Saving model 1\nEpoch 547, Loss: 0.00050061393994838\nValidation Loss improved to 0.013913033530116081 Saving model 1\nEpoch 548, Loss: 0.00046935270074754953\nValidation Loss improved to 0.013910425826907158 Saving model 1\nEpoch 549, Loss: 0.000441284675616771\nValidation Loss improved to 0.013908186927437782 Saving model 1\nEpoch 550, Loss: 0.0004513050662353635\nValidation Loss improved to 0.013905986212193966 Saving model 1\nEpoch 551, Loss: 0.0005360126378946006\nValidation Loss improved to 0.013903418555855751 Saving model 1\nEpoch 552, Loss: 0.00040012854151427746\nValidation Loss improved to 0.013900925405323505 Saving model 1\nEpoch 553, Loss: 0.00038198186666704714\nValidation Loss improved to 0.013898482546210289 Saving model 1\nEpoch 554, Loss: 0.0004028122057206929\nValidation Loss improved to 0.013896285556256771 Saving model 1\nEpoch 555, Loss: 0.0003763215208891779\nValidation Loss improved to 0.013893838971853256 Saving model 1\nEpoch 556, Loss: 0.000435637281043455\nValidation Loss improved to 0.01389217283576727 Saving model 1\nEpoch 557, Loss: 0.000565473863389343\nValidation Loss improved to 0.013890167698264122 Saving model 1\nEpoch 558, Loss: 0.0004916591569781303\nValidation Loss improved to 0.013887383043766022 Saving model 1\nEpoch 559, Loss: 0.00044290113146416843\nValidation Loss improved to 0.013885002583265305 Saving model 1\nEpoch 560, Loss: 0.00046870464575476944\nValidation Loss improved to 0.013882234692573547 Saving model 1\nEpoch 561, Loss: 0.0005366287659853697\nValidation Loss improved to 0.013879814185202122 Saving model 1\nEpoch 562, Loss: 0.0005533449584618211\nValidation Loss improved to 0.013877158984541893 Saving model 1\nEpoch 563, Loss: 0.0005096912500448525\nValidation Loss improved to 0.013874192722141743 Saving model 1\nEpoch 564, Loss: 0.0006212589214555919\nValidation Loss improved to 0.013871882110834122 Saving model 1\nEpoch 565, Loss: 0.0004170938627794385\nValidation Loss improved to 0.0138692706823349 Saving model 1\nEpoch 566, Loss: 0.0004780264280270785\nValidation Loss improved to 0.013865945860743523 Saving model 1\nEpoch 567, Loss: 0.00040278921369463205\nValidation Loss improved to 0.01386338286101818 Saving model 1\nEpoch 568, Loss: 0.00037812264054082334\nValidation Loss improved to 0.013860370963811874 Saving model 1\nEpoch 569, Loss: 0.00038636295357719064\nValidation Loss improved to 0.013857591897249222 Saving model 1\nEpoch 570, Loss: 0.0004468986007850617\nValidation Loss improved to 0.013854875229299068 Saving model 1\nEpoch 571, Loss: 0.0004821116745006293\nValidation Loss improved to 0.013851827941834927 Saving model 1\nEpoch 572, Loss: 0.00047127509606070817\nValidation Loss improved to 0.013849485665559769 Saving model 1\nEpoch 573, Loss: 0.0005162851884961128\nValidation Loss improved to 0.013846932910382748 Saving model 1\nEpoch 574, Loss: 0.00039670843398198485\nValidation Loss improved to 0.013844676315784454 Saving model 1\nEpoch 575, Loss: 0.0005729363183490932\nValidation Loss improved to 0.01384176779538393 Saving model 1\nEpoch 576, Loss: 0.0003912359243258834\nValidation Loss improved to 0.01383882574737072 Saving model 1\nEpoch 577, Loss: 0.00037946729571558535\nValidation Loss improved to 0.013836100697517395 Saving model 1\nEpoch 578, Loss: 0.0003852401569020003\nValidation Loss improved to 0.013833193108439445 Saving model 1\nEpoch 579, Loss: 0.0004280645225662738\nValidation Loss improved to 0.013830377720296383 Saving model 1\nEpoch 580, Loss: 0.00038930997834540904\nValidation Loss improved to 0.01382730808109045 Saving model 1\nEpoch 581, Loss: 0.0004368467489257455\nValidation Loss improved to 0.013825164176523685 Saving model 1\nEpoch 582, Loss: 0.0006877214182168245\nValidation Loss improved to 0.01382199302315712 Saving model 1\nEpoch 583, Loss: 0.0004327620263211429\nValidation Loss improved to 0.013819518499076366 Saving model 1\nEpoch 584, Loss: 0.00038373659481294453\nValidation Loss improved to 0.013816661201417446 Saving model 1\nEpoch 585, Loss: 0.0003362582647241652\nValidation Loss improved to 0.013814045116305351 Saving model 1\nEpoch 586, Loss: 0.0003861873992718756\nValidation Loss improved to 0.013811036013066769 Saving model 1\nEpoch 587, Loss: 0.0003426308976486325\nValidation Loss improved to 0.013808432966470718 Saving model 1\nEpoch 588, Loss: 0.0003869823121931404\nValidation Loss improved to 0.013805650174617767 Saving model 1\nEpoch 589, Loss: 0.00032889359863474965\nValidation Loss improved to 0.013803121633827686 Saving model 1\nEpoch 590, Loss: 0.00037493082345463336\nValidation Loss improved to 0.013800328597426414 Saving model 1\nEpoch 591, Loss: 0.00038877021870575845\nValidation Loss improved to 0.013797773979604244 Saving model 1\nEpoch 592, Loss: 0.0005571764195337892\nValidation Loss improved to 0.013794967904686928 Saving model 1\nEpoch 593, Loss: 0.0004627868765965104\nValidation Loss improved to 0.013792166486382484 Saving model 1\nEpoch 594, Loss: 0.0004428120737429708\nValidation Loss improved to 0.013789650984108448 Saving model 1\nEpoch 595, Loss: 0.0003505338972900063\nValidation Loss improved to 0.013787029311060905 Saving model 1\nEpoch 596, Loss: 0.0004424751386977732\nValidation Loss improved to 0.013783962465822697 Saving model 1\nEpoch 597, Loss: 0.00037605181569233537\nValidation Loss improved to 0.013781138695776463 Saving model 1\nEpoch 598, Loss: 0.0004472288419492543\nValidation Loss improved to 0.013778061605989933 Saving model 1\nEpoch 599, Loss: 0.0005058383103460073\nValidation Loss improved to 0.01377543993294239 Saving model 1\nEpoch 600, Loss: 0.0005509147886186838\nValidation Loss improved to 0.013772969134151936 Saving model 1\nEpoch 601, Loss: 0.00042550580110400915\nValidation Loss improved to 0.013770307414233685 Saving model 1\nEpoch 602, Loss: 0.00039502372965216637\nValidation Loss improved to 0.013768043369054794 Saving model 1\nEpoch 603, Loss: 0.0004280056164134294\nValidation Loss improved to 0.01376518327742815 Saving model 1\nEpoch 604, Loss: 0.00045976144610904157\nValidation Loss improved to 0.013762117363512516 Saving model 1\nEpoch 605, Loss: 0.00033411552431061864\nValidation Loss improved to 0.01375923864543438 Saving model 1\nEpoch 606, Loss: 0.0003532036207616329\nValidation Loss improved to 0.013756437227129936 Saving model 1\nEpoch 607, Loss: 0.0004607636365108192\nValidation Loss improved to 0.013753553852438927 Saving model 1\nEpoch 608, Loss: 0.0004346334026195109\nValidation Loss improved to 0.013751012273132801 Saving model 1\nEpoch 609, Loss: 0.0003744499699678272\nValidation Loss improved to 0.013748359866440296 Saving model 1\nEpoch 610, Loss: 0.0003245632105972618\nValidation Loss improved to 0.01374562457203865 Saving model 1\nEpoch 611, Loss: 0.00036443976568989456\nValidation Loss improved to 0.013742927461862564 Saving model 1\nEpoch 612, Loss: 0.0003809395129792392\nValidation Loss improved to 0.01374037191271782 Saving model 1\nEpoch 613, Loss: 0.0003819428966380656\nValidation Loss improved to 0.013737493194639683 Saving model 1\nEpoch 614, Loss: 0.0003491465176921338\nValidation Loss improved to 0.013734716922044754 Saving model 1\nEpoch 615, Loss: 0.0004326089983806014\nValidation Loss improved to 0.01373170968145132 Saving model 1\nEpoch 616, Loss: 0.0004475022724363953\nValidation Loss improved to 0.013728720135986805 Saving model 1\nEpoch 617, Loss: 0.0006133462302386761\nValidation Loss improved to 0.013725988566875458 Saving model 1\nEpoch 618, Loss: 0.0004586789000313729\nValidation Loss improved to 0.013723173178732395 Saving model 1\nEpoch 619, Loss: 0.0004016634193249047\nValidation Loss improved to 0.013720457442104816 Saving model 1\nEpoch 620, Loss: 0.00040502764750272036\nValidation Loss improved to 0.013717525638639927 Saving model 1\nEpoch 621, Loss: 0.00039152835961431265\nValidation Loss improved to 0.01371487695723772 Saving model 1\nEpoch 622, Loss: 0.0003600006748456508\nValidation Loss improved to 0.013712099753320217 Saving model 1\nEpoch 623, Loss: 0.00035306575591675937\nValidation Loss improved to 0.013709146529436111 Saving model 1\nEpoch 624, Loss: 0.0003620381176006049\nValidation Loss improved to 0.013706264086067677 Saving model 1\nEpoch 625, Loss: 0.00036993762478232384\nValidation Loss improved to 0.01370401680469513 Saving model 1\nEpoch 626, Loss: 0.00043517269659787416\nValidation Loss improved to 0.01370089128613472 Saving model 1\nEpoch 627, Loss: 0.0006610208074562252\nValidation Loss improved to 0.013698158785700798 Saving model 1\nEpoch 628, Loss: 0.0004676192766055465\nValidation Loss improved to 0.013695292174816132 Saving model 1\nEpoch 629, Loss: 0.0004913200973533094\nValidation Loss improved to 0.01369236595928669 Saving model 1\nEpoch 630, Loss: 0.0004315930709708482\nValidation Loss improved to 0.01368950679898262 Saving model 1\nEpoch 631, Loss: 0.00041352282278239727\nValidation Loss improved to 0.013686275109648705 Saving model 1\nEpoch 632, Loss: 0.0003853458329103887\nValidation Loss improved to 0.013683594763278961 Saving model 1\nEpoch 633, Loss: 0.0004180597607046366\nValidation Loss improved to 0.01368069089949131 Saving model 1\nEpoch 634, Loss: 0.0003914280387107283\nValidation Loss improved to 0.013678150251507759 Saving model 1\nEpoch 635, Loss: 0.0003796092059928924\nValidation Loss improved to 0.013674958609044552 Saving model 1\nEpoch 636, Loss: 0.00035515526542440057\nValidation Loss improved to 0.013672169297933578 Saving model 1\nEpoch 637, Loss: 0.0003783261636272073\nValidation Loss improved to 0.0136693324893713 Saving model 1\nEpoch 638, Loss: 0.0003790832997765392\nValidation Loss improved to 0.013666285201907158 Saving model 1\nEpoch 639, Loss: 0.00035644331364892423\nValidation Loss improved to 0.01366365421563387 Saving model 1\nEpoch 640, Loss: 0.0004044818051625043\nValidation Loss improved to 0.013660767115652561 Saving model 1\nEpoch 641, Loss: 0.00033898212132044137\nValidation Loss improved to 0.013658064417541027 Saving model 1\nEpoch 642, Loss: 0.00037687603617087007\nValidation Loss improved to 0.01365510281175375 Saving model 1\nEpoch 643, Loss: 0.00042643831693567336\nValidation Loss improved to 0.013652494177222252 Saving model 1\nEpoch 644, Loss: 0.000482940929941833\nValidation Loss improved to 0.013649819418787956 Saving model 1\nEpoch 645, Loss: 0.00034336495446041226\nValidation Loss improved to 0.01364688016474247 Saving model 1\nEpoch 646, Loss: 0.000319109094562009\nValidation Loss improved to 0.013644494116306305 Saving model 1\nEpoch 647, Loss: 0.00032710243249312043\nValidation Loss improved to 0.01364171039313078 Saving model 1\nEpoch 648, Loss: 0.0002800908114295453\nValidation Loss improved to 0.013638956472277641 Saving model 1\nEpoch 649, Loss: 0.000322252803016454\nValidation Loss improved to 0.013636528514325619 Saving model 1\nEpoch 650, Loss: 0.00045853471965529025\nValidation Loss improved to 0.013633533380925655 Saving model 1\nEpoch 651, Loss: 0.0003867938939947635\nValidation Loss improved to 0.013630714267492294 Saving model 1\nEpoch 652, Loss: 0.0005037561641074717\nValidation Loss improved to 0.01362790446728468 Saving model 1\nEpoch 653, Loss: 0.0003858084965031594\nValidation Loss improved to 0.013624825514853 Saving model 1\nEpoch 654, Loss: 0.00033687398536130786\nValidation Loss improved to 0.013621963560581207 Saving model 1\nEpoch 655, Loss: 0.0003194778400938958\nValidation Loss improved to 0.013619028963148594 Saving model 1\nEpoch 656, Loss: 0.00039197708247229457\nValidation Loss improved to 0.013616508804261684 Saving model 1\nEpoch 657, Loss: 0.00046384258894249797\nValidation Loss improved to 0.013613585382699966 Saving model 1\nEpoch 658, Loss: 0.00048484341823495924\nValidation Loss improved to 0.01361055951565504 Saving model 1\nEpoch 659, Loss: 0.00036503846058622\nValidation Loss improved to 0.013607744127511978 Saving model 1\nEpoch 660, Loss: 0.00038769980892539024\nValidation Loss improved to 0.013604627922177315 Saving model 1\nEpoch 661, Loss: 0.000533675542101264\nValidation Loss improved to 0.013601720333099365 Saving model 1\nEpoch 662, Loss: 0.0004984965198673308\nValidation Loss improved to 0.013598552905023098 Saving model 1\nEpoch 663, Loss: 0.0004834118881262839\nValidation Loss improved to 0.013595608994364738 Saving model 1\nEpoch 664, Loss: 0.00044672368676401675\nValidation Loss improved to 0.013592759147286415 Saving model 1\nEpoch 665, Loss: 0.0003611611609812826\nValidation Loss improved to 0.013590024784207344 Saving model 1\nEpoch 666, Loss: 0.0004600798711180687\nValidation Loss improved to 0.01358717866241932 Saving model 1\nEpoch 667, Loss: 0.00040180730866268277\nValidation Loss improved to 0.013584519736468792 Saving model 1\nEpoch 668, Loss: 0.0005211662501096725\nValidation Loss improved to 0.013582213781774044 Saving model 1\nEpoch 669, Loss: 0.000637055141851306\nValidation Loss improved to 0.013579077087342739 Saving model 1\nEpoch 670, Loss: 0.0004173366178292781\nValidation Loss improved to 0.013575497083365917 Saving model 1\nEpoch 671, Loss: 0.00042579034925438464\nValidation Loss improved to 0.01357258204370737 Saving model 1\nEpoch 672, Loss: 0.0004905776586383581\nValidation Loss improved to 0.013569374568760395 Saving model 1\nEpoch 673, Loss: 0.0005798520869575441\nValidation Loss improved to 0.0135658485814929 Saving model 1\nEpoch 674, Loss: 0.0006820840644650161\nValidation Loss improved to 0.013563089072704315 Saving model 1\nEpoch 675, Loss: 0.0003818651312030852\nValidation Loss improved to 0.013559676706790924 Saving model 1\nEpoch 676, Loss: 0.0003656303742900491\nValidation Loss improved to 0.013556918129324913 Saving model 1\nEpoch 677, Loss: 0.00037781408173032105\nValidation Loss improved to 0.013554083183407784 Saving model 1\nEpoch 678, Loss: 0.000265335023868829\nValidation Loss improved to 0.013551145792007446 Saving model 1\nEpoch 679, Loss: 0.0005113059305585921\nValidation Loss improved to 0.013548268005251884 Saving model 1\nEpoch 680, Loss: 0.0005445928545668721\nValidation Loss improved to 0.013544834218919277 Saving model 1\nEpoch 681, Loss: 0.0005818278877995908\nValidation Loss improved to 0.013542274944484234 Saving model 1\nEpoch 682, Loss: 0.00035964895505458117\nValidation Loss improved to 0.013539157807826996 Saving model 1\nEpoch 683, Loss: 0.00036923462175764143\nValidation Loss improved to 0.013536439277231693 Saving model 1\nEpoch 684, Loss: 0.0005192880635149777\nValidation Loss improved to 0.01353378314524889 Saving model 1\nEpoch 685, Loss: 0.0004065636603627354\nValidation Loss improved to 0.013531122356653214 Saving model 1\nEpoch 686, Loss: 0.0004589758755173534\nValidation Loss improved to 0.013528524897992611 Saving model 1\nEpoch 687, Loss: 0.0004937238991260529\nValidation Loss improved to 0.013525839895009995 Saving model 1\nEpoch 688, Loss: 0.000330117269186303\nValidation Loss improved to 0.013523039408028126 Saving model 1\nEpoch 689, Loss: 0.0004574289487209171\nValidation Loss improved to 0.013520374894142151 Saving model 1\nEpoch 690, Loss: 0.00046000248403288424\nValidation Loss improved to 0.013517456129193306 Saving model 1\nEpoch 691, Loss: 0.00043720155372284353\nValidation Loss improved to 0.013514789752662182 Saving model 1\nEpoch 692, Loss: 0.00041522018727846444\nValidation Loss improved to 0.013512269593775272 Saving model 1\nEpoch 693, Loss: 0.0003769879986066371\nValidation Loss improved to 0.013509818352758884 Saving model 1\nEpoch 694, Loss: 0.00047440946218557656\nValidation Loss improved to 0.013507366180419922 Saving model 1\nEpoch 695, Loss: 0.0006034858524799347\nValidation Loss improved to 0.013504519127309322 Saving model 1\nEpoch 696, Loss: 0.0005283296923153102\nValidation Loss improved to 0.013501322828233242 Saving model 1\nEpoch 697, Loss: 0.0004522378440015018\nValidation Loss improved to 0.013498405925929546 Saving model 1\nEpoch 698, Loss: 0.000454263121355325\nValidation Loss improved to 0.013495281338691711 Saving model 1\nEpoch 699, Loss: 0.00030804230482317507\nValidation Loss improved to 0.013492600992321968 Saving model 1\nEpoch 700, Loss: 0.0003673473256640136\nValidation Loss improved to 0.013489552773535252 Saving model 1\nEpoch 701, Loss: 0.0003093468549195677\nValidation Loss improved to 0.0134868323802948 Saving model 1\nEpoch 702, Loss: 0.0003477534919511527\nValidation Loss improved to 0.013484223745763302 Saving model 1\nEpoch 703, Loss: 0.00043645568075589836\nValidation Loss improved to 0.013481412082910538 Saving model 1\nEpoch 704, Loss: 0.0003357120440341532\nValidation Loss improved to 0.013479018583893776 Saving model 1\nEpoch 705, Loss: 0.00044603372225537896\nValidation Loss improved to 0.013475888408720493 Saving model 1\nEpoch 706, Loss: 0.00033231466659344733\nValidation Loss improved to 0.013473291881382465 Saving model 1\nEpoch 707, Loss: 0.0003494535048957914\nValidation Loss improved to 0.013470469042658806 Saving model 1\nEpoch 708, Loss: 0.0002970299101434648\nValidation Loss improved to 0.01346774585545063 Saving model 1\nEpoch 709, Loss: 0.0004336941346991807\nValidation Loss improved to 0.013464483432471752 Saving model 1\nEpoch 710, Loss: 0.0004634768993128091\nValidation Loss improved to 0.013461806811392307 Saving model 1\nEpoch 711, Loss: 0.00033098491257987916\nValidation Loss improved to 0.013458942994475365 Saving model 1\nEpoch 712, Loss: 0.000442660937551409\nValidation Loss improved to 0.013455783016979694 Saving model 1\nEpoch 713, Loss: 0.0007626906153745949\nValidation Loss improved to 0.013452834449708462 Saving model 1\nEpoch 714, Loss: 0.0004628394090104848\nValidation Loss improved to 0.013450335711240768 Saving model 1\nEpoch 715, Loss: 0.00040043232729658484\nValidation Loss improved to 0.013447336852550507 Saving model 1\nEpoch 716, Loss: 0.00032087776344269514\nValidation Loss improved to 0.013444331474602222 Saving model 1\nEpoch 717, Loss: 0.0003330764884594828\nValidation Loss improved to 0.01344134472310543 Saving model 1\nEpoch 718, Loss: 0.0002698454190976918\nValidation Loss improved to 0.013438687659800053 Saving model 1\nEpoch 719, Loss: 0.00028092804132029414\nValidation Loss improved to 0.01343591883778572 Saving model 1\nEpoch 720, Loss: 0.00033267715480178595\nValidation Loss improved to 0.01343277283012867 Saving model 1\nEpoch 721, Loss: 0.0004056834732182324\nValidation Loss improved to 0.013429997488856316 Saving model 1\nEpoch 722, Loss: 0.00033033618819899857\nValidation Loss improved to 0.01342717744410038 Saving model 1\nEpoch 723, Loss: 0.0004563464899547398\nValidation Loss improved to 0.01342399325221777 Saving model 1\nEpoch 724, Loss: 0.00039894215296953917\nValidation Loss improved to 0.013421198353171349 Saving model 1\nEpoch 725, Loss: 0.00028923043282702565\nValidation Loss improved to 0.013418277725577354 Saving model 1\nEpoch 726, Loss: 0.00028190723969601095\nValidation Loss improved to 0.013415418565273285 Saving model 1\nEpoch 727, Loss: 0.00030946065089665353\nValidation Loss improved to 0.013412726111710072 Saving model 1\nEpoch 728, Loss: 0.0003098989254795015\nValidation Loss improved to 0.01340988464653492 Saving model 1\nEpoch 729, Loss: 0.0003922971663996577\nValidation Loss improved to 0.013406798243522644 Saving model 1\nEpoch 730, Loss: 0.00036612292751669884\nValidation Loss improved to 0.013403695076704025 Saving model 1\nEpoch 731, Loss: 0.00034547189716249704\nValidation Loss improved to 0.013400658965110779 Saving model 1\nEpoch 732, Loss: 0.00027506958576850593\nValidation Loss improved to 0.013397637754678726 Saving model 1\nEpoch 733, Loss: 0.00033385097049176693\nValidation Loss improved to 0.013394949026405811 Saving model 1\nEpoch 734, Loss: 0.0004271035431884229\nValidation Loss improved to 0.013391593471169472 Saving model 1\nEpoch 735, Loss: 0.000327817106153816\nValidation Loss improved to 0.013388517312705517 Saving model 1\nEpoch 736, Loss: 0.00033697966136969626\nValidation Loss improved to 0.013385668396949768 Saving model 1\nEpoch 737, Loss: 0.000492420862428844\nValidation Loss improved to 0.013383273035287857 Saving model 1\nEpoch 738, Loss: 0.0004526124685071409\nValidation Loss improved to 0.01338006742298603 Saving model 1\nEpoch 739, Loss: 0.00033235817681998014\nValidation Loss improved to 0.013377208262681961 Saving model 1\nEpoch 740, Loss: 0.00030695705208927393\nValidation Loss improved to 0.013374270871281624 Saving model 1\nEpoch 741, Loss: 0.00032302745967172086\nValidation Loss improved to 0.01337120495736599 Saving model 1\nEpoch 742, Loss: 0.0003192724543623626\nValidation Loss improved to 0.01336840819567442 Saving model 1\nEpoch 743, Loss: 0.0002924222208093852\nValidation Loss improved to 0.013365533202886581 Saving model 1\nEpoch 744, Loss: 0.0004336986457929015\nValidation Loss improved to 0.013362431898713112 Saving model 1\nEpoch 745, Loss: 0.00045320537174120545\nValidation Loss improved to 0.013359461911022663 Saving model 1\nEpoch 746, Loss: 0.0004112659953534603\nValidation Loss improved to 0.013356199488043785 Saving model 1\nEpoch 747, Loss: 0.00030780557426624\nValidation Loss improved to 0.013353286311030388 Saving model 1\nEpoch 748, Loss: 0.000430030282586813\nValidation Loss improved to 0.013350168243050575 Saving model 1\nEpoch 749, Loss: 0.00034803265589289367\nValidation Loss improved to 0.013347089290618896 Saving model 1\nEpoch 750, Loss: 0.00034075192525051534\nValidation Loss improved to 0.013344264589250088 Saving model 1\nEpoch 751, Loss: 0.00035855796886608005\nValidation Loss improved to 0.013341069221496582 Saving model 1\nEpoch 752, Loss: 0.0005313028814271092\nValidation Loss improved to 0.013337777927517891 Saving model 1\nEpoch 753, Loss: 0.0005987955955788493\nValidation Loss improved to 0.013334954157471657 Saving model 1\nEpoch 754, Loss: 0.0003612472501117736\nValidation Loss improved to 0.01333201490342617 Saving model 1\nEpoch 755, Loss: 0.0002929855836555362\nValidation Loss improved to 0.01332936156541109 Saving model 1\nEpoch 756, Loss: 0.0002747678372543305\nValidation Loss improved to 0.013326642103493214 Saving model 1\nEpoch 757, Loss: 0.00029149939655326307\nValidation Loss improved to 0.013323713093996048 Saving model 1\nEpoch 758, Loss: 0.0004202828276902437\nValidation Loss improved to 0.013320639729499817 Saving model 1\nEpoch 759, Loss: 0.0003789463371504098\nValidation Loss improved to 0.013317652978003025 Saving model 1\nEpoch 760, Loss: 0.00042460419354029\nValidation Loss improved to 0.013314672745764256 Saving model 1\nEpoch 761, Loss: 0.000312814605422318\nValidation Loss improved to 0.013311713002622128 Saving model 1\nEpoch 762, Loss: 0.0003032955573871732\nValidation Loss improved to 0.013308973982930183 Saving model 1\nEpoch 763, Loss: 0.000263447524048388\nValidation Loss improved to 0.013305991888046265 Saving model 1\nEpoch 764, Loss: 0.00037200411316007376\nValidation Loss improved to 0.013302765786647797 Saving model 1\nEpoch 765, Loss: 0.00035651688813231885\nValidation Loss improved to 0.013299990445375443 Saving model 1\nEpoch 766, Loss: 0.0002741844509728253\nValidation Loss improved to 0.013297409750521183 Saving model 1\nEpoch 767, Loss: 0.0003830486093647778\nValidation Loss improved to 0.013294447213411331 Saving model 1\nEpoch 768, Loss: 0.00046523785567842424\nValidation Loss improved to 0.013291547074913979 Saving model 1\nEpoch 769, Loss: 0.0003403248847462237\nValidation Loss improved to 0.01328867208212614 Saving model 1\nEpoch 770, Loss: 0.0003505095955915749\nValidation Loss improved to 0.013285691849887371 Saving model 1\nEpoch 771, Loss: 0.0005304687656462193\nValidation Loss improved to 0.013282696716487408 Saving model 1\nEpoch 772, Loss: 0.0004050508141517639\nValidation Loss improved to 0.013279867358505726 Saving model 1\nEpoch 773, Loss: 0.0003517150762490928\nValidation Loss improved to 0.013276892714202404 Saving model 1\nEpoch 774, Loss: 0.0004498991183936596\nValidation Loss improved to 0.013274024240672588 Saving model 1\nEpoch 775, Loss: 0.00033731525763869286\nValidation Loss improved to 0.013271189294755459 Saving model 1\nEpoch 776, Loss: 0.00027018412947654724\nValidation Loss improved to 0.013268406502902508 Saving model 1\nEpoch 777, Loss: 0.00042368454160168767\nValidation Loss improved to 0.01326513011008501 Saving model 1\nEpoch 778, Loss: 0.000506087439134717\nValidation Loss improved to 0.013261874206364155 Saving model 1\nEpoch 779, Loss: 0.00032169188489206135\nValidation Loss improved to 0.013258951716125011 Saving model 1\nEpoch 780, Loss: 0.0003189484414178878\nValidation Loss improved to 0.013256184756755829 Saving model 1\nEpoch 781, Loss: 0.00036517594708129764\nValidation Loss improved to 0.013253438286483288 Saving model 1\nEpoch 782, Loss: 0.0004370507667772472\nValidation Loss improved to 0.013250909745693207 Saving model 1\nEpoch 783, Loss: 0.0003075828426517546\nValidation Loss improved to 0.013248125091195107 Saving model 1\nEpoch 784, Loss: 0.00030919897835701704\nValidation Loss improved to 0.013245312497019768 Saving model 1\nEpoch 785, Loss: 0.00029404828092083335\nValidation Loss improved to 0.013242436572909355 Saving model 1\nEpoch 786, Loss: 0.0002702195488382131\nValidation Loss improved to 0.013239392079412937 Saving model 1\nEpoch 787, Loss: 0.0003693236212711781\nValidation Loss improved to 0.013236532919108868 Saving model 1\nEpoch 788, Loss: 0.0003950715181417763\nValidation Loss improved to 0.013233572244644165 Saving model 1\nEpoch 789, Loss: 0.0003352277562953532\nValidation Loss improved to 0.013230998069047928 Saving model 1\nEpoch 790, Loss: 0.0003525212232489139\nValidation Loss improved to 0.01322821993380785 Saving model 1\nEpoch 791, Loss: 0.00029140309197828174\nValidation Loss improved to 0.013225404545664787 Saving model 1\nEpoch 792, Loss: 0.0002572173543740064\nValidation Loss improved to 0.013222580775618553 Saving model 1\nEpoch 793, Loss: 0.0003532197151798755\nValidation Loss improved to 0.013219759799540043 Saving model 1\nEpoch 794, Loss: 0.0002922987041529268\nValidation Loss improved to 0.013216949999332428 Saving model 1\nEpoch 795, Loss: 0.00024596197181381285\nValidation Loss improved to 0.013214092701673508 Saving model 1\nEpoch 796, Loss: 0.0002821296511683613\nValidation Loss improved to 0.013211130164563656 Saving model 1\nEpoch 797, Loss: 0.00042746172402985394\nValidation Loss improved to 0.0132084209471941 Saving model 1\nEpoch 798, Loss: 0.00040992264985106885\nValidation Loss improved to 0.013205685652792454 Saving model 1\nEpoch 799, Loss: 0.0002891391923185438\nValidation Loss improved to 0.013203063048422337 Saving model 1\nEpoch 800, Loss: 0.00028826380730606616\nValidation Loss improved to 0.013200202956795692 Saving model 1\nEpoch 801, Loss: 0.0003671626327559352\nValidation Loss improved to 0.013197292573750019 Saving model 1\nEpoch 802, Loss: 0.00031594003667123616\nValidation Loss improved to 0.013194572180509567 Saving model 1\nEpoch 803, Loss: 0.00032424283563159406\nValidation Loss improved to 0.013191726058721542 Saving model 1\nEpoch 804, Loss: 0.00047117346548475325\nValidation Loss improved to 0.01318901777267456 Saving model 1\nEpoch 805, Loss: 0.0005242370534688234\nValidation Loss improved to 0.013186450116336346 Saving model 1\nEpoch 806, Loss: 0.00037640714435838163\nValidation Loss improved to 0.013183333911001682 Saving model 1\nEpoch 807, Loss: 0.00028429602389223874\nValidation Loss improved to 0.013180573470890522 Saving model 1\nEpoch 808, Loss: 0.00027042688452638686\nValidation Loss improved to 0.013177831657230854 Saving model 1\nEpoch 809, Loss: 0.0004089516296517104\nValidation Loss improved to 0.013174834661185741 Saving model 1\nEpoch 810, Loss: 0.0003627072146628052\nValidation Loss improved to 0.013172075152397156 Saving model 1\nEpoch 811, Loss: 0.0002597954880911857\nValidation Loss improved to 0.013169508427381516 Saving model 1\nEpoch 812, Loss: 0.0003295031492598355\nValidation Loss improved to 0.013166538439691067 Saving model 1\nEpoch 813, Loss: 0.00030285355751402676\nValidation Loss improved to 0.013163729570806026 Saving model 1\nEpoch 814, Loss: 0.00033219504985027015\nValidation Loss improved to 0.013160938397049904 Saving model 1\nEpoch 815, Loss: 0.00031714391661807895\nValidation Loss improved to 0.01315818727016449 Saving model 1\nEpoch 816, Loss: 0.0004721281584352255\nValidation Loss improved to 0.013155187480151653 Saving model 1\nEpoch 817, Loss: 0.000272644916549325\nValidation Loss improved to 0.013152556493878365 Saving model 1\nEpoch 818, Loss: 0.0003235611948184669\nValidation Loss improved to 0.013149852864444256 Saving model 1\nEpoch 819, Loss: 0.00026759080355986953\nValidation Loss improved to 0.013147173449397087 Saving model 1\nEpoch 820, Loss: 0.0003303330158814788\nValidation Loss improved to 0.013144477270543575 Saving model 1\nEpoch 821, Loss: 0.0003700369852595031\nValidation Loss improved to 0.013141372241079807 Saving model 1\nEpoch 822, Loss: 0.0003329027385916561\nValidation Loss improved to 0.01313876174390316 Saving model 1\nEpoch 823, Loss: 0.0004246253811288625\nValidation Loss improved to 0.01313584204763174 Saving model 1\nEpoch 824, Loss: 0.0004853221180383116\nValidation Loss improved to 0.013133238069713116 Saving model 1\nEpoch 825, Loss: 0.0002709459513425827\nValidation Loss improved to 0.013130457140505314 Saving model 1\nEpoch 826, Loss: 0.000504261814057827\nValidation Loss improved to 0.013127194717526436 Saving model 1\nEpoch 827, Loss: 0.0004570608143694699\nValidation Loss improved to 0.01312448550015688 Saving model 1\nEpoch 828, Loss: 0.0003013176319655031\nValidation Loss improved to 0.013121901080012321 Saving model 1\nEpoch 829, Loss: 0.0003628709237091243\nValidation Loss improved to 0.013119259849190712 Saving model 1\nEpoch 830, Loss: 0.0003223038511350751\nValidation Loss improved to 0.013116497546434402 Saving model 1\nEpoch 831, Loss: 0.0003371015191078186\nValidation Loss improved to 0.0131137790158391 Saving model 1\nEpoch 832, Loss: 0.00046740580000914633\nValidation Loss improved to 0.013111269101500511 Saving model 1\nEpoch 833, Loss: 0.0002481363480910659\nValidation Loss improved to 0.013108879327774048 Saving model 1\nEpoch 834, Loss: 0.00032982384436763823\nValidation Loss improved to 0.013106134720146656 Saving model 1\nEpoch 835, Loss: 0.00026409735437482595\nValidation Loss improved to 0.013103551231324673 Saving model 1\nEpoch 836, Loss: 0.0003121989720966667\nValidation Loss improved to 0.013101104646921158 Saving model 1\nEpoch 837, Loss: 0.0003297301009297371\nValidation Loss improved to 0.013098452240228653 Saving model 1\nEpoch 838, Loss: 0.00034603761741891503\nValidation Loss improved to 0.013095521368086338 Saving model 1\nEpoch 839, Loss: 0.0003121571789961308\nValidation Loss improved to 0.013092750683426857 Saving model 1\nEpoch 840, Loss: 0.0003038257418666035\nValidation Loss improved to 0.01308988593518734 Saving model 1\nEpoch 841, Loss: 0.0002996422117576003\nValidation Loss improved to 0.013086782768368721 Saving model 1\nEpoch 842, Loss: 0.00037242763210088015\nValidation Loss improved to 0.013084163889288902 Saving model 1\nEpoch 843, Loss: 0.0002309868432348594\nValidation Loss improved to 0.0130815040320158 Saving model 1\nEpoch 844, Loss: 0.0003196825855411589\nValidation Loss improved to 0.013078749179840088 Saving model 1\nEpoch 845, Loss: 0.0003383682924322784\nValidation Loss improved to 0.013076110742986202 Saving model 1\nEpoch 846, Loss: 0.0003145974478684366\nValidation Loss improved to 0.01307336799800396 Saving model 1\nEpoch 847, Loss: 0.00030702410731464624\nValidation Loss improved to 0.013070676475763321 Saving model 1\nEpoch 848, Loss: 0.000318396050715819\nValidation Loss improved to 0.013068269938230515 Saving model 1\nEpoch 849, Loss: 0.000278485007584095\nValidation Loss improved to 0.013065578415989876 Saving model 1\nEpoch 850, Loss: 0.0003731795004568994\nValidation Loss improved to 0.013063140213489532 Saving model 1\nEpoch 851, Loss: 0.00041311027598567307\nValidation Loss improved to 0.013060376979410648 Saving model 1\nEpoch 852, Loss: 0.00040235675987787545\nValidation Loss improved to 0.0130576491355896 Saving model 1\nEpoch 853, Loss: 0.0005199830047786236\nValidation Loss improved to 0.013054595328867435 Saving model 1\nEpoch 854, Loss: 0.0002945939777418971\nValidation Loss improved to 0.013052243739366531 Saving model 1\nEpoch 855, Loss: 0.0002718203468248248\nValidation Loss improved to 0.013049363158643246 Saving model 1\nEpoch 856, Loss: 0.0002611386589705944\nValidation Loss improved to 0.013046705164015293 Saving model 1\nEpoch 857, Loss: 0.0002355977485422045\nValidation Loss improved to 0.013044297695159912 Saving model 1\nEpoch 858, Loss: 0.0002920856641139835\nValidation Loss improved to 0.013041717931628227 Saving model 1\nEpoch 859, Loss: 0.0002756192407105118\nValidation Loss improved to 0.013038983568549156 Saving model 1\nEpoch 860, Loss: 0.00030505002359859645\nValidation Loss improved to 0.013036122545599937 Saving model 1\nEpoch 861, Loss: 0.0003936633002012968\nValidation Loss improved to 0.013033303432166576 Saving model 1\nEpoch 862, Loss: 0.0003640491049736738\nValidation Loss improved to 0.013030610047280788 Saving model 1\nEpoch 863, Loss: 0.00029745808569714427\nValidation Loss improved to 0.013027937151491642 Saving model 1\nEpoch 864, Loss: 0.00022856052964925766\nValidation Loss improved to 0.013025163672864437 Saving model 1\nEpoch 865, Loss: 0.000306207308312878\nValidation Loss improved to 0.01302250288426876 Saving model 1\nEpoch 866, Loss: 0.00030608384986408055\nValidation Loss improved to 0.01301976852118969 Saving model 1\nEpoch 867, Loss: 0.0003022780583705753\nValidation Loss improved to 0.013017053715884686 Saving model 1\nEpoch 868, Loss: 0.00036488877958618104\nValidation Loss improved to 0.013014188036322594 Saving model 1\nEpoch 869, Loss: 0.0002870724711101502\nValidation Loss improved to 0.013011799193918705 Saving model 1\nEpoch 870, Loss: 0.00045398608199320734\nValidation Loss improved to 0.013009198941290379 Saving model 1\nEpoch 871, Loss: 0.00030103596509434283\nValidation Loss improved to 0.013006458058953285 Saving model 1\nEpoch 872, Loss: 0.00036976023693569005\nValidation Loss improved to 0.013003651052713394 Saving model 1\nEpoch 873, Loss: 0.0005361700314097106\nValidation Loss improved to 0.01300099864602089 Saving model 1\nEpoch 874, Loss: 0.0003385006857570261\nValidation Loss improved to 0.012998517602682114 Saving model 1\nEpoch 875, Loss: 0.00033870822517201304\nValidation Loss improved to 0.012995680794119835 Saving model 1\nEpoch 876, Loss: 0.0003084193158429116\nValidation Loss improved to 0.012993143871426582 Saving model 1\nEpoch 877, Loss: 0.0002986493636853993\nValidation Loss improved to 0.01299038901925087 Saving model 1\nEpoch 878, Loss: 0.00038415685412473977\nValidation Loss improved to 0.012988007627427578 Saving model 1\nEpoch 879, Loss: 0.0004124944971408695\nValidation Loss improved to 0.012985115870833397 Saving model 1\nEpoch 880, Loss: 0.00036028551403433084\nValidation Loss improved to 0.012982513755559921 Saving model 1\nEpoch 881, Loss: 0.00030235486337915063\nValidation Loss improved to 0.012980163097381592 Saving model 1\nEpoch 882, Loss: 0.0003008261846844107\nValidation Loss improved to 0.01297757402062416 Saving model 1\nEpoch 883, Loss: 0.0002680006728041917\nValidation Loss improved to 0.012975065968930721 Saving model 1\nEpoch 884, Loss: 0.00028424966149032116\nValidation Loss improved to 0.012972126714885235 Saving model 1\nEpoch 885, Loss: 0.0003939222660847008\nValidation Loss improved to 0.012969762086868286 Saving model 1\nEpoch 886, Loss: 0.0002648532681632787\nValidation Loss improved to 0.012966996990144253 Saving model 1\nEpoch 887, Loss: 0.00025308935437351465\nValidation Loss improved to 0.012964225374162197 Saving model 1\nEpoch 888, Loss: 0.000337084784405306\nValidation Loss improved to 0.012961567379534245 Saving model 1\nEpoch 889, Loss: 0.00028333242516964674\nValidation Loss improved to 0.01295927818864584 Saving model 1\nEpoch 890, Loss: 0.00040430467925034463\nValidation Loss improved to 0.012956654652953148 Saving model 1\nEpoch 891, Loss: 0.000431881460826844\nValidation Loss improved to 0.01295400783419609 Saving model 1\nEpoch 892, Loss: 0.0003692921600304544\nValidation Loss improved to 0.012951342388987541 Saving model 1\nEpoch 893, Loss: 0.0003053845721296966\nValidation Loss improved to 0.012948825024068356 Saving model 1\nEpoch 894, Loss: 0.000343719613738358\nValidation Loss improved to 0.012945843860507011 Saving model 1\nEpoch 895, Loss: 0.00040058340528048575\nValidation Loss improved to 0.012943457812070847 Saving model 1\nEpoch 896, Loss: 0.00027140002930536866\nValidation Loss improved to 0.01294094230979681 Saving model 1\nEpoch 897, Loss: 0.00040189456194639206\nValidation Loss improved to 0.012938317842781544 Saving model 1\nEpoch 898, Loss: 0.00039771603769622743\nValidation Loss improved to 0.012935985811054707 Saving model 1\nEpoch 899, Loss: 0.0004000755143351853\nValidation Loss improved to 0.012933427467942238 Saving model 1\nEpoch 900, Loss: 0.0003079681482631713\nValidation Loss improved to 0.012930834665894508 Saving model 1\nEpoch 901, Loss: 0.0002636765129864216\nValidation Loss improved to 0.01292827632278204 Saving model 1\nEpoch 902, Loss: 0.0002820404479280114\nValidation Loss improved to 0.012925900518894196 Saving model 1\nEpoch 903, Loss: 0.0002799142093863338\nValidation Loss improved to 0.012923353351652622 Saving model 1\nEpoch 904, Loss: 0.0003218374913558364\nValidation Loss improved to 0.012920734472572803 Saving model 1\nEpoch 905, Loss: 0.00025870627723634243\nValidation Loss improved to 0.012918061576783657 Saving model 1\nEpoch 906, Loss: 0.00028390117222443223\nValidation Loss improved to 0.012915675528347492 Saving model 1\nEpoch 907, Loss: 0.00031534285517409444\nValidation Loss improved to 0.012913077138364315 Saving model 1\nEpoch 908, Loss: 0.0003382446593604982\nValidation Loss improved to 0.012910369783639908 Saving model 1\nEpoch 909, Loss: 0.0003728453302755952\nValidation Loss improved to 0.012907984666526318 Saving model 1\nEpoch 910, Loss: 0.00034758413676172495\nValidation Loss improved to 0.012905429117381573 Saving model 1\nEpoch 911, Loss: 0.0004126557323615998\nValidation Loss improved to 0.012902889400720596 Saving model 1\nEpoch 912, Loss: 0.00045361483353190124\nValidation Loss improved to 0.01290006935596466 Saving model 1\nEpoch 913, Loss: 0.0003572857240214944\nValidation Loss improved to 0.012897476553916931 Saving model 1\nEpoch 914, Loss: 0.00038040662184357643\nValidation Loss improved to 0.012895213440060616 Saving model 1\nEpoch 915, Loss: 0.00033106980845332146\nValidation Loss improved to 0.01289245393127203 Saving model 1\nEpoch 916, Loss: 0.00026567705208435655\nValidation Loss improved to 0.012890017591416836 Saving model 1\nEpoch 917, Loss: 0.00025495022418908775\nValidation Loss improved to 0.012887673452496529 Saving model 1\nEpoch 918, Loss: 0.00030974613036960363\nValidation Loss improved to 0.012885117903351784 Saving model 1\nEpoch 919, Loss: 0.0003238019999116659\nValidation Loss improved to 0.012882507406175137 Saving model 1\nEpoch 920, Loss: 0.0004042302898596972\nValidation Loss improved to 0.012879900634288788 Saving model 1\nEpoch 921, Loss: 0.0002717506722547114\nValidation Loss improved to 0.012877504341304302 Saving model 1\nEpoch 922, Loss: 0.00030395708745345473\nValidation Loss improved to 0.012875018641352654 Saving model 1\nEpoch 923, Loss: 0.0002893039199989289\nValidation Loss improved to 0.012872569262981415 Saving model 1\nEpoch 924, Loss: 0.00028245736029930413\nValidation Loss improved to 0.012870066799223423 Saving model 1\nEpoch 925, Loss: 0.00031541817588731647\nValidation Loss improved to 0.012867585755884647 Saving model 1\nEpoch 926, Loss: 0.00030663920915685594\nValidation Loss improved to 0.012865071184933186 Saving model 1\nEpoch 927, Loss: 0.0002629481314215809\nValidation Loss improved to 0.01286262460052967 Saving model 1\nEpoch 928, Loss: 0.0002766084799077362\nValidation Loss improved to 0.012860294431447983 Saving model 1\nEpoch 929, Loss: 0.00025484993238933384\nValidation Loss improved to 0.012857993133366108 Saving model 1\nEpoch 930, Loss: 0.00030320134828798473\nValidation Loss improved to 0.012855708599090576 Saving model 1\nEpoch 931, Loss: 0.0002655193384271115\nValidation Loss improved to 0.012853212654590607 Saving model 1\nEpoch 932, Loss: 0.0002568890049587935\nValidation Loss improved to 0.012850916013121605 Saving model 1\nEpoch 933, Loss: 0.0003651884908322245\nValidation Loss improved to 0.012848341837525368 Saving model 1\nEpoch 934, Loss: 0.00026133033679798245\nValidation Loss improved to 0.012845957651734352 Saving model 1\nEpoch 935, Loss: 0.0003072448307648301\nValidation Loss improved to 0.012843629345297813 Saving model 1\nEpoch 936, Loss: 0.0003383104340173304\nValidation Loss improved to 0.01284095086157322 Saving model 1\nEpoch 937, Loss: 0.00041272465023212135\nValidation Loss improved to 0.012838579714298248 Saving model 1\nEpoch 938, Loss: 0.00030579246231354773\nValidation Loss improved to 0.012835999950766563 Saving model 1\nEpoch 939, Loss: 0.00044556782813742757\nValidation Loss improved to 0.01283358782529831 Saving model 1\nEpoch 940, Loss: 0.00041987485019490123\nValidation Loss improved to 0.012830798514187336 Saving model 1\nEpoch 941, Loss: 0.00038912356831133366\nValidation Loss improved to 0.012828437611460686 Saving model 1\nEpoch 942, Loss: 0.0003522510814946145\nValidation Loss improved to 0.012826134450733662 Saving model 1\nEpoch 943, Loss: 0.00027882185531780124\nValidation Loss improved to 0.01282369066029787 Saving model 1\nEpoch 944, Loss: 0.000253762467764318\nValidation Loss improved to 0.012821335345506668 Saving model 1\nEpoch 945, Loss: 0.0002699917822610587\nValidation Loss improved to 0.012818913906812668 Saving model 1\nEpoch 946, Loss: 0.00029762685880996287\nValidation Loss improved to 0.012816745787858963 Saving model 1\nEpoch 947, Loss: 0.0003314595378469676\nValidation Loss improved to 0.012814235873520374 Saving model 1\nEpoch 948, Loss: 0.00027208763640373945\nValidation Loss improved to 0.01281197089701891 Saving model 1\nEpoch 949, Loss: 0.00025675754295662045\nValidation Loss improved to 0.012809359468519688 Saving model 1\nEpoch 950, Loss: 0.00031615878106094897\nValidation Loss improved to 0.01280691847205162 Saving model 1\nEpoch 951, Loss: 0.00029580394038930535\nValidation Loss improved to 0.012804336845874786 Saving model 1\nEpoch 952, Loss: 0.00033309785067103803\nValidation Loss improved to 0.012801862321794033 Saving model 1\nEpoch 953, Loss: 0.00027166627114638686\nValidation Loss improved to 0.012799334712326527 Saving model 1\nEpoch 954, Loss: 0.00031355206738226116\nValidation Loss improved to 0.012796585448086262 Saving model 1\nEpoch 955, Loss: 0.0002626175119075924\nValidation Loss improved to 0.01279416959732771 Saving model 1\nEpoch 956, Loss: 0.0002344446984352544\nValidation Loss improved to 0.012792058289051056 Saving model 1\nEpoch 957, Loss: 0.00028700989787466824\nValidation Loss improved to 0.012789834290742874 Saving model 1\nEpoch 958, Loss: 0.00028854553238488734\nValidation Loss improved to 0.01278779748827219 Saving model 1\nEpoch 959, Loss: 0.000429087842348963\nValidation Loss improved to 0.012785292230546474 Saving model 1\nEpoch 960, Loss: 0.0002670769754331559\nValidation Loss improved to 0.01278303749859333 Saving model 1\nEpoch 961, Loss: 0.00038021098589524627\nValidation Loss improved to 0.012780627235770226 Saving model 1\nEpoch 962, Loss: 0.00043954700231552124\nValidation Loss improved to 0.012778304517269135 Saving model 1\nEpoch 963, Loss: 0.0003302245168015361\nValidation Loss improved to 0.012776040472090244 Saving model 1\nEpoch 964, Loss: 0.0002725514932535589\nValidation Loss improved to 0.012773633003234863 Saving model 1\nEpoch 965, Loss: 0.0003295197384431958\nValidation Loss improved to 0.012771076522767544 Saving model 1\nEpoch 966, Loss: 0.00036536919651553035\nValidation Loss improved to 0.012768838554620743 Saving model 1\nEpoch 967, Loss: 0.0003118032473139465\nValidation Loss improved to 0.012766486965119839 Saving model 1\nEpoch 968, Loss: 0.00033324805554002523\nValidation Loss improved to 0.012764140963554382 Saving model 1\nEpoch 969, Loss: 0.0002551299112383276\nValidation Loss improved to 0.01276200171560049 Saving model 1\nEpoch 970, Loss: 0.0002513512154109776\nValidation Loss improved to 0.012759720906615257 Saving model 1\nEpoch 971, Loss: 0.0002653119445312768\nValidation Loss improved to 0.012757577002048492 Saving model 1\nEpoch 972, Loss: 0.0002832409518305212\nValidation Loss improved to 0.012755277566611767 Saving model 1\nEpoch 973, Loss: 0.0002832581230904907\nValidation Loss improved to 0.012753155082464218 Saving model 1\nEpoch 974, Loss: 0.0003160345950163901\nValidation Loss improved to 0.01275093574076891 Saving model 1\nEpoch 975, Loss: 0.00043633542372845113\nValidation Loss improved to 0.012748422101140022 Saving model 1\nEpoch 976, Loss: 0.0003926382341887802\nValidation Loss improved to 0.012745947577059269 Saving model 1\nEpoch 977, Loss: 0.0002876515791285783\nValidation Loss improved to 0.012743636034429073 Saving model 1\nEpoch 978, Loss: 0.000290037743980065\nValidation Loss improved to 0.012741200625896454 Saving model 1\nEpoch 979, Loss: 0.00026028629508800805\nValidation Loss improved to 0.012739003635942936 Saving model 1\nEpoch 980, Loss: 0.0003991809790022671\nValidation Loss improved to 0.012736416421830654 Saving model 1\nEpoch 981, Loss: 0.0003510388487484306\nValidation Loss improved to 0.012734154239296913 Saving model 1\nEpoch 982, Loss: 0.0002244625793537125\nValidation Loss improved to 0.012731711380183697 Saving model 1\nEpoch 983, Loss: 0.00026160856941714883\nValidation Loss improved to 0.012729504145681858 Saving model 1\nEpoch 984, Loss: 0.00027371052419766784\nValidation Loss improved to 0.012727285735309124 Saving model 1\nEpoch 985, Loss: 0.0003374032967258245\nValidation Loss improved to 0.012725096195936203 Saving model 1\nEpoch 986, Loss: 0.00031830472289584577\nValidation Loss improved to 0.012722780928015709 Saving model 1\nEpoch 987, Loss: 0.0002561849250923842\nValidation Loss improved to 0.01272056344896555 Saving model 1\nEpoch 988, Loss: 0.00026020759833045304\nValidation Loss improved to 0.012718484736979008 Saving model 1\nEpoch 989, Loss: 0.0002762934600468725\nValidation Loss improved to 0.012715951539576054 Saving model 1\nEpoch 990, Loss: 0.0003118521417491138\nValidation Loss improved to 0.012713690288364887 Saving model 1\nEpoch 991, Loss: 0.0003498152655083686\nValidation Loss improved to 0.012711557559669018 Saving model 1\nEpoch 992, Loss: 0.0003443422610871494\nValidation Loss improved to 0.012709466740489006 Saving model 1\nEpoch 993, Loss: 0.00025661580730229616\nValidation Loss improved to 0.012707105837762356 Saving model 1\nEpoch 994, Loss: 0.0002807428245432675\nValidation Loss improved to 0.012704768218100071 Saving model 1\nEpoch 995, Loss: 0.00028600002406165004\nValidation Loss improved to 0.012702619656920433 Saving model 1\nEpoch 996, Loss: 0.00032287335488945246\nValidation Loss improved to 0.012700255960226059 Saving model 1\nEpoch 997, Loss: 0.00023595787934027612\nValidation Loss improved to 0.012698006816208363 Saving model 1\nEpoch 998, Loss: 0.00024417779059149325\nValidation Loss improved to 0.012695708312094212 Saving model 1\nEpoch 999, Loss: 0.0002836289058905095\nValidation Loss improved to 0.01269336137920618 Saving model 1\nEpoch 0, Loss: 0.20984084904193878\nValidation Loss improved to 0.06787514686584473 Saving model 2\nEpoch 1, Loss: 0.09791804850101471\nValidation Loss improved to 0.0675516426563263 Saving model 2\nEpoch 2, Loss: 0.0728946104645729\nValidation Loss improved to 0.06571416556835175 Saving model 2\nEpoch 3, Loss: 0.043897587805986404\nValidation Loss improved to 0.06295391172170639 Saving model 2\nEpoch 4, Loss: 0.02742226980626583\nValidation Loss improved to 0.05548001453280449 Saving model 2\nEpoch 5, Loss: 0.02596796303987503\nValidation Loss improved to 0.05258379504084587 Saving model 2\nEpoch 6, Loss: 0.030124573037028313\nValidation Loss improved to 0.050220221281051636 Saving model 2\nEpoch 7, Loss: 0.03641853481531143\nValidation Loss improved to 0.046450063586235046 Saving model 2\nEpoch 8, Loss: 0.02174578607082367\nValidation Loss improved to 0.04480386897921562 Saving model 2\nEpoch 9, Loss: 0.015265729278326035\nValidation Loss improved to 0.04371742904186249 Saving model 2\nEpoch 10, Loss: 0.019166182726621628\nValidation Loss improved to 0.042521264404058456 Saving model 2\nEpoch 11, Loss: 0.016880933195352554\nValidation Loss improved to 0.04239000752568245 Saving model 2\nEpoch 12, Loss: 0.01863138936460018\nValidation Loss improved to 0.04195645451545715 Saving model 2\nEpoch 13, Loss: 0.017455780878663063\nEpoch 14, Loss: 0.015342297963798046\nValidation Loss improved to 0.04185844585299492 Saving model 2\nEpoch 15, Loss: 0.01475155632942915\nValidation Loss improved to 0.04154155030846596 Saving model 2\nEpoch 16, Loss: 0.018899304792284966\nValidation Loss improved to 0.04100039228796959 Saving model 2\nEpoch 17, Loss: 0.0223714429885149\nValidation Loss improved to 0.04090859740972519 Saving model 2\nEpoch 18, Loss: 0.016696978360414505\nValidation Loss improved to 0.04072142764925957 Saving model 2\nEpoch 19, Loss: 0.014338855631649494\nValidation Loss improved to 0.04070422425866127 Saving model 2\nEpoch 20, Loss: 0.013690450228750706\nValidation Loss improved to 0.04061851650476456 Saving model 2\nEpoch 21, Loss: 0.014185055159032345\nEpoch 22, Loss: 0.021575599908828735\nEpoch 23, Loss: 0.014234314672648907\nEpoch 24, Loss: 0.015451817773282528\nEpoch 25, Loss: 0.01642110012471676\nEpoch 26, Loss: 0.010531874373555183\nEpoch 27, Loss: 0.00990324467420578\nEpoch 28, Loss: 0.01056367252022028\nEpoch 29, Loss: 0.011524945497512817\nEpoch 30, Loss: 0.010392454452812672\nEpoch 31, Loss: 0.01672789454460144\nEpoch    32: reducing learning rate of group 0 to 1.5000e-04.\nEpoch 32, Loss: 0.009313416667282581\nEpoch 33, Loss: 0.007837019860744476\nEpoch 34, Loss: 0.006903532426804304\nEpoch 35, Loss: 0.006606726907193661\nEpoch 36, Loss: 0.0062371352687478065\nEpoch 37, Loss: 0.005913121160119772\nEpoch 38, Loss: 0.005825976841151714\nEpoch 39, Loss: 0.005669236648827791\nEpoch 40, Loss: 0.005663013551384211\nEpoch 41, Loss: 0.005526542663574219\nEpoch 42, Loss: 0.005468680057674646\nEpoch    43: reducing learning rate of group 0 to 7.5000e-06.\nEpoch 43, Loss: 0.005877287592738867\nEpoch 44, Loss: 0.005424720700830221\nEpoch 45, Loss: 0.005251630209386349\nEpoch 46, Loss: 0.005256173200905323\nEpoch 47, Loss: 0.005257495678961277\nEpoch 48, Loss: 0.005236515775322914\nEpoch 49, Loss: 0.005246691405773163\nEpoch 50, Loss: 0.005227971356362104\nEpoch 51, Loss: 0.005228906869888306\nEpoch 52, Loss: 0.005219960585236549\nEpoch 53, Loss: 0.005220403894782066\nEpoch    54: reducing learning rate of group 0 to 3.7500e-07.\nEpoch 54, Loss: 0.005201437510550022\nEpoch 55, Loss: 0.0052002971060574055\nEpoch 56, Loss: 0.005199918057769537\nEpoch 57, Loss: 0.005199707578867674\nEpoch 58, Loss: 0.005199442617595196\nEpoch 59, Loss: 0.0051992302760481834\nEpoch 60, Loss: 0.005199119448661804\nEpoch 61, Loss: 0.005199596285820007\nEpoch 62, Loss: 0.005198040511459112\nEpoch 63, Loss: 0.005198103841394186\nEpoch 64, Loss: 0.005197592079639435\nEpoch    65: reducing learning rate of group 0 to 1.8750e-08.\nEpoch 65, Loss: 0.005197114311158657\nEpoch 66, Loss: 0.005197082180529833\nEpoch 67, Loss: 0.00519707053899765\nEpoch 68, Loss: 0.005197069142013788\nEpoch 69, Loss: 0.0051970528438687325\nEpoch 70, Loss: 0.0051971073262393475\nEpoch 71, Loss: 0.0051970528438687325\nEpoch 72, Loss: 0.005197013728320599\nEpoch 73, Loss: 0.005197030957788229\nEpoch 74, Loss: 0.005197041667997837\nEpoch 75, Loss: 0.005196987185627222\nEpoch    76: reducing learning rate of group 0 to 9.3750e-10.\nEpoch 76, Loss: 0.005196958314627409\nEpoch 77, Loss: 0.005196958780288696\nEpoch 78, Loss: 0.005196958314627409\nEpoch 79, Loss: 0.005196958314627409\nEpoch 80, Loss: 0.005196958314627409\nEpoch 81, Loss: 0.005196958314627409\nEpoch 82, Loss: 0.005196958780288696\nEpoch 83, Loss: 0.005196958780288696\nEpoch 84, Loss: 0.005196958314627409\nEpoch 85, Loss: 0.005196958314627409\nEpoch 86, Loss: 0.005196958780288696\nEpoch 87, Loss: 0.005196958314627409\nEpoch 88, Loss: 0.005196958780288696\nEpoch 89, Loss: 0.005196958780288696\nEpoch 90, Loss: 0.005196959245949984\nEpoch 91, Loss: 0.005196959245949984\nEpoch 92, Loss: 0.005196958780288696\nEpoch 93, Loss: 0.005196957848966122\nEpoch 94, Loss: 0.005196958314627409\nEpoch 95, Loss: 0.005196958780288696\nEpoch 96, Loss: 0.005196957848966122\nEpoch 97, Loss: 0.005196958314627409\nEpoch 98, Loss: 0.005196958780288696\nEpoch 99, Loss: 0.005196958314627409\nEpoch 100, Loss: 0.005196958314627409\nEpoch 101, Loss: 0.005196958780288696\nEpoch 102, Loss: 0.005196959245949984\nEpoch 103, Loss: 0.005196959711611271\nEpoch 104, Loss: 0.005196960177272558\nEpoch 105, Loss: 0.005196958780288696\nEpoch 106, Loss: 0.005196958314627409\nEpoch 107, Loss: 0.005196958314627409\nEpoch 108, Loss: 0.005196957848966122\nEpoch 109, Loss: 0.005196958314627409\nEpoch 110, Loss: 0.005196958314627409\nEpoch 111, Loss: 0.005196957848966122\nEpoch 112, Loss: 0.005196957383304834\nEpoch 113, Loss: 0.005196956917643547\nEpoch 114, Loss: 0.005196957383304834\nEpoch 115, Loss: 0.005196957383304834\nEpoch 116, Loss: 0.005196957848966122\nEpoch 117, Loss: 0.005196957848966122\nEpoch 118, Loss: 0.005196957848966122\nEpoch 119, Loss: 0.005196957383304834\nEpoch 120, Loss: 0.005196956917643547\nEpoch 121, Loss: 0.005196957383304834\nEpoch 122, Loss: 0.00519695645198226\nEpoch 123, Loss: 0.005196957383304834\nEpoch 124, Loss: 0.005196957383304834\nEpoch 125, Loss: 0.005196957848966122\nEpoch 126, Loss: 0.005196956917643547\nEpoch 127, Loss: 0.005196957848966122\nEpoch 128, Loss: 0.005196957383304834\nEpoch 129, Loss: 0.005196956917643547\nEpoch 130, Loss: 0.005196957848966122\nEpoch 131, Loss: 0.005196957383304834\nEpoch 132, Loss: 0.005196957848966122\nEpoch 133, Loss: 0.005196957848966122\nEpoch 134, Loss: 0.005196957848966122\nEpoch 135, Loss: 0.005196956917643547\nEpoch 136, Loss: 0.005196957383304834\nEpoch 137, Loss: 0.005196957848966122\nEpoch 138, Loss: 0.005196957383304834\nEpoch 139, Loss: 0.005196957848966122\nEpoch 140, Loss: 0.005196956917643547\nEpoch 141, Loss: 0.005196957848966122\nEpoch 142, Loss: 0.005196956917643547\nEpoch 143, Loss: 0.005196957383304834\nEpoch 144, Loss: 0.00519695645198226\nEpoch 145, Loss: 0.005196956917643547\nEpoch 146, Loss: 0.00519695645198226\nEpoch 147, Loss: 0.005196955520659685\nEpoch 148, Loss: 0.005196955520659685\nEpoch 149, Loss: 0.0051969545893371105\nEpoch 150, Loss: 0.005196955520659685\nEpoch 151, Loss: 0.005196955520659685\nEpoch 152, Loss: 0.005196955054998398\nEpoch 153, Loss: 0.00519695645198226\nEpoch 154, Loss: 0.00519695645198226\nEpoch 155, Loss: 0.0051969559863209724\nEpoch 156, Loss: 0.005196956917643547\nEpoch 157, Loss: 0.005196957383304834\nEpoch 158, Loss: 0.00519695645198226\nEpoch 159, Loss: 0.00519695645198226\nEpoch 160, Loss: 0.0051969559863209724\nEpoch 161, Loss: 0.00519695645198226\nEpoch 162, Loss: 0.005196956917643547\nEpoch 163, Loss: 0.00519695645198226\nEpoch 164, Loss: 0.005196956917643547\nEpoch 165, Loss: 0.0051969559863209724\nEpoch 166, Loss: 0.005196956917643547\nEpoch 167, Loss: 0.0051969559863209724\nEpoch 168, Loss: 0.005196955520659685\nEpoch 169, Loss: 0.0051969559863209724\nEpoch 170, Loss: 0.005196955520659685\nEpoch 171, Loss: 0.00519695645198226\nEpoch 172, Loss: 0.005196956917643547\nEpoch 173, Loss: 0.005196955520659685\nEpoch 174, Loss: 0.0051969559863209724\nEpoch 175, Loss: 0.00519695645198226\nEpoch 176, Loss: 0.00519695645198226\nEpoch 177, Loss: 0.00519695645198226\nEpoch 178, Loss: 0.0051969559863209724\nEpoch 179, Loss: 0.00519695645198226\nEpoch 180, Loss: 0.005196956917643547\nEpoch 181, Loss: 0.005196955520659685\nEpoch 182, Loss: 0.0051969559863209724\nEpoch 183, Loss: 0.00519695645198226\nEpoch 184, Loss: 0.005196956917643547\nEpoch 185, Loss: 0.0051969559863209724\nEpoch 186, Loss: 0.0051969559863209724\nEpoch 187, Loss: 0.005196955520659685\nEpoch 188, Loss: 0.0051969559863209724\nEpoch 189, Loss: 0.0051969545893371105\nEpoch 190, Loss: 0.005196956917643547\nEpoch 191, Loss: 0.00519695645198226\nEpoch 192, Loss: 0.005196955054998398\nEpoch 193, Loss: 0.005196955054998398\nEpoch 194, Loss: 0.005196955054998398\nEpoch 195, Loss: 0.0051969545893371105\nEpoch 196, Loss: 0.0051969545893371105\nEpoch 197, Loss: 0.005196955054998398\nEpoch 198, Loss: 0.005196955054998398\nEpoch 199, Loss: 0.005196955520659685\nEpoch 200, Loss: 0.005196955520659685\nEpoch 201, Loss: 0.005196955520659685\nEpoch 202, Loss: 0.005196955054998398\nEpoch 203, Loss: 0.005196955054998398\nEpoch 204, Loss: 0.005196955054998398\nEpoch 205, Loss: 0.0051969559863209724\nEpoch 206, Loss: 0.005196955054998398\nEpoch 207, Loss: 0.0051969545893371105\nEpoch 208, Loss: 0.0051969545893371105\nEpoch 209, Loss: 0.005196955054998398\nEpoch 210, Loss: 0.005196954123675823\nEpoch 211, Loss: 0.005196954123675823\nEpoch 212, Loss: 0.005196954123675823\nEpoch 213, Loss: 0.005196954123675823\nEpoch 214, Loss: 0.005196953658014536\nEpoch 215, Loss: 0.0051969545893371105\nEpoch 216, Loss: 0.0051969545893371105\nEpoch 217, Loss: 0.005196954123675823\nEpoch 218, Loss: 0.005196954123675823\nEpoch 219, Loss: 0.0051969545893371105\nEpoch 220, Loss: 0.005196955054998398\nEpoch 221, Loss: 0.005196954123675823\nEpoch 222, Loss: 0.0051969545893371105\nEpoch 223, Loss: 0.0051969545893371105\nEpoch 224, Loss: 0.005196954123675823\nEpoch 225, Loss: 0.005196953658014536\nEpoch 226, Loss: 0.005196953658014536\nEpoch 227, Loss: 0.005196954123675823\nEpoch 228, Loss: 0.005196954123675823\nEpoch 229, Loss: 0.005196954123675823\nEpoch 230, Loss: 0.005196953192353249\nEpoch 231, Loss: 0.005196953658014536\nEpoch 232, Loss: 0.005196953192353249\nEpoch 233, Loss: 0.005196953192353249\nEpoch 234, Loss: 0.005196952261030674\nEpoch 235, Loss: 0.005196951795369387\nEpoch 236, Loss: 0.005196951795369387\nEpoch 237, Loss: 0.005196952261030674\nEpoch 238, Loss: 0.005196951329708099\nEpoch 239, Loss: 0.005196951795369387\nEpoch 240, Loss: 0.005196951329708099\nEpoch 241, Loss: 0.005196950864046812\nEpoch 242, Loss: 0.005196950864046812\nEpoch 243, Loss: 0.005196951329708099\nEpoch 244, Loss: 0.005196950864046812\nEpoch 245, Loss: 0.005196951795369387\nEpoch 246, Loss: 0.005196951329708099\nEpoch 247, Loss: 0.005196952261030674\nEpoch 248, Loss: 0.005196952726691961\nEpoch 249, Loss: 0.005196952261030674\nEpoch 250, Loss: 0.005196951795369387\nEpoch 251, Loss: 0.005196951329708099\nEpoch 252, Loss: 0.005196951329708099\nEpoch 253, Loss: 0.005196951329708099\nEpoch 254, Loss: 0.005196951329708099\nEpoch 255, Loss: 0.005196950864046812\nEpoch 256, Loss: 0.005196950864046812\nEpoch 257, Loss: 0.0051969499327242374\nEpoch 258, Loss: 0.005196950398385525\nEpoch 259, Loss: 0.005196951329708099\nEpoch 260, Loss: 0.00519694946706295\nEpoch 261, Loss: 0.005196949001401663\nEpoch 262, Loss: 0.005196949001401663\nEpoch 263, Loss: 0.00519694946706295\nEpoch 264, Loss: 0.0051969499327242374\nEpoch 265, Loss: 0.00519694946706295\nEpoch 266, Loss: 0.005196949001401663\nEpoch 267, Loss: 0.00519694946706295\nEpoch 268, Loss: 0.00519694946706295\nEpoch 269, Loss: 0.005196950398385525\nEpoch 270, Loss: 0.005196950864046812\nEpoch 271, Loss: 0.005196950398385525\nEpoch 272, Loss: 0.0051969485357403755\nEpoch 273, Loss: 0.005196947604417801\nEpoch 274, Loss: 0.005196949001401663\nEpoch 275, Loss: 0.005196949001401663\nEpoch 276, Loss: 0.0051969499327242374\nEpoch 277, Loss: 0.005196950398385525\nEpoch 278, Loss: 0.00519694946706295\nEpoch 279, Loss: 0.0051969485357403755\nEpoch 280, Loss: 0.00519694946706295\nEpoch 281, Loss: 0.0051969485357403755\nEpoch 282, Loss: 0.005196947138756514\nEpoch 283, Loss: 0.0051969485357403755\nEpoch 284, Loss: 0.005196948070079088\nEpoch 285, Loss: 0.005196947138756514\nEpoch 286, Loss: 0.005196947604417801\nEpoch 287, Loss: 0.005196947138756514\nEpoch 288, Loss: 0.005196947604417801\nEpoch 289, Loss: 0.005196947604417801\nEpoch 290, Loss: 0.005196948070079088\nEpoch 291, Loss: 0.0051969485357403755\nEpoch 292, Loss: 0.005196949001401663\nEpoch 293, Loss: 0.005196947604417801\nEpoch 294, Loss: 0.005196948070079088\nEpoch 295, Loss: 0.005196948070079088\nEpoch 296, Loss: 0.0051969485357403755\nEpoch 297, Loss: 0.005196947604417801\nEpoch 298, Loss: 0.005196947138756514\nEpoch 299, Loss: 0.005196947604417801\nEpoch 300, Loss: 0.005196947604417801\nEpoch 301, Loss: 0.005196947138756514\nEpoch 302, Loss: 0.005196947604417801\nEpoch 303, Loss: 0.005196948070079088\nEpoch 304, Loss: 0.005196946673095226\nEpoch 305, Loss: 0.005196947138756514\nEpoch 306, Loss: 0.005196946673095226\nEpoch 307, Loss: 0.005196947138756514\nEpoch 308, Loss: 0.005196946673095226\nEpoch 309, Loss: 0.005196946673095226\nEpoch 310, Loss: 0.005196945741772652\nEpoch 311, Loss: 0.005196946673095226\nEpoch 312, Loss: 0.005196946673095226\nEpoch 313, Loss: 0.005196946673095226\nEpoch 314, Loss: 0.005196945741772652\nEpoch 315, Loss: 0.005196944810450077\nEpoch 316, Loss: 0.005196944810450077\nEpoch 317, Loss: 0.005196944810450077\nEpoch 318, Loss: 0.005196944810450077\nEpoch 319, Loss: 0.005196944810450077\nEpoch 320, Loss: 0.00519694434478879\nEpoch 321, Loss: 0.00519694434478879\nEpoch 322, Loss: 0.00519694434478879\nEpoch 323, Loss: 0.005196943413466215\nEpoch 324, Loss: 0.00519694434478879\nEpoch 325, Loss: 0.0051969438791275024\nEpoch 326, Loss: 0.00519694434478879\nEpoch 327, Loss: 0.005196942947804928\nEpoch 328, Loss: 0.005196942947804928\nEpoch 329, Loss: 0.005196943413466215\nEpoch 330, Loss: 0.005196942947804928\nEpoch 331, Loss: 0.005196942947804928\nEpoch 332, Loss: 0.005196942016482353\nEpoch 333, Loss: 0.005196942947804928\nEpoch 334, Loss: 0.005196942016482353\nEpoch 335, Loss: 0.005196941550821066\nEpoch 336, Loss: 0.005196942947804928\nEpoch 337, Loss: 0.005196941085159779\nEpoch 338, Loss: 0.005196941550821066\nEpoch 339, Loss: 0.005196942016482353\nEpoch 340, Loss: 0.005196944810450077\nEpoch 341, Loss: 0.0051969438791275024\nEpoch 342, Loss: 0.005196943413466215\nEpoch 343, Loss: 0.005196942947804928\nEpoch 344, Loss: 0.0051969438791275024\nEpoch 345, Loss: 0.0051969438791275024\nEpoch 346, Loss: 0.0051969438791275024\nEpoch 347, Loss: 0.005196942947804928\nEpoch 348, Loss: 0.005196942947804928\nEpoch 349, Loss: 0.0051969424821436405\nEpoch 350, Loss: 0.0051969424821436405\nEpoch 351, Loss: 0.005196942016482353\nEpoch 352, Loss: 0.005196941085159779\nEpoch 353, Loss: 0.005196941085159779\nEpoch 354, Loss: 0.005196941550821066\nEpoch 355, Loss: 0.005196941085159779\nEpoch 356, Loss: 0.005196940153837204\nEpoch 357, Loss: 0.005196941085159779\nEpoch 358, Loss: 0.005196941550821066\nEpoch 359, Loss: 0.005196940153837204\nEpoch 360, Loss: 0.005196938756853342\nEpoch 361, Loss: 0.005196938291192055\nEpoch 362, Loss: 0.005196938291192055\nEpoch 363, Loss: 0.005196938291192055\nEpoch 364, Loss: 0.005196938756853342\nEpoch 365, Loss: 0.005196939688175917\nEpoch 366, Loss: 0.005196938291192055\nEpoch 367, Loss: 0.005196939222514629\nEpoch 368, Loss: 0.005196938756853342\nEpoch 369, Loss: 0.005196938291192055\nEpoch 370, Loss: 0.005196938756853342\nEpoch 371, Loss: 0.0051969378255307674\nEpoch 372, Loss: 0.0051969378255307674\nEpoch 373, Loss: 0.005196938291192055\nEpoch 374, Loss: 0.005196938291192055\nEpoch 375, Loss: 0.0051969378255307674\nEpoch 376, Loss: 0.005196938291192055\nEpoch 377, Loss: 0.005196939222514629\nEpoch 378, Loss: 0.005196939688175917\nEpoch 379, Loss: 0.005196938291192055\nEpoch 380, Loss: 0.005196938291192055\nEpoch 381, Loss: 0.005196939222514629\nEpoch 382, Loss: 0.005196939222514629\nEpoch 383, Loss: 0.005196939688175917\nEpoch 384, Loss: 0.005196938291192055\nEpoch 385, Loss: 0.005196939222514629\nEpoch 386, Loss: 0.005196939222514629\nEpoch 387, Loss: 0.005196939688175917\nEpoch 388, Loss: 0.005196939688175917\nEpoch 389, Loss: 0.005196939222514629\nEpoch 390, Loss: 0.005196938291192055\nEpoch 391, Loss: 0.005196939222514629\nEpoch 392, Loss: 0.005196938756853342\nEpoch 393, Loss: 0.005196938291192055\nEpoch 394, Loss: 0.005196938291192055\nEpoch 395, Loss: 0.005196939222514629\nEpoch 396, Loss: 0.005196938291192055\nEpoch 397, Loss: 0.005196938291192055\nEpoch 398, Loss: 0.00519693735986948\nEpoch 399, Loss: 0.005196938291192055\nEpoch 400, Loss: 0.00519693735986948\nEpoch 401, Loss: 0.00519693735986948\nEpoch 402, Loss: 0.0051969364285469055\nEpoch 403, Loss: 0.005196936894208193\nEpoch 404, Loss: 0.005196935962885618\nEpoch 405, Loss: 0.005196936894208193\nEpoch 406, Loss: 0.0051969364285469055\nEpoch 407, Loss: 0.005196935497224331\nEpoch 408, Loss: 0.005196935962885618\nEpoch 409, Loss: 0.0051969364285469055\nEpoch 410, Loss: 0.005196935031563044\nEpoch 411, Loss: 0.005196935962885618\nEpoch 412, Loss: 0.005196935962885618\nEpoch 413, Loss: 0.005196936894208193\nEpoch 414, Loss: 0.005196935497224331\nEpoch 415, Loss: 0.005196935962885618\nEpoch 416, Loss: 0.005196935962885618\nEpoch 417, Loss: 0.0051969364285469055\nEpoch 418, Loss: 0.0051969378255307674\nEpoch 419, Loss: 0.005196935962885618\nEpoch 420, Loss: 0.005196935497224331\nEpoch 421, Loss: 0.005196936894208193\nEpoch 422, Loss: 0.005196935497224331\nEpoch 423, Loss: 0.005196935031563044\nEpoch 424, Loss: 0.005196935497224331\nEpoch 425, Loss: 0.005196935031563044\nEpoch 426, Loss: 0.005196935962885618\nEpoch 427, Loss: 0.005196934100240469\nEpoch 428, Loss: 0.005196934100240469\nEpoch 429, Loss: 0.005196935962885618\nEpoch 430, Loss: 0.0051969364285469055\nEpoch 431, Loss: 0.005196935497224331\nEpoch 432, Loss: 0.005196934565901756\nEpoch 433, Loss: 0.005196935962885618\nEpoch 434, Loss: 0.005196935497224331\nEpoch 435, Loss: 0.005196935031563044\nEpoch 436, Loss: 0.005196934100240469\nEpoch 437, Loss: 0.005196934565901756\nEpoch 438, Loss: 0.005196933168917894\nEpoch 439, Loss: 0.005196934100240469\nEpoch 440, Loss: 0.005196932703256607\nEpoch 441, Loss: 0.005196932703256607\nEpoch 442, Loss: 0.0051969317719340324\nEpoch 443, Loss: 0.00519693223759532\nEpoch 444, Loss: 0.0051969317719340324\nEpoch 445, Loss: 0.0051969317719340324\nEpoch 446, Loss: 0.005196931306272745\nEpoch 447, Loss: 0.0051969317719340324\nEpoch 448, Loss: 0.005196931306272745\nEpoch 449, Loss: 0.005196930840611458\nEpoch 450, Loss: 0.0051969317719340324\nEpoch 451, Loss: 0.005196931306272745\nEpoch 452, Loss: 0.0051969317719340324\nEpoch 453, Loss: 0.005196931306272745\nEpoch 454, Loss: 0.005196930840611458\nEpoch 455, Loss: 0.0051969317719340324\nEpoch 456, Loss: 0.005196931306272745\nEpoch 457, Loss: 0.0051969317719340324\nEpoch 458, Loss: 0.0051969317719340324\nEpoch 459, Loss: 0.005196931306272745\nEpoch 460, Loss: 0.0051969317719340324\nEpoch 461, Loss: 0.005196931306272745\nEpoch 462, Loss: 0.0051969317719340324\nEpoch 463, Loss: 0.0051969317719340324\nEpoch 464, Loss: 0.005196930840611458\nEpoch 465, Loss: 0.005196931306272745\nEpoch 466, Loss: 0.005196930840611458\nEpoch 467, Loss: 0.005196930840611458\nEpoch 468, Loss: 0.005196931306272745\nEpoch 469, Loss: 0.005196930840611458\nEpoch 470, Loss: 0.005196931306272745\nEpoch 471, Loss: 0.005196929909288883\nEpoch 472, Loss: 0.0051969303749501705\nEpoch 473, Loss: 0.005196930840611458\nEpoch 474, Loss: 0.005196931306272745\nEpoch 475, Loss: 0.005196930840611458\nEpoch 476, Loss: 0.0051969303749501705\nEpoch 477, Loss: 0.005196930840611458\nEpoch 478, Loss: 0.005196931306272745\nEpoch 479, Loss: 0.005196931306272745\nEpoch 480, Loss: 0.005196929909288883\nEpoch 481, Loss: 0.005196929443627596\nEpoch 482, Loss: 0.005196929443627596\nEpoch 483, Loss: 0.005196929909288883\nEpoch 484, Loss: 0.005196928977966309\nEpoch 485, Loss: 0.005196928977966309\nEpoch 486, Loss: 0.005196928977966309\nEpoch 487, Loss: 0.005196927580982447\nEpoch 488, Loss: 0.005196927580982447\nEpoch 489, Loss: 0.005196928046643734\nEpoch 490, Loss: 0.005196928512305021\nEpoch 491, Loss: 0.005196928977966309\nEpoch 492, Loss: 0.005196929909288883\nEpoch 493, Loss: 0.005196928977966309\nEpoch 494, Loss: 0.005196929443627596\nEpoch 495, Loss: 0.005196928977966309\nEpoch 496, Loss: 0.005196928977966309\nEpoch 497, Loss: 0.005196928512305021\nEpoch 498, Loss: 0.005196929443627596\nEpoch 499, Loss: 0.005196928977966309\nEpoch 500, Loss: 0.005196929443627596\nEpoch 501, Loss: 0.005196928977966309\nEpoch 502, Loss: 0.005196928977966309\nEpoch 503, Loss: 0.005196927580982447\nEpoch 504, Loss: 0.005196927115321159\nEpoch 505, Loss: 0.005196927115321159\nEpoch 506, Loss: 0.005196926183998585\nEpoch 507, Loss: 0.005196926183998585\nEpoch 508, Loss: 0.005196926183998585\nEpoch 509, Loss: 0.005196927115321159\nEpoch 510, Loss: 0.005196927580982447\nEpoch 511, Loss: 0.005196927115321159\nEpoch 512, Loss: 0.0051969257183372974\nEpoch 513, Loss: 0.005196926183998585\nEpoch 514, Loss: 0.00519692525267601\nEpoch 515, Loss: 0.00519692525267601\nEpoch 516, Loss: 0.00519692525267601\nEpoch 517, Loss: 0.00519692525267601\nEpoch 518, Loss: 0.005196924787014723\nEpoch 519, Loss: 0.00519692525267601\nEpoch 520, Loss: 0.00519692525267601\nEpoch 521, Loss: 0.00519692525267601\nEpoch 522, Loss: 0.005196926649659872\nEpoch 523, Loss: 0.005196926183998585\nEpoch 524, Loss: 0.0051969257183372974\nEpoch 525, Loss: 0.005196926183998585\nEpoch 526, Loss: 0.00519692525267601\nEpoch 527, Loss: 0.00519692525267601\nEpoch 528, Loss: 0.00519692525267601\nEpoch 529, Loss: 0.005196924787014723\nEpoch 530, Loss: 0.00519692525267601\nEpoch 531, Loss: 0.00519692525267601\nEpoch 532, Loss: 0.0051969257183372974\nEpoch 533, Loss: 0.005196924787014723\nEpoch 534, Loss: 0.005196924787014723\nEpoch 535, Loss: 0.0051969243213534355\nEpoch 536, Loss: 0.005196924787014723\nEpoch 537, Loss: 0.0051969243213534355\nEpoch 538, Loss: 0.005196923855692148\nEpoch 539, Loss: 0.005196922924369574\nEpoch 540, Loss: 0.005196922924369574\nEpoch 541, Loss: 0.005196922924369574\nEpoch 542, Loss: 0.005196923390030861\nEpoch 543, Loss: 0.005196923855692148\nEpoch 544, Loss: 0.005196922924369574\nEpoch 545, Loss: 0.005196923855692148\nEpoch 546, Loss: 0.005196923390030861\nEpoch 547, Loss: 0.005196922924369574\nEpoch 548, Loss: 0.005196923390030861\nEpoch 549, Loss: 0.005196923390030861\nEpoch 550, Loss: 0.005196923390030861\nEpoch 551, Loss: 0.005196922924369574\nEpoch 552, Loss: 0.005196921993046999\nEpoch 553, Loss: 0.005196922458708286\nEpoch 554, Loss: 0.005196922924369574\nEpoch 555, Loss: 0.005196921527385712\nEpoch 556, Loss: 0.005196923855692148\nEpoch 557, Loss: 0.005196922458708286\nEpoch 558, Loss: 0.005196922924369574\nEpoch 559, Loss: 0.005196923855692148\nEpoch 560, Loss: 0.005196921061724424\nEpoch 561, Loss: 0.005196921993046999\nEpoch 562, Loss: 0.005196921993046999\nEpoch 563, Loss: 0.00519692013040185\nEpoch 564, Loss: 0.005196921527385712\nEpoch 565, Loss: 0.005196921061724424\nEpoch 566, Loss: 0.0051969196647405624\nEpoch 567, Loss: 0.00519692013040185\nEpoch 568, Loss: 0.0051969196647405624\nEpoch 569, Loss: 0.00519692013040185\nEpoch 570, Loss: 0.0051969196647405624\nEpoch 571, Loss: 0.005196920596063137\nEpoch 572, Loss: 0.0051969196647405624\nEpoch 573, Loss: 0.00519692013040185\nEpoch 574, Loss: 0.005196920596063137\nEpoch 575, Loss: 0.00519692013040185\nEpoch 576, Loss: 0.005196920596063137\nEpoch 577, Loss: 0.005196920596063137\nEpoch 578, Loss: 0.00519692013040185\nEpoch 579, Loss: 0.005196921061724424\nEpoch 580, Loss: 0.005196920596063137\nEpoch 581, Loss: 0.005196920596063137\nEpoch 582, Loss: 0.00519692013040185\nEpoch 583, Loss: 0.005196920596063137\nEpoch 584, Loss: 0.005196921061724424\nEpoch 585, Loss: 0.005196921061724424\nEpoch 586, Loss: 0.005196921993046999\nEpoch 587, Loss: 0.005196921993046999\nEpoch 588, Loss: 0.005196921061724424\nEpoch 589, Loss: 0.005196920596063137\nEpoch 590, Loss: 0.005196920596063137\nEpoch 591, Loss: 0.00519692013040185\nEpoch 592, Loss: 0.005196921061724424\nEpoch 593, Loss: 0.005196920596063137\nEpoch 594, Loss: 0.005196920596063137\nEpoch 595, Loss: 0.005196920596063137\nEpoch 596, Loss: 0.005196920596063137\nEpoch 597, Loss: 0.0051969196647405624\nEpoch 598, Loss: 0.00519692013040185\nEpoch 599, Loss: 0.00519692013040185\nEpoch 600, Loss: 0.005196919199079275\nEpoch 601, Loss: 0.0051969182677567005\nEpoch 602, Loss: 0.005196919199079275\nEpoch 603, Loss: 0.005196918733417988\nEpoch 604, Loss: 0.0051969182677567005\nEpoch 605, Loss: 0.0051969182677567005\nEpoch 606, Loss: 0.0051969182677567005\nEpoch 607, Loss: 0.005196917802095413\nEpoch 608, Loss: 0.005196917802095413\nEpoch 609, Loss: 0.0051969182677567005\nEpoch 610, Loss: 0.0051969182677567005\nEpoch 611, Loss: 0.0051969182677567005\nEpoch 612, Loss: 0.005196917336434126\nEpoch 613, Loss: 0.005196916870772839\nEpoch 614, Loss: 0.0051969182677567005\nEpoch 615, Loss: 0.0051969182677567005\nEpoch 616, Loss: 0.005196917802095413\nEpoch 617, Loss: 0.005196917802095413\nEpoch 618, Loss: 0.005196917336434126\nEpoch 619, Loss: 0.005196916870772839\nEpoch 620, Loss: 0.005196917802095413\nEpoch 621, Loss: 0.005196917802095413\nEpoch 622, Loss: 0.005196917336434126\nEpoch 623, Loss: 0.005196915939450264\nEpoch 624, Loss: 0.005196917336434126\nEpoch 625, Loss: 0.005196915939450264\nEpoch 626, Loss: 0.005196915473788977\nEpoch 627, Loss: 0.005196915473788977\nEpoch 628, Loss: 0.005196915939450264\nEpoch 629, Loss: 0.005196915008127689\nEpoch 630, Loss: 0.005196915008127689\nEpoch 631, Loss: 0.005196917336434126\nEpoch 632, Loss: 0.005196917336434126\nEpoch 633, Loss: 0.005196916870772839\nEpoch 634, Loss: 0.005196916405111551\nEpoch 635, Loss: 0.005196916870772839\nEpoch 636, Loss: 0.005196916870772839\nEpoch 637, Loss: 0.005196915939450264\nEpoch 638, Loss: 0.005196915939450264\nEpoch 639, Loss: 0.005196915473788977\nEpoch 640, Loss: 0.005196915473788977\nEpoch 641, Loss: 0.005196915008127689\nEpoch 642, Loss: 0.005196915473788977\nEpoch 643, Loss: 0.005196915008127689\nEpoch 644, Loss: 0.005196915008127689\nEpoch 645, Loss: 0.005196914542466402\nEpoch 646, Loss: 0.0051969136111438274\nEpoch 647, Loss: 0.00519691314548254\nEpoch 648, Loss: 0.0051969136111438274\nEpoch 649, Loss: 0.00519691314548254\nEpoch 650, Loss: 0.0051969136111438274\nEpoch 651, Loss: 0.00519691314548254\nEpoch 652, Loss: 0.00519691314548254\nEpoch 653, Loss: 0.005196914076805115\nEpoch 654, Loss: 0.00519691314548254\nEpoch 655, Loss: 0.00519691314548254\nEpoch 656, Loss: 0.00519691314548254\nEpoch 657, Loss: 0.005196912679821253\nEpoch 658, Loss: 0.005196911748498678\nEpoch 659, Loss: 0.005196911748498678\nEpoch 660, Loss: 0.005196911748498678\nEpoch 661, Loss: 0.005196912679821253\nEpoch 662, Loss: 0.00519691314548254\nEpoch 663, Loss: 0.0051969122141599655\nEpoch 664, Loss: 0.005196911748498678\nEpoch 665, Loss: 0.005196911748498678\nEpoch 666, Loss: 0.005196911748498678\nEpoch 667, Loss: 0.0051969122141599655\nEpoch 668, Loss: 0.005196911748498678\nEpoch 669, Loss: 0.0051969122141599655\nEpoch 670, Loss: 0.005196911748498678\nEpoch 671, Loss: 0.005196911748498678\nEpoch 672, Loss: 0.005196911748498678\nEpoch 673, Loss: 0.005196911748498678\nEpoch 674, Loss: 0.005196911748498678\nEpoch 675, Loss: 0.005196911748498678\nEpoch 676, Loss: 0.005196911748498678\nEpoch 677, Loss: 0.005196911748498678\nEpoch 678, Loss: 0.005196911748498678\nEpoch 679, Loss: 0.005196911282837391\nEpoch 680, Loss: 0.0051969122141599655\nEpoch 681, Loss: 0.005196911748498678\nEpoch 682, Loss: 0.005196910817176104\nEpoch 683, Loss: 0.005196910817176104\nEpoch 684, Loss: 0.005196910351514816\nEpoch 685, Loss: 0.005196910351514816\nEpoch 686, Loss: 0.005196911282837391\nEpoch 687, Loss: 0.005196911282837391\nEpoch 688, Loss: 0.005196910817176104\nEpoch 689, Loss: 0.005196910817176104\nEpoch 690, Loss: 0.005196910817176104\nEpoch 691, Loss: 0.005196910817176104\nEpoch 692, Loss: 0.005196910351514816\nEpoch 693, Loss: 0.005196910817176104\nEpoch 694, Loss: 0.005196910817176104\nEpoch 695, Loss: 0.005196911282837391\nEpoch 696, Loss: 0.005196911748498678\nEpoch 697, Loss: 0.005196911748498678\nEpoch 698, Loss: 0.0051969136111438274\nEpoch 699, Loss: 0.005196912679821253\nEpoch 700, Loss: 0.005196912679821253\nEpoch 701, Loss: 0.0051969122141599655\nEpoch 702, Loss: 0.005196911748498678\nEpoch 703, Loss: 0.005196911282837391\nEpoch 704, Loss: 0.005196910817176104\nEpoch 705, Loss: 0.005196911282837391\nEpoch 706, Loss: 0.005196911282837391\nEpoch 707, Loss: 0.005196909420192242\nEpoch 708, Loss: 0.005196911282837391\nEpoch 709, Loss: 0.005196911282837391\nEpoch 710, Loss: 0.005196909420192242\nEpoch 711, Loss: 0.005196909885853529\nEpoch 712, Loss: 0.005196909420192242\nEpoch 713, Loss: 0.005196908488869667\nEpoch 714, Loss: 0.005196908488869667\nEpoch 715, Loss: 0.005196908954530954\nEpoch 716, Loss: 0.005196908954530954\nEpoch 717, Loss: 0.00519690802320838\nEpoch 718, Loss: 0.005196908954530954\nEpoch 719, Loss: 0.005196909420192242\nEpoch 720, Loss: 0.005196909420192242\nEpoch 721, Loss: 0.005196909885853529\nEpoch 722, Loss: 0.005196909885853529\nEpoch 723, Loss: 0.005196909420192242\nEpoch 724, Loss: 0.005196908488869667\nEpoch 725, Loss: 0.005196909420192242\nEpoch 726, Loss: 0.005196908488869667\nEpoch 727, Loss: 0.0051969075575470924\nEpoch 728, Loss: 0.00519690802320838\nEpoch 729, Loss: 0.00519690802320838\nEpoch 730, Loss: 0.0051969075575470924\nEpoch 731, Loss: 0.0051969075575470924\nEpoch 732, Loss: 0.00519690802320838\nEpoch 733, Loss: 0.005196906626224518\nEpoch 734, Loss: 0.005196907091885805\nEpoch 735, Loss: 0.005196906626224518\nEpoch 736, Loss: 0.005196907091885805\nEpoch 737, Loss: 0.005196906626224518\nEpoch 738, Loss: 0.00519690802320838\nEpoch 739, Loss: 0.0051969075575470924\nEpoch 740, Loss: 0.0051969061605632305\nEpoch 741, Loss: 0.005196906626224518\nEpoch 742, Loss: 0.005196906626224518\nEpoch 743, Loss: 0.005196905694901943\nEpoch 744, Loss: 0.0051969061605632305\nEpoch 745, Loss: 0.005196905229240656\nEpoch 746, Loss: 0.005196904297918081\nEpoch 747, Loss: 0.005196905229240656\nEpoch 748, Loss: 0.005196905229240656\nEpoch 749, Loss: 0.005196905229240656\nEpoch 750, Loss: 0.005196905229240656\nEpoch 751, Loss: 0.005196904763579369\nEpoch 752, Loss: 0.005196904297918081\nEpoch 753, Loss: 0.005196904763579369\nEpoch 754, Loss: 0.005196904297918081\nEpoch 755, Loss: 0.005196904763579369\nEpoch 756, Loss: 0.005196904763579369\nEpoch 757, Loss: 0.005196903832256794\nEpoch 758, Loss: 0.005196903832256794\nEpoch 759, Loss: 0.005196904763579369\nEpoch 760, Loss: 0.005196904297918081\nEpoch 761, Loss: 0.005196904297918081\nEpoch 762, Loss: 0.005196904297918081\nEpoch 763, Loss: 0.005196904763579369\nEpoch 764, Loss: 0.005196903832256794\nEpoch 765, Loss: 0.005196903832256794\nEpoch 766, Loss: 0.005196903366595507\nEpoch 767, Loss: 0.005196901969611645\nEpoch 768, Loss: 0.005196902435272932\nEpoch 769, Loss: 0.005196901969611645\nEpoch 770, Loss: 0.005196902900934219\nEpoch 771, Loss: 0.005196902900934219\nEpoch 772, Loss: 0.005196903366595507\nEpoch 773, Loss: 0.005196903832256794\nEpoch 774, Loss: 0.005196902900934219\nEpoch 775, Loss: 0.005196902435272932\nEpoch 776, Loss: 0.005196901969611645\nEpoch 777, Loss: 0.005196902435272932\nEpoch 778, Loss: 0.005196902435272932\nEpoch 779, Loss: 0.005196902435272932\nEpoch 780, Loss: 0.005196902435272932\nEpoch 781, Loss: 0.005196901969611645\nEpoch 782, Loss: 0.005196900572627783\nEpoch 783, Loss: 0.005196900572627783\nEpoch 784, Loss: 0.005196900572627783\nEpoch 785, Loss: 0.0051969015039503574\nEpoch 786, Loss: 0.00519690103828907\nEpoch 787, Loss: 0.00519690103828907\nEpoch 788, Loss: 0.005196900572627783\nEpoch 789, Loss: 0.00519690103828907\nEpoch 790, Loss: 0.00519690103828907\nEpoch 791, Loss: 0.00519690103828907\nEpoch 792, Loss: 0.005196900572627783\nEpoch 793, Loss: 0.00519690103828907\nEpoch 794, Loss: 0.00519690103828907\nEpoch 795, Loss: 0.0051969015039503574\nEpoch 796, Loss: 0.0051969015039503574\nEpoch 797, Loss: 0.00519690103828907\nEpoch 798, Loss: 0.0051969001069664955\nEpoch 799, Loss: 0.005196900572627783\nEpoch 800, Loss: 0.00519690103828907\nEpoch 801, Loss: 0.0051969015039503574\nEpoch 802, Loss: 0.00519690103828907\nEpoch 803, Loss: 0.0051969015039503574\nEpoch 804, Loss: 0.005196900572627783\nEpoch 805, Loss: 0.00519690103828907\nEpoch 806, Loss: 0.0051969001069664955\nEpoch 807, Loss: 0.005196899641305208\nEpoch 808, Loss: 0.005196899641305208\nEpoch 809, Loss: 0.005196898709982634\nEpoch 810, Loss: 0.005196898709982634\nEpoch 811, Loss: 0.005196898709982634\nEpoch 812, Loss: 0.005196898709982634\nEpoch 813, Loss: 0.005196898709982634\nEpoch 814, Loss: 0.005196898709982634\nEpoch 815, Loss: 0.005196898244321346\nEpoch 816, Loss: 0.005196898244321346\nEpoch 817, Loss: 0.005196898244321346\nEpoch 818, Loss: 0.005196897312998772\nEpoch 819, Loss: 0.005196898709982634\nEpoch 820, Loss: 0.005196898244321346\nEpoch 821, Loss: 0.005196898244321346\nEpoch 822, Loss: 0.005196897312998772\nEpoch 823, Loss: 0.005196897312998772\nEpoch 824, Loss: 0.005196897778660059\nEpoch 825, Loss: 0.005196897778660059\nEpoch 826, Loss: 0.005196896381676197\nEpoch 827, Loss: 0.005196896847337484\nEpoch 828, Loss: 0.005196896847337484\nEpoch 829, Loss: 0.005196896381676197\nEpoch 830, Loss: 0.005196896381676197\nEpoch 831, Loss: 0.005196896847337484\nEpoch 832, Loss: 0.005196897312998772\nEpoch 833, Loss: 0.00519689591601491\nEpoch 834, Loss: 0.005196896381676197\nEpoch 835, Loss: 0.005196896381676197\nEpoch 836, Loss: 0.005196896847337484\nEpoch 837, Loss: 0.005196896847337484\nEpoch 838, Loss: 0.005196896381676197\nEpoch 839, Loss: 0.005196896847337484\nEpoch 840, Loss: 0.005196896381676197\nEpoch 841, Loss: 0.005196896381676197\nEpoch 842, Loss: 0.005196896847337484\nEpoch 843, Loss: 0.005196897312998772\nEpoch 844, Loss: 0.005196897778660059\nEpoch 845, Loss: 0.005196896847337484\nEpoch 846, Loss: 0.005196896847337484\nEpoch 847, Loss: 0.00519689591601491\nEpoch 848, Loss: 0.00519689591601491\nEpoch 849, Loss: 0.00519689591601491\nEpoch 850, Loss: 0.00519689591601491\nEpoch 851, Loss: 0.005196896381676197\nEpoch 852, Loss: 0.00519689591601491\nEpoch 853, Loss: 0.0051968954503536224\nEpoch 854, Loss: 0.005196894984692335\nEpoch 855, Loss: 0.005196894984692335\nEpoch 856, Loss: 0.0051968954503536224\nEpoch 857, Loss: 0.005196894984692335\nEpoch 858, Loss: 0.0051968954503536224\nEpoch 859, Loss: 0.005196894519031048\nEpoch 860, Loss: 0.00519689591601491\nEpoch 861, Loss: 0.005196894519031048\nEpoch 862, Loss: 0.005196894519031048\nEpoch 863, Loss: 0.0051968940533697605\nEpoch 864, Loss: 0.005196893587708473\nEpoch 865, Loss: 0.005196893587708473\nEpoch 866, Loss: 0.005196894519031048\nEpoch 867, Loss: 0.005196894984692335\nEpoch 868, Loss: 0.0051968940533697605\nEpoch 869, Loss: 0.005196894519031048\nEpoch 870, Loss: 0.0051968940533697605\nEpoch 871, Loss: 0.0051968940533697605\nEpoch 872, Loss: 0.005196893587708473\nEpoch 873, Loss: 0.0051968940533697605\nEpoch 874, Loss: 0.005196894984692335\nEpoch 875, Loss: 0.005196893587708473\nEpoch 876, Loss: 0.0051968940533697605\nEpoch 877, Loss: 0.005196892656385899\nEpoch 878, Loss: 0.005196893587708473\nEpoch 879, Loss: 0.0051968940533697605\nEpoch 880, Loss: 0.005196893587708473\nEpoch 881, Loss: 0.005196893122047186\nEpoch 882, Loss: 0.005196893122047186\nEpoch 883, Loss: 0.005196893587708473\nEpoch 884, Loss: 0.005196893587708473\nEpoch 885, Loss: 0.005196893587708473\nEpoch 886, Loss: 0.005196892656385899\nEpoch 887, Loss: 0.005196893122047186\nEpoch 888, Loss: 0.005196892190724611\nEpoch 889, Loss: 0.005196892656385899\nEpoch 890, Loss: 0.005196893122047186\nEpoch 891, Loss: 0.005196892190724611\nEpoch 892, Loss: 0.005196892656385899\nEpoch 893, Loss: 0.005196892656385899\nEpoch 894, Loss: 0.005196893122047186\nEpoch 895, Loss: 0.005196892656385899\nEpoch 896, Loss: 0.005196893122047186\nEpoch 897, Loss: 0.005196893587708473\nEpoch 898, Loss: 0.005196892656385899\nEpoch 899, Loss: 0.005196892190724611\nEpoch 900, Loss: 0.005196893122047186\nEpoch 901, Loss: 0.005196892190724611\nEpoch 902, Loss: 0.005196891725063324\nEpoch 903, Loss: 0.005196891725063324\nEpoch 904, Loss: 0.005196890328079462\nEpoch 905, Loss: 0.005196891259402037\nEpoch 906, Loss: 0.005196890793740749\nEpoch 907, Loss: 0.005196891259402037\nEpoch 908, Loss: 0.005196890793740749\nEpoch 909, Loss: 0.005196890793740749\nEpoch 910, Loss: 0.005196889862418175\nEpoch 911, Loss: 0.0051968889310956\nEpoch 912, Loss: 0.0051968893967568874\nEpoch 913, Loss: 0.005196889862418175\nEpoch 914, Loss: 0.005196889862418175\nEpoch 915, Loss: 0.005196888465434313\nEpoch 916, Loss: 0.0051968893967568874\nEpoch 917, Loss: 0.005196889862418175\nEpoch 918, Loss: 0.005196889862418175\nEpoch 919, Loss: 0.0051968889310956\nEpoch 920, Loss: 0.0051968879997730255\nEpoch 921, Loss: 0.0051968889310956\nEpoch 922, Loss: 0.005196889862418175\nEpoch 923, Loss: 0.0051968893967568874\nEpoch 924, Loss: 0.0051968889310956\nEpoch 925, Loss: 0.0051968893967568874\nEpoch 926, Loss: 0.0051968889310956\nEpoch 927, Loss: 0.0051968889310956\nEpoch 928, Loss: 0.005196889862418175\nEpoch 929, Loss: 0.005196890793740749\nEpoch 930, Loss: 0.0051968893967568874\nEpoch 931, Loss: 0.0051968889310956\nEpoch 932, Loss: 0.005196887534111738\nEpoch 933, Loss: 0.005196888465434313\nEpoch 934, Loss: 0.0051968879997730255\nEpoch 935, Loss: 0.005196887534111738\nEpoch 936, Loss: 0.005196887534111738\nEpoch 937, Loss: 0.005196888465434313\nEpoch 938, Loss: 0.0051968879997730255\nEpoch 939, Loss: 0.005196887534111738\nEpoch 940, Loss: 0.005196886602789164\nEpoch 941, Loss: 0.005196886602789164\nEpoch 942, Loss: 0.005196886137127876\nEpoch 943, Loss: 0.005196886602789164\nEpoch 944, Loss: 0.005196886137127876\nEpoch 945, Loss: 0.005196886602789164\nEpoch 946, Loss: 0.005196886602789164\nEpoch 947, Loss: 0.005196886137127876\nEpoch 948, Loss: 0.005196887068450451\nEpoch 949, Loss: 0.005196887068450451\nEpoch 950, Loss: 0.005196885205805302\nEpoch 951, Loss: 0.005196886137127876\nEpoch 952, Loss: 0.005196885205805302\nEpoch 953, Loss: 0.005196886137127876\nEpoch 954, Loss: 0.005196885205805302\nEpoch 955, Loss: 0.005196885205805302\nEpoch 956, Loss: 0.005196885205805302\nEpoch 957, Loss: 0.005196885205805302\nEpoch 958, Loss: 0.005196885671466589\nEpoch 959, Loss: 0.005196885671466589\nEpoch 960, Loss: 0.005196885671466589\nEpoch 961, Loss: 0.005196885205805302\nEpoch 962, Loss: 0.005196885205805302\nEpoch 963, Loss: 0.005196886137127876\nEpoch 964, Loss: 0.005196886137127876\nEpoch 965, Loss: 0.005196886137127876\nEpoch 966, Loss: 0.005196886137127876\nEpoch 967, Loss: 0.005196886602789164\nEpoch 968, Loss: 0.005196886602789164\nEpoch 969, Loss: 0.005196885671466589\nEpoch 970, Loss: 0.005196885205805302\nEpoch 971, Loss: 0.005196886137127876\nEpoch 972, Loss: 0.005196885671466589\nEpoch 973, Loss: 0.005196885205805302\nEpoch 974, Loss: 0.005196885205805302\nEpoch 975, Loss: 0.005196885205805302\nEpoch 976, Loss: 0.005196885205805302\nEpoch 977, Loss: 0.005196884740144014\nEpoch 978, Loss: 0.005196884740144014\nEpoch 979, Loss: 0.005196885205805302\nEpoch 980, Loss: 0.005196885205805302\nEpoch 981, Loss: 0.005196884740144014\nEpoch 982, Loss: 0.005196885205805302\nEpoch 983, Loss: 0.005196885205805302\nEpoch 984, Loss: 0.005196885205805302\nEpoch 985, Loss: 0.005196885205805302\nEpoch 986, Loss: 0.005196885205805302\nEpoch 987, Loss: 0.005196884740144014\nEpoch 988, Loss: 0.005196885205805302\nEpoch 989, Loss: 0.005196885671466589\nEpoch 990, Loss: 0.005196885205805302\nEpoch 991, Loss: 0.005196884740144014\nEpoch 992, Loss: 0.005196884740144014\nEpoch 993, Loss: 0.005196884274482727\nEpoch 994, Loss: 0.005196884274482727\nEpoch 995, Loss: 0.005196884740144014\nEpoch 996, Loss: 0.005196885205805302\nEpoch 997, Loss: 0.005196884274482727\nEpoch 998, Loss: 0.005196884740144014\nEpoch 999, Loss: 0.005196884740144014\nEpoch 0, Loss: 0.21194522082805634\nValidation Loss improved to 0.15596188604831696 Saving model 3\nEpoch 1, Loss: 0.08412915468215942\nValidation Loss improved to 0.14372341334819794 Saving model 3\nEpoch 2, Loss: 0.06308746337890625\nValidation Loss improved to 0.13106413185596466 Saving model 3\nEpoch 3, Loss: 0.05571094900369644\nValidation Loss improved to 0.12113139033317566 Saving model 3\nEpoch 4, Loss: 0.047557856887578964\nValidation Loss improved to 0.10889507085084915 Saving model 3\nEpoch 5, Loss: 0.03436459228396416\nValidation Loss improved to 0.10021317005157471 Saving model 3\nEpoch 6, Loss: 0.024787334725260735\nValidation Loss improved to 0.09104917198419571 Saving model 3\nEpoch 7, Loss: 0.018602639436721802\nValidation Loss improved to 0.08393160253763199 Saving model 3\nEpoch 8, Loss: 0.02013312466442585\nValidation Loss improved to 0.07794900238513947 Saving model 3\nEpoch 9, Loss: 0.02399192377924919\nValidation Loss improved to 0.07263810932636261 Saving model 3\nEpoch 10, Loss: 0.02144128456711769\nValidation Loss improved to 0.06838390231132507 Saving model 3\nEpoch 11, Loss: 0.018980087712407112\nValidation Loss improved to 0.06660488992929459 Saving model 3\nEpoch 12, Loss: 0.020058272406458855\nValidation Loss improved to 0.0635242834687233 Saving model 3\nEpoch 13, Loss: 0.016584690660238266\nValidation Loss improved to 0.06082789972424507 Saving model 3\nEpoch 14, Loss: 0.01984589360654354\nValidation Loss improved to 0.05825004354119301 Saving model 3\nEpoch 15, Loss: 0.015083030797541142\nValidation Loss improved to 0.05664665624499321 Saving model 3\nEpoch 16, Loss: 0.016306646168231964\nValidation Loss improved to 0.055930379778146744 Saving model 3\nEpoch 17, Loss: 0.012684657238423824\nValidation Loss improved to 0.054766878485679626 Saving model 3\nEpoch 18, Loss: 0.014612582512199879\nValidation Loss improved to 0.05317217856645584 Saving model 3\nEpoch 19, Loss: 0.01866786740720272\nValidation Loss improved to 0.051606565713882446 Saving model 3\nEpoch 20, Loss: 0.013014079071581364\nValidation Loss improved to 0.05026170611381531 Saving model 3\nEpoch 21, Loss: 0.010549962520599365\nValidation Loss improved to 0.049091409891843796 Saving model 3\nEpoch 22, Loss: 0.012927800416946411\nValidation Loss improved to 0.048003848642110825 Saving model 3\nEpoch 23, Loss: 0.013168075121939182\nValidation Loss improved to 0.04726555943489075 Saving model 3\nEpoch 24, Loss: 0.015563718043267727\nValidation Loss improved to 0.046513352543115616 Saving model 3\nEpoch 25, Loss: 0.011194402351975441\nValidation Loss improved to 0.04565556347370148 Saving model 3\nEpoch 26, Loss: 0.010620090179145336\nValidation Loss improved to 0.04507272318005562 Saving model 3\nEpoch 27, Loss: 0.010536995716392994\nValidation Loss improved to 0.04412050545215607 Saving model 3\nEpoch 28, Loss: 0.00945233553647995\nValidation Loss improved to 0.04316234961152077 Saving model 3\nEpoch 29, Loss: 0.012104954570531845\nValidation Loss improved to 0.04269036650657654 Saving model 3\nEpoch 30, Loss: 0.015299232676625252\nValidation Loss improved to 0.0420980267226696 Saving model 3\nEpoch 31, Loss: 0.012029713951051235\nValidation Loss improved to 0.04166509956121445 Saving model 3\nEpoch 32, Loss: 0.02041727676987648\nValidation Loss improved to 0.0409206859767437 Saving model 3\nEpoch 33, Loss: 0.01601308025419712\nValidation Loss improved to 0.04019906744360924 Saving model 3\nEpoch 34, Loss: 0.01642031967639923\nValidation Loss improved to 0.039749305695295334 Saving model 3\nEpoch 35, Loss: 0.017400512471795082\nValidation Loss improved to 0.0396711528301239 Saving model 3\nEpoch 36, Loss: 0.01102515496313572\nValidation Loss improved to 0.039320871233940125 Saving model 3\nEpoch 37, Loss: 0.012405234389007092\nValidation Loss improved to 0.038654740899801254 Saving model 3\nEpoch 38, Loss: 0.014815560542047024\nValidation Loss improved to 0.038076553493738174 Saving model 3\nEpoch 39, Loss: 0.012682248838245869\nValidation Loss improved to 0.03748950734734535 Saving model 3\nEpoch 40, Loss: 0.010717492550611496\nEpoch 41, Loss: 0.01603231392800808\nEpoch 42, Loss: 0.014799190685153008\nValidation Loss improved to 0.03709433600306511 Saving model 3\nEpoch 43, Loss: 0.010773930698633194\nValidation Loss improved to 0.036790281534194946 Saving model 3\nEpoch 44, Loss: 0.009257180616259575\nValidation Loss improved to 0.036640021950006485 Saving model 3\nEpoch 45, Loss: 0.009494814090430737\nValidation Loss improved to 0.03615816310048103 Saving model 3\nEpoch 46, Loss: 0.008343995548784733\nValidation Loss improved to 0.03575509786605835 Saving model 3\nEpoch 47, Loss: 0.008117049932479858\nValidation Loss improved to 0.035465702414512634 Saving model 3\nEpoch 48, Loss: 0.01047365739941597\nValidation Loss improved to 0.03507760167121887 Saving model 3\nEpoch 49, Loss: 0.010911083780229092\nValidation Loss improved to 0.034713130444288254 Saving model 3\nEpoch 50, Loss: 0.007692959159612656\nValidation Loss improved to 0.034292496740818024 Saving model 3\nEpoch 51, Loss: 0.008334123529493809\nValidation Loss improved to 0.03423822671175003 Saving model 3\nEpoch 52, Loss: 0.013031828217208385\nValidation Loss improved to 0.034049250185489655 Saving model 3\nEpoch 53, Loss: 0.011894281953573227\nValidation Loss improved to 0.033639874309301376 Saving model 3\nEpoch 54, Loss: 0.01422719657421112\nValidation Loss improved to 0.03324752300977707 Saving model 3\nEpoch 55, Loss: 0.011239211075007915\nValidation Loss improved to 0.03307885676622391 Saving model 3\nEpoch 56, Loss: 0.009855223819613457\nValidation Loss improved to 0.032719507813453674 Saving model 3\nEpoch 57, Loss: 0.010399002581834793\nValidation Loss improved to 0.032447315752506256 Saving model 3\nEpoch 58, Loss: 0.007267444394528866\nValidation Loss improved to 0.032371219247579575 Saving model 3\nEpoch 59, Loss: 0.009836338460445404\nValidation Loss improved to 0.032078687101602554 Saving model 3\nEpoch 60, Loss: 0.013823731802403927\nValidation Loss improved to 0.031785670667886734 Saving model 3\nEpoch 61, Loss: 0.012891031801700592\nValidation Loss improved to 0.03151750564575195 Saving model 3\nEpoch 62, Loss: 0.017141396179795265\nEpoch 63, Loss: 0.015505502000451088\nValidation Loss improved to 0.03143082186579704 Saving model 3\nEpoch 64, Loss: 0.009770204313099384\nValidation Loss improved to 0.03134150803089142 Saving model 3\nEpoch 65, Loss: 0.007419228553771973\nValidation Loss improved to 0.031030647456645966 Saving model 3\nEpoch 66, Loss: 0.00838452484458685\nValidation Loss improved to 0.030859123915433884 Saving model 3\nEpoch 67, Loss: 0.007031811401247978\nValidation Loss improved to 0.030721906572580338 Saving model 3\nEpoch 68, Loss: 0.00712498277425766\nValidation Loss improved to 0.030584344640374184 Saving model 3\nEpoch 69, Loss: 0.008111660368740559\nValidation Loss improved to 0.030450161546468735 Saving model 3\nEpoch 70, Loss: 0.006591232027858496\nValidation Loss improved to 0.03027624823153019 Saving model 3\nEpoch 71, Loss: 0.007127318065613508\nValidation Loss improved to 0.03001878410577774 Saving model 3\nEpoch 72, Loss: 0.007207958959043026\nValidation Loss improved to 0.02977587655186653 Saving model 3\nEpoch 73, Loss: 0.008380986750125885\nValidation Loss improved to 0.029520662501454353 Saving model 3\nEpoch 74, Loss: 0.006013224367052317\nValidation Loss improved to 0.029385117813944817 Saving model 3\nEpoch 75, Loss: 0.004108792170882225\nValidation Loss improved to 0.02923915535211563 Saving model 3\nEpoch 76, Loss: 0.005553559400141239\nEpoch 77, Loss: 0.013073801063001156\nEpoch 78, Loss: 0.007344617508351803\nValidation Loss improved to 0.029151368886232376 Saving model 3\nEpoch 79, Loss: 0.007088623940944672\nValidation Loss improved to 0.02904188260436058 Saving model 3\nEpoch 80, Loss: 0.006623405963182449\nValidation Loss improved to 0.028796792030334473 Saving model 3\nEpoch 81, Loss: 0.007040112279355526\nValidation Loss improved to 0.028549253940582275 Saving model 3\nEpoch 82, Loss: 0.006678330712020397\nValidation Loss improved to 0.028389401733875275 Saving model 3\nEpoch 83, Loss: 0.005403047893196344\nValidation Loss improved to 0.028157545253634453 Saving model 3\nEpoch 84, Loss: 0.00977480597794056\nEpoch 85, Loss: 0.012117267586290836\nValidation Loss improved to 0.028014732524752617 Saving model 3\nEpoch 86, Loss: 0.007728866301476955\nValidation Loss improved to 0.02776576206088066 Saving model 3\nEpoch 87, Loss: 0.00943825300782919\nEpoch 88, Loss: 0.014897785149514675\nValidation Loss improved to 0.027644237503409386 Saving model 3\nEpoch 89, Loss: 0.009447670541703701\nValidation Loss improved to 0.02757127396762371 Saving model 3\nEpoch 90, Loss: 0.00735839968547225\nValidation Loss improved to 0.02743387781083584 Saving model 3\nEpoch 91, Loss: 0.007392410654574633\nEpoch 92, Loss: 0.011912721209228039\nValidation Loss improved to 0.027372397482395172 Saving model 3\nEpoch 93, Loss: 0.009146890603005886\nValidation Loss improved to 0.027230773121118546 Saving model 3\nEpoch 94, Loss: 0.005962939001619816\nValidation Loss improved to 0.02704206295311451 Saving model 3\nEpoch 95, Loss: 0.0068172793835401535\nEpoch 96, Loss: 0.006426060106605291\nValidation Loss improved to 0.026893651112914085 Saving model 3\nEpoch 97, Loss: 0.004543552175164223\nValidation Loss improved to 0.02671360969543457 Saving model 3\nEpoch 98, Loss: 0.0055295866914093494\nValidation Loss improved to 0.026553992182016373 Saving model 3\nEpoch 99, Loss: 0.006171361543238163\nValidation Loss improved to 0.026378706097602844 Saving model 3\nEpoch 100, Loss: 0.008054415695369244\nValidation Loss improved to 0.02627226710319519 Saving model 3\nEpoch 101, Loss: 0.008855987340211868\nValidation Loss improved to 0.026209866628050804 Saving model 3\nEpoch 102, Loss: 0.009913763962686062\nEpoch 103, Loss: 0.010369464755058289\nValidation Loss improved to 0.026134243234992027 Saving model 3\nEpoch 104, Loss: 0.006530757527798414\nValidation Loss improved to 0.026062941178679466 Saving model 3\nEpoch 105, Loss: 0.007195836398750544\nValidation Loss improved to 0.025888826698064804 Saving model 3\nEpoch 106, Loss: 0.006942768581211567\nValidation Loss improved to 0.025731168687343597 Saving model 3\nEpoch 107, Loss: 0.006831658538430929\nValidation Loss improved to 0.025566158816218376 Saving model 3\nEpoch 108, Loss: 0.005542214028537273\nValidation Loss improved to 0.025417432188987732 Saving model 3\nEpoch 109, Loss: 0.00527518056333065\nValidation Loss improved to 0.025261165574193 Saving model 3\nEpoch 110, Loss: 0.007218596525490284\nValidation Loss improved to 0.025164330378174782 Saving model 3\nEpoch 111, Loss: 0.005320260301232338\nValidation Loss improved to 0.025032300502061844 Saving model 3\nEpoch 112, Loss: 0.006065648514777422\nValidation Loss improved to 0.02488078735768795 Saving model 3\nEpoch 113, Loss: 0.005964071024209261\nValidation Loss improved to 0.02477755956351757 Saving model 3\nEpoch 114, Loss: 0.004583347588777542\nValidation Loss improved to 0.02461705543100834 Saving model 3\nEpoch 115, Loss: 0.00503756757825613\nValidation Loss improved to 0.02450655773282051 Saving model 3\nEpoch 116, Loss: 0.005773533601313829\nValidation Loss improved to 0.024403566494584084 Saving model 3\nEpoch 117, Loss: 0.003928604535758495\nValidation Loss improved to 0.02430172637104988 Saving model 3\nEpoch 118, Loss: 0.005907226353883743\nValidation Loss improved to 0.024207014590501785 Saving model 3\nEpoch 119, Loss: 0.005072066560387611\nValidation Loss improved to 0.024049576371908188 Saving model 3\nEpoch 120, Loss: 0.004655663389712572\nValidation Loss improved to 0.023967484012246132 Saving model 3\nEpoch 121, Loss: 0.005402941722422838\nValidation Loss improved to 0.02385134808719158 Saving model 3\nEpoch 122, Loss: 0.0055327280424535275\nValidation Loss improved to 0.02373596839606762 Saving model 3\nEpoch 123, Loss: 0.007330180145800114\nValidation Loss improved to 0.02361242286860943 Saving model 3\nEpoch 124, Loss: 0.008208866231143475\nValidation Loss improved to 0.023498130962252617 Saving model 3\nEpoch 125, Loss: 0.00836095679551363\nValidation Loss improved to 0.02338278666138649 Saving model 3\nEpoch 126, Loss: 0.0064948624931275845\nValidation Loss improved to 0.02329556830227375 Saving model 3\nEpoch 127, Loss: 0.0068235257640480995\nValidation Loss improved to 0.023251546546816826 Saving model 3\nEpoch 128, Loss: 0.007535524666309357\nValidation Loss improved to 0.023168222978711128 Saving model 3\nEpoch 129, Loss: 0.007621375378221273\nValidation Loss improved to 0.023048898205161095 Saving model 3\nEpoch 130, Loss: 0.004088553134351969\nValidation Loss improved to 0.022953713312745094 Saving model 3\nEpoch 131, Loss: 0.004811808932572603\nValidation Loss improved to 0.022862667217850685 Saving model 3\nEpoch 132, Loss: 0.005892392247915268\nValidation Loss improved to 0.022772537544369698 Saving model 3\nEpoch 133, Loss: 0.00925661250948906\nValidation Loss improved to 0.022673295810818672 Saving model 3\nEpoch 134, Loss: 0.008439930155873299\nValidation Loss improved to 0.022617118433117867 Saving model 3\nEpoch 135, Loss: 0.005471406504511833\nValidation Loss improved to 0.022601759061217308 Saving model 3\nEpoch 136, Loss: 0.007270668167620897\nValidation Loss improved to 0.022496726363897324 Saving model 3\nEpoch 137, Loss: 0.004263288341462612\nValidation Loss improved to 0.022381018847227097 Saving model 3\nEpoch 138, Loss: 0.00636259326711297\nValidation Loss improved to 0.0222855843603611 Saving model 3\nEpoch 139, Loss: 0.004762276075780392\nValidation Loss improved to 0.022193467244505882 Saving model 3\nEpoch 140, Loss: 0.005482034757733345\nValidation Loss improved to 0.022123871371150017 Saving model 3\nEpoch 141, Loss: 0.007661961019039154\nValidation Loss improved to 0.022049570456147194 Saving model 3\nEpoch 142, Loss: 0.009211627766489983\nValidation Loss improved to 0.02194778062403202 Saving model 3\nEpoch 143, Loss: 0.007040996570140123\nValidation Loss improved to 0.02185569889843464 Saving model 3\nEpoch 144, Loss: 0.006195799447596073\nValidation Loss improved to 0.021745627745985985 Saving model 3\nEpoch 145, Loss: 0.004955877084285021\nValidation Loss improved to 0.021681709215044975 Saving model 3\nEpoch 146, Loss: 0.006889111362397671\nValidation Loss improved to 0.021579816937446594 Saving model 3\nEpoch 147, Loss: 0.0046596587635576725\nValidation Loss improved to 0.02150215022265911 Saving model 3\nEpoch 148, Loss: 0.004149565473198891\nValidation Loss improved to 0.021420547738671303 Saving model 3\nEpoch 149, Loss: 0.007016591727733612\nValidation Loss improved to 0.021361056715250015 Saving model 3\nEpoch 150, Loss: 0.0069265724159777164\nValidation Loss improved to 0.021299781277775764 Saving model 3\nEpoch 151, Loss: 0.005221278872340918\nValidation Loss improved to 0.021241765469312668 Saving model 3\nEpoch 152, Loss: 0.005423655733466148\nValidation Loss improved to 0.021193014457821846 Saving model 3\nEpoch 153, Loss: 0.004155488219112158\nValidation Loss improved to 0.021113263443112373 Saving model 3\nEpoch 154, Loss: 0.00664400914683938\nValidation Loss improved to 0.021109698340296745 Saving model 3\nEpoch 155, Loss: 0.005289054475724697\nValidation Loss improved to 0.021055353805422783 Saving model 3\nEpoch 156, Loss: 0.005673462525010109\nEpoch 157, Loss: 0.0065957484766840935\nEpoch 158, Loss: 0.0079726567491889\nValidation Loss improved to 0.02099202759563923 Saving model 3\nEpoch 159, Loss: 0.006040006410330534\nValidation Loss improved to 0.020930789411067963 Saving model 3\nEpoch 160, Loss: 0.00812821090221405\nValidation Loss improved to 0.02086080238223076 Saving model 3\nEpoch 161, Loss: 0.006579883862286806\nValidation Loss improved to 0.020798837766051292 Saving model 3\nEpoch 162, Loss: 0.004916005302220583\nValidation Loss improved to 0.020790914073586464 Saving model 3\nEpoch 163, Loss: 0.00575489504262805\nValidation Loss improved to 0.020725151523947716 Saving model 3\nEpoch 164, Loss: 0.0048767817206680775\nValidation Loss improved to 0.020659854635596275 Saving model 3\nEpoch 165, Loss: 0.005809462163597345\nValidation Loss improved to 0.020598361268639565 Saving model 3\nEpoch 166, Loss: 0.0051534767262637615\nValidation Loss improved to 0.02052922360599041 Saving model 3\nEpoch 167, Loss: 0.006791203748434782\nValidation Loss improved to 0.020484313368797302 Saving model 3\nEpoch 168, Loss: 0.006379290018230677\nValidation Loss improved to 0.02042326331138611 Saving model 3\nEpoch 169, Loss: 0.005815927404910326\nValidation Loss improved to 0.020383484661579132 Saving model 3\nEpoch 170, Loss: 0.005112328100949526\nValidation Loss improved to 0.02036273665726185 Saving model 3\nEpoch 171, Loss: 0.00605349475517869\nValidation Loss improved to 0.020310210064053535 Saving model 3\nEpoch 172, Loss: 0.003746099304407835\nValidation Loss improved to 0.020242130383849144 Saving model 3\nEpoch 173, Loss: 0.006427379325032234\nValidation Loss improved to 0.020193632692098618 Saving model 3\nEpoch 174, Loss: 0.006485446356236935\nValidation Loss improved to 0.020132308825850487 Saving model 3\nEpoch 175, Loss: 0.0048597934655845165\nValidation Loss improved to 0.0200568288564682 Saving model 3\nEpoch 176, Loss: 0.008113477379083633\nValidation Loss improved to 0.02002749592065811 Saving model 3\nEpoch 177, Loss: 0.006016156170517206\nValidation Loss improved to 0.01995260640978813 Saving model 3\nEpoch 178, Loss: 0.007378044072538614\nValidation Loss improved to 0.019931798800826073 Saving model 3\nEpoch 179, Loss: 0.007228836417198181\nValidation Loss improved to 0.019864141941070557 Saving model 3\nEpoch 180, Loss: 0.006003418937325478\nValidation Loss improved to 0.01978062279522419 Saving model 3\nEpoch 181, Loss: 0.006138003431260586\nValidation Loss improved to 0.019718511030077934 Saving model 3\nEpoch 182, Loss: 0.0069525097496807575\nValidation Loss improved to 0.019691329449415207 Saving model 3\nEpoch 183, Loss: 0.0070176743902266026\nValidation Loss improved to 0.019617239013314247 Saving model 3\nEpoch 184, Loss: 0.005337135400623083\nValidation Loss improved to 0.019566133618354797 Saving model 3\nEpoch 185, Loss: 0.004248804412782192\nValidation Loss improved to 0.019500527530908585 Saving model 3\nEpoch 186, Loss: 0.005346322897821665\nValidation Loss improved to 0.019446875900030136 Saving model 3\nEpoch 187, Loss: 0.008878994733095169\nEpoch 188, Loss: 0.009085951372981071\nEpoch 189, Loss: 0.01135954912751913\nEpoch 190, Loss: 0.008017178624868393\nValidation Loss improved to 0.01943824626505375 Saving model 3\nEpoch 191, Loss: 0.00761332456022501\nValidation Loss improved to 0.019400380551815033 Saving model 3\nEpoch 192, Loss: 0.0066090840846300125\nValidation Loss improved to 0.0193383377045393 Saving model 3\nEpoch 193, Loss: 0.004184783902019262\nValidation Loss improved to 0.0192869920283556 Saving model 3\nEpoch 194, Loss: 0.0033757714554667473\nValidation Loss improved to 0.019235139712691307 Saving model 3\nEpoch 195, Loss: 0.011821053922176361\nValidation Loss improved to 0.01922614313662052 Saving model 3\nEpoch 196, Loss: 0.009899516589939594\nValidation Loss improved to 0.019206982105970383 Saving model 3\nEpoch 197, Loss: 0.006816628854721785\nValidation Loss improved to 0.019162479788064957 Saving model 3\nEpoch 198, Loss: 0.0044106850400567055\nValidation Loss improved to 0.019113365560770035 Saving model 3\nEpoch 199, Loss: 0.005287757609039545\nValidation Loss improved to 0.019055023789405823 Saving model 3\nEpoch 200, Loss: 0.006219558417797089\nValidation Loss improved to 0.01902029849588871 Saving model 3\nEpoch 201, Loss: 0.0066865356639027596\nValidation Loss improved to 0.018979480490088463 Saving model 3\nEpoch 202, Loss: 0.005372908897697926\nValidation Loss improved to 0.018944917246699333 Saving model 3\nEpoch 203, Loss: 0.005292701069265604\nValidation Loss improved to 0.018885666504502296 Saving model 3\nEpoch 204, Loss: 0.006302869413048029\nValidation Loss improved to 0.018858730792999268 Saving model 3\nEpoch 205, Loss: 0.0072416383773088455\nValidation Loss improved to 0.018819177523255348 Saving model 3\nEpoch 206, Loss: 0.0064901686273515224\nValidation Loss improved to 0.018768737092614174 Saving model 3\nEpoch 207, Loss: 0.0033585240598767996\nValidation Loss improved to 0.018718818202614784 Saving model 3\nEpoch 208, Loss: 0.004780900198966265\nValidation Loss improved to 0.01867288164794445 Saving model 3\nEpoch 209, Loss: 0.0069526126608252525\nValidation Loss improved to 0.018664788454771042 Saving model 3\nEpoch 210, Loss: 0.0044914307072758675\nValidation Loss improved to 0.018620433285832405 Saving model 3\nEpoch 211, Loss: 0.005745400674641132\nValidation Loss improved to 0.018591560423374176 Saving model 3\nEpoch 212, Loss: 0.0041914223693311214\nValidation Loss improved to 0.018533451482653618 Saving model 3\nEpoch 213, Loss: 0.006512333173304796\nValidation Loss improved to 0.01848667487502098 Saving model 3\nEpoch 214, Loss: 0.005493462085723877\nEpoch 215, Loss: 0.005962907802313566\nValidation Loss improved to 0.018443506211042404 Saving model 3\nEpoch 216, Loss: 0.004807827062904835\nValidation Loss improved to 0.018398623913526535 Saving model 3\nEpoch 217, Loss: 0.004673540126532316\nValidation Loss improved to 0.01835407316684723 Saving model 3\nEpoch 218, Loss: 0.005471428390592337\nValidation Loss improved to 0.01834760792553425 Saving model 3\nEpoch 219, Loss: 0.006434268783777952\nValidation Loss improved to 0.018303528428077698 Saving model 3\nEpoch 220, Loss: 0.006525521632283926\nValidation Loss improved to 0.018254108726978302 Saving model 3\nEpoch 221, Loss: 0.007373766973614693\nValidation Loss improved to 0.018219249323010445 Saving model 3\nEpoch 222, Loss: 0.004240571055561304\nValidation Loss improved to 0.01818173937499523 Saving model 3\nEpoch 223, Loss: 0.0051958514377474785\nValidation Loss improved to 0.01813272014260292 Saving model 3\nEpoch 224, Loss: 0.005774390418082476\nValidation Loss improved to 0.018109537661075592 Saving model 3\nEpoch 225, Loss: 0.006346242967993021\nValidation Loss improved to 0.0180643480271101 Saving model 3\nEpoch 226, Loss: 0.004381923004984856\nValidation Loss improved to 0.018023109063506126 Saving model 3\nEpoch 227, Loss: 0.00442939018830657\nValidation Loss improved to 0.01799597404897213 Saving model 3\nEpoch 228, Loss: 0.004250515252351761\nValidation Loss improved to 0.0179484561085701 Saving model 3\nEpoch 229, Loss: 0.007242001127451658\nEpoch 230, Loss: 0.0053030760027468204\nEpoch 231, Loss: 0.004633283242583275\nValidation Loss improved to 0.01792112924158573 Saving model 3\nEpoch 232, Loss: 0.003953175153583288\nValidation Loss improved to 0.017878856509923935 Saving model 3\nEpoch 233, Loss: 0.0040870895609259605\nValidation Loss improved to 0.0178512129932642 Saving model 3\nEpoch 234, Loss: 0.004452300723642111\nValidation Loss improved to 0.01782042160630226 Saving model 3\nEpoch 235, Loss: 0.004404282197356224\nValidation Loss improved to 0.0177829097956419 Saving model 3\nEpoch 236, Loss: 0.00488501088693738\nValidation Loss improved to 0.017775418236851692 Saving model 3\nEpoch 237, Loss: 0.003915414214134216\nValidation Loss improved to 0.01773802377283573 Saving model 3\nEpoch 238, Loss: 0.007112535182386637\nValidation Loss improved to 0.017705600708723068 Saving model 3\nEpoch 239, Loss: 0.0057830652222037315\nValidation Loss improved to 0.017664173617959023 Saving model 3\nEpoch 240, Loss: 0.005893999245017767\nValidation Loss improved to 0.01762312464416027 Saving model 3\nEpoch 241, Loss: 0.007901189848780632\nValidation Loss improved to 0.017582599073648453 Saving model 3\nEpoch 242, Loss: 0.0062673669308424\nValidation Loss improved to 0.017577113583683968 Saving model 3\nEpoch 243, Loss: 0.007753788027912378\nValidation Loss improved to 0.017563413828611374 Saving model 3\nEpoch 244, Loss: 0.006837792228907347\nValidation Loss improved to 0.01751963049173355 Saving model 3\nEpoch 245, Loss: 0.00317005580291152\nValidation Loss improved to 0.017498329281806946 Saving model 3\nEpoch 246, Loss: 0.004093598108738661\nValidation Loss improved to 0.017483964562416077 Saving model 3\nEpoch 247, Loss: 0.004394377116113901\nValidation Loss improved to 0.01744440756738186 Saving model 3\nEpoch 248, Loss: 0.009589130990207195\nValidation Loss improved to 0.017408300191164017 Saving model 3\nEpoch 249, Loss: 0.01114418264478445\nValidation Loss improved to 0.017376579344272614 Saving model 3\nEpoch 250, Loss: 0.006622250657528639\nValidation Loss improved to 0.017345771193504333 Saving model 3\nEpoch 251, Loss: 0.005568935535848141\nValidation Loss improved to 0.01730388216674328 Saving model 3\nEpoch 252, Loss: 0.005117250140756369\nValidation Loss improved to 0.017266524955630302 Saving model 3\nEpoch 253, Loss: 0.0062536620534956455\nValidation Loss improved to 0.01725289411842823 Saving model 3\nEpoch 254, Loss: 0.0032208827324211597\nValidation Loss improved to 0.017242269590497017 Saving model 3\nEpoch 255, Loss: 0.007311687804758549\nValidation Loss improved to 0.017208242788910866 Saving model 3\nEpoch 256, Loss: 0.007544645573943853\nValidation Loss improved to 0.017174862325191498 Saving model 3\nEpoch 257, Loss: 0.006965605076402426\nValidation Loss improved to 0.017137465998530388 Saving model 3\nEpoch 258, Loss: 0.005314147099852562\nValidation Loss improved to 0.017101503908634186 Saving model 3\nEpoch 259, Loss: 0.00413814140483737\nValidation Loss improved to 0.017069311812520027 Saving model 3\nEpoch 260, Loss: 0.004695111885666847\nValidation Loss improved to 0.017052510753273964 Saving model 3\nEpoch 261, Loss: 0.004717874340713024\nValidation Loss improved to 0.01703229732811451 Saving model 3\nEpoch 262, Loss: 0.005620245821774006\nValidation Loss improved to 0.01699664257466793 Saving model 3\nEpoch 263, Loss: 0.004449537955224514\nValidation Loss improved to 0.016960401087999344 Saving model 3\nEpoch 264, Loss: 0.0056761992163956165\nValidation Loss improved to 0.01693064533174038 Saving model 3\nEpoch 265, Loss: 0.0041082571260631084\nValidation Loss improved to 0.01690167561173439 Saving model 3\nEpoch 266, Loss: 0.004431118723005056\nValidation Loss improved to 0.016884105280041695 Saving model 3\nEpoch 267, Loss: 0.0033615808933973312\nValidation Loss improved to 0.016877485439181328 Saving model 3\nEpoch 268, Loss: 0.004132532048970461\nValidation Loss improved to 0.01684187725186348 Saving model 3\nEpoch 269, Loss: 0.003524096217006445\nValidation Loss improved to 0.016812466084957123 Saving model 3\nEpoch 270, Loss: 0.0029005329124629498\nValidation Loss improved to 0.016788391396403313 Saving model 3\nEpoch 271, Loss: 0.004473796579986811\nValidation Loss improved to 0.016756964847445488 Saving model 3\nEpoch 272, Loss: 0.00447206012904644\nValidation Loss improved to 0.016727903857827187 Saving model 3\nEpoch 273, Loss: 0.004168327897787094\nValidation Loss improved to 0.01669645495712757 Saving model 3\nEpoch 274, Loss: 0.004362371750175953\nValidation Loss improved to 0.01666218973696232 Saving model 3\nEpoch 275, Loss: 0.003894617548212409\nValidation Loss improved to 0.016634371131658554 Saving model 3\nEpoch 276, Loss: 0.0032871279399842024\nValidation Loss improved to 0.01660052500665188 Saving model 3\nEpoch 277, Loss: 0.0033019676338881254\nValidation Loss improved to 0.016571946442127228 Saving model 3\nEpoch 278, Loss: 0.004947693552821875\nValidation Loss improved to 0.01655295118689537 Saving model 3\nEpoch 279, Loss: 0.007678934372961521\nValidation Loss improved to 0.016533425077795982 Saving model 3\nEpoch 280, Loss: 0.0065428693778812885\nValidation Loss improved to 0.016499629244208336 Saving model 3\nEpoch 281, Loss: 0.0057310676202178\nValidation Loss improved to 0.01646600477397442 Saving model 3\nEpoch 282, Loss: 0.008279978297650814\nValidation Loss improved to 0.01643264852464199 Saving model 3\nEpoch 283, Loss: 0.006808768026530743\nEpoch 284, Loss: 0.0076917000114917755\nEpoch 285, Loss: 0.008135267533361912\nEpoch 286, Loss: 0.004904207307845354\nEpoch 287, Loss: 0.005262172780930996\nValidation Loss improved to 0.01642741821706295 Saving model 3\nEpoch 288, Loss: 0.0029760340694338083\nValidation Loss improved to 0.016397390514612198 Saving model 3\nEpoch 289, Loss: 0.0032702002208679914\nValidation Loss improved to 0.016381876543164253 Saving model 3\nEpoch 290, Loss: 0.0036371599417179823\nValidation Loss improved to 0.016345756128430367 Saving model 3\nEpoch 291, Loss: 0.005255729425698519\nValidation Loss improved to 0.016321716830134392 Saving model 3\nEpoch 292, Loss: 0.004612933844327927\nValidation Loss improved to 0.016294388100504875 Saving model 3\nEpoch 293, Loss: 0.004265681374818087\nValidation Loss improved to 0.0162651464343071 Saving model 3\nEpoch 294, Loss: 0.004601599648594856\nValidation Loss improved to 0.016234295442700386 Saving model 3\nEpoch 295, Loss: 0.006039346568286419\nValidation Loss improved to 0.01622861996293068 Saving model 3\nEpoch 296, Loss: 0.006069102790206671\nValidation Loss improved to 0.01621170900762081 Saving model 3\nEpoch 297, Loss: 0.005213080905377865\nValidation Loss improved to 0.016206994652748108 Saving model 3\nEpoch 298, Loss: 0.0066971927881240845\nValidation Loss improved to 0.01617254875600338 Saving model 3\nEpoch 299, Loss: 0.004095785785466433\nValidation Loss improved to 0.016149941831827164 Saving model 3\nEpoch 300, Loss: 0.0032824943773448467\nValidation Loss improved to 0.01612452045083046 Saving model 3\nEpoch 301, Loss: 0.0036464196164160967\nValidation Loss improved to 0.01610669307410717 Saving model 3\nEpoch 302, Loss: 0.0031392467208206654\nValidation Loss improved to 0.016082752496004105 Saving model 3\nEpoch 303, Loss: 0.005739927291870117\nValidation Loss improved to 0.01606794446706772 Saving model 3\nEpoch 304, Loss: 0.004671027883887291\nValidation Loss improved to 0.016039462760090828 Saving model 3\nEpoch 305, Loss: 0.003912965301424265\nValidation Loss improved to 0.016009461134672165 Saving model 3\nEpoch 306, Loss: 0.007565823849290609\nValidation Loss improved to 0.015979042276740074 Saving model 3\nEpoch 307, Loss: 0.0057893358170986176\nValidation Loss improved to 0.015968957915902138 Saving model 3\nEpoch 308, Loss: 0.004200990777462721\nValidation Loss improved to 0.015947166830301285 Saving model 3\nEpoch 309, Loss: 0.003455899888649583\nValidation Loss improved to 0.015934346243739128 Saving model 3\nEpoch 310, Loss: 0.005601946730166674\nValidation Loss improved to 0.01591375097632408 Saving model 3\nEpoch 311, Loss: 0.003313925350084901\nValidation Loss improved to 0.015881670638918877 Saving model 3\nEpoch 312, Loss: 0.0041138604283332825\nValidation Loss improved to 0.015859831124544144 Saving model 3\nEpoch 313, Loss: 0.004308050032705069\nValidation Loss improved to 0.015830064192414284 Saving model 3\nEpoch 314, Loss: 0.004471087362617254\nValidation Loss improved to 0.01580999232828617 Saving model 3\nEpoch 315, Loss: 0.006083858665078878\nValidation Loss improved to 0.015776628628373146 Saving model 3\nEpoch 316, Loss: 0.00588618591427803\nValidation Loss improved to 0.01575738936662674 Saving model 3\nEpoch 317, Loss: 0.005427139811217785\nEpoch 318, Loss: 0.005775847006589174\nValidation Loss improved to 0.01575198769569397 Saving model 3\nEpoch 319, Loss: 0.004977008327841759\nValidation Loss improved to 0.015739943832159042 Saving model 3\nEpoch 320, Loss: 0.004158326890319586\nValidation Loss improved to 0.01571626402437687 Saving model 3\nEpoch 321, Loss: 0.009867041371762753\nValidation Loss improved to 0.015698963776230812 Saving model 3\nEpoch 322, Loss: 0.0053560626693069935\nValidation Loss improved to 0.01567056216299534 Saving model 3\nEpoch 323, Loss: 0.008816204033792019\nValidation Loss improved to 0.015664927661418915 Saving model 3\nEpoch 324, Loss: 0.007889638654887676\nValidation Loss improved to 0.015657523646950722 Saving model 3\nEpoch 325, Loss: 0.0056226300075650215\nValidation Loss improved to 0.01564943604171276 Saving model 3\nEpoch 326, Loss: 0.005824489984661341\nValidation Loss improved to 0.01563095673918724 Saving model 3\nEpoch 327, Loss: 0.004557749722152948\nValidation Loss improved to 0.015606057830154896 Saving model 3\nEpoch 328, Loss: 0.006988752167671919\nValidation Loss improved to 0.015593109652400017 Saving model 3\nEpoch 329, Loss: 0.005822786130011082\nValidation Loss improved to 0.015573066659271717 Saving model 3\nEpoch 330, Loss: 0.0036497097462415695\nValidation Loss improved to 0.015546956099569798 Saving model 3\nEpoch 331, Loss: 0.004269538912922144\nValidation Loss improved to 0.015525324270129204 Saving model 3\nEpoch 332, Loss: 0.003827510867267847\nValidation Loss improved to 0.01550230197608471 Saving model 3\nEpoch 333, Loss: 0.005199403967708349\nEpoch 334, Loss: 0.004289928358048201\nValidation Loss improved to 0.015488472767174244 Saving model 3\nEpoch 335, Loss: 0.009267998859286308\nValidation Loss improved to 0.015486501157283783 Saving model 3\nEpoch 336, Loss: 0.0067114937119185925\nValidation Loss improved to 0.015464152209460735 Saving model 3\nEpoch 337, Loss: 0.005268291104584932\nValidation Loss improved to 0.015452612191438675 Saving model 3\nEpoch 338, Loss: 0.004153565037995577\nValidation Loss improved to 0.015444089658558369 Saving model 3\nEpoch 339, Loss: 0.002839270979166031\nValidation Loss improved to 0.01542885322123766 Saving model 3\nEpoch 340, Loss: 0.0036519451532512903\nValidation Loss improved to 0.015410010702908039 Saving model 3\nEpoch 341, Loss: 0.005041843745857477\nValidation Loss improved to 0.015385483391582966 Saving model 3\nEpoch 342, Loss: 0.004297629930078983\nValidation Loss improved to 0.015365413390100002 Saving model 3\nEpoch 343, Loss: 0.005120106972754002\nValidation Loss improved to 0.015358302742242813 Saving model 3\nEpoch 344, Loss: 0.005032285116612911\nValidation Loss improved to 0.015339842066168785 Saving model 3\nEpoch 345, Loss: 0.004780230112373829\nValidation Loss improved to 0.015324383042752743 Saving model 3\nEpoch 346, Loss: 0.005786546040326357\nValidation Loss improved to 0.015296388417482376 Saving model 3\nEpoch 347, Loss: 0.00920367892831564\nEpoch 348, Loss: 0.009464023634791374\nEpoch 349, Loss: 0.006109118927270174\nValidation Loss improved to 0.015290405601263046 Saving model 3\nEpoch 350, Loss: 0.005863456055521965\nValidation Loss improved to 0.015284355729818344 Saving model 3\nEpoch 351, Loss: 0.0064764730632305145\nValidation Loss improved to 0.015270986594259739 Saving model 3\nEpoch 352, Loss: 0.004873019643127918\nEpoch 353, Loss: 0.00613169651478529\nValidation Loss improved to 0.01526616234332323 Saving model 3\nEpoch 354, Loss: 0.0037846730556339025\nValidation Loss improved to 0.015247986651957035 Saving model 3\nEpoch 355, Loss: 0.0049225324764847755\nValidation Loss improved to 0.015225238166749477 Saving model 3\nEpoch 356, Loss: 0.00652543967589736\nValidation Loss improved to 0.015209983102977276 Saving model 3\nEpoch 357, Loss: 0.006696359720081091\nValidation Loss improved to 0.015206512063741684 Saving model 3\nEpoch 358, Loss: 0.004763106349855661\nValidation Loss improved to 0.015200413763523102 Saving model 3\nEpoch 359, Loss: 0.003380542853847146\nValidation Loss improved to 0.015185218304395676 Saving model 3\nEpoch 360, Loss: 0.005557811353355646\nValidation Loss improved to 0.015178561210632324 Saving model 3\nEpoch 361, Loss: 0.004132253583520651\nValidation Loss improved to 0.0151599682867527 Saving model 3\nEpoch 362, Loss: 0.005681495647877455\nValidation Loss improved to 0.01515038963407278 Saving model 3\nEpoch 363, Loss: 0.0036374630872160196\nValidation Loss improved to 0.015123892575502396 Saving model 3\nEpoch 364, Loss: 0.004341537598520517\nEpoch 365, Loss: 0.006772094406187534\nEpoch 366, Loss: 0.00502275163307786\nValidation Loss improved to 0.01510782539844513 Saving model 3\nEpoch 367, Loss: 0.0044153109192848206\nValidation Loss improved to 0.015087741427123547 Saving model 3\nEpoch 368, Loss: 0.004214092157781124\nValidation Loss improved to 0.015079076401889324 Saving model 3\nEpoch 369, Loss: 0.0033510865177959204\nValidation Loss improved to 0.015064002946019173 Saving model 3\nEpoch 370, Loss: 0.004508520010858774\nValidation Loss improved to 0.015055065974593163 Saving model 3\nEpoch 371, Loss: 0.0032164803706109524\nValidation Loss improved to 0.015033658593893051 Saving model 3\nEpoch 372, Loss: 0.0047714742831885815\nValidation Loss improved to 0.01502467505633831 Saving model 3\nEpoch 373, Loss: 0.0048571680672466755\nEpoch 374, Loss: 0.006542965304106474\nValidation Loss improved to 0.015021727420389652 Saving model 3\nEpoch 375, Loss: 0.006344279274344444\nValidation Loss improved to 0.014998099766671658 Saving model 3\nEpoch 376, Loss: 0.005580077413469553\nValidation Loss improved to 0.014989753253757954 Saving model 3\nEpoch 377, Loss: 0.004346906673163176\nValidation Loss improved to 0.014967303723096848 Saving model 3\nEpoch 378, Loss: 0.00343208946287632\nValidation Loss improved to 0.014952394179999828 Saving model 3\nEpoch 379, Loss: 0.004430850502103567\nValidation Loss improved to 0.014940992929041386 Saving model 3\nEpoch 380, Loss: 0.003103303024545312\nValidation Loss improved to 0.014914792031049728 Saving model 3\nEpoch 381, Loss: 0.0034203387331217527\nValidation Loss improved to 0.014896044507622719 Saving model 3\nEpoch 382, Loss: 0.0026544127613306046\nValidation Loss improved to 0.014885383658111095 Saving model 3\nEpoch 383, Loss: 0.006960118189454079\nEpoch 384, Loss: 0.008135981857776642\nValidation Loss improved to 0.014874512329697609 Saving model 3\nEpoch 385, Loss: 0.006591344717890024\nValidation Loss improved to 0.0148554602637887 Saving model 3\nEpoch 386, Loss: 0.003281973535194993\nValidation Loss improved to 0.014839629642665386 Saving model 3\nEpoch 387, Loss: 0.0046664574183523655\nValidation Loss improved to 0.014832613058388233 Saving model 3\nEpoch 388, Loss: 0.004011658951640129\nValidation Loss improved to 0.014814500696957111 Saving model 3\nEpoch 389, Loss: 0.0036728237755596638\nValidation Loss improved to 0.014791551046073437 Saving model 3\nEpoch 390, Loss: 0.004987652413547039\nValidation Loss improved to 0.014775305986404419 Saving model 3\nEpoch 391, Loss: 0.006794262211769819\nValidation Loss improved to 0.014768162742257118 Saving model 3\nEpoch 392, Loss: 0.0046573965810239315\nValidation Loss improved to 0.014760336838662624 Saving model 3\nEpoch 393, Loss: 0.004391049034893513\nValidation Loss improved to 0.014748823828995228 Saving model 3\nEpoch 394, Loss: 0.0032786063384264708\nValidation Loss improved to 0.01474812999367714 Saving model 3\nEpoch 395, Loss: 0.005596327129751444\nValidation Loss improved to 0.014732721261680126 Saving model 3\nEpoch 396, Loss: 0.004227418918162584\nValidation Loss improved to 0.014717194251716137 Saving model 3\nEpoch 397, Loss: 0.005182758904993534\nValidation Loss improved to 0.014713689684867859 Saving model 3\nEpoch 398, Loss: 0.005101522896438837\nValidation Loss improved to 0.014700023457407951 Saving model 3\nEpoch 399, Loss: 0.004043687134981155\nValidation Loss improved to 0.01468109991401434 Saving model 3\nEpoch 400, Loss: 0.003490353235974908\nValidation Loss improved to 0.014666962437331676 Saving model 3\nEpoch 401, Loss: 0.004562176298350096\nValidation Loss improved to 0.014662332832813263 Saving model 3\nEpoch 402, Loss: 0.003753342665731907\nValidation Loss improved to 0.014648680575191975 Saving model 3\nEpoch 403, Loss: 0.003570428118109703\nValidation Loss improved to 0.014624729752540588 Saving model 3\nEpoch 404, Loss: 0.0037649034056812525\nValidation Loss improved to 0.014620176516473293 Saving model 3\nEpoch 405, Loss: 0.005181507207453251\nValidation Loss improved to 0.01460112165659666 Saving model 3\nEpoch 406, Loss: 0.003919520415365696\nValidation Loss improved to 0.014596476219594479 Saving model 3\nEpoch 407, Loss: 0.004796203225851059\nValidation Loss improved to 0.014591041952371597 Saving model 3\nEpoch 408, Loss: 0.004907342605292797\nValidation Loss improved to 0.014579370617866516 Saving model 3\nEpoch 409, Loss: 0.005563416518270969\nValidation Loss improved to 0.014562699943780899 Saving model 3\nEpoch 410, Loss: 0.004200456663966179\nValidation Loss improved to 0.014540567062795162 Saving model 3\nEpoch 411, Loss: 0.004436475690454245\nValidation Loss improved to 0.014520102180540562 Saving model 3\nEpoch 412, Loss: 0.0041606007143855095\nValidation Loss improved to 0.014503004029393196 Saving model 3\nEpoch 413, Loss: 0.00454166904091835\nEpoch 414, Loss: 0.007196522783488035\nValidation Loss improved to 0.014502394944429398 Saving model 3\nEpoch 415, Loss: 0.0035503681283444166\nValidation Loss improved to 0.014480196870863438 Saving model 3\nEpoch 416, Loss: 0.003630787367001176\nValidation Loss improved to 0.01445681881159544 Saving model 3\nEpoch 417, Loss: 0.005417505744844675\nValidation Loss improved to 0.014445578679442406 Saving model 3\nEpoch 418, Loss: 0.0042064557783305645\nValidation Loss improved to 0.014432886615395546 Saving model 3\nEpoch 419, Loss: 0.005782764870673418\nValidation Loss improved to 0.0144205242395401 Saving model 3\nEpoch 420, Loss: 0.006070223171263933\nValidation Loss improved to 0.014410209842026234 Saving model 3\nEpoch 421, Loss: 0.0035194039810448885\nValidation Loss improved to 0.014389757066965103 Saving model 3\nEpoch 422, Loss: 0.0029420803766697645\nValidation Loss improved to 0.014371821656823158 Saving model 3\nEpoch 423, Loss: 0.003959510941058397\nValidation Loss improved to 0.014362187124788761 Saving model 3\nEpoch 424, Loss: 0.0057826838456094265\nEpoch 425, Loss: 0.002795750042423606\nValidation Loss improved to 0.014351222664117813 Saving model 3\nEpoch 426, Loss: 0.0035394756123423576\nValidation Loss improved to 0.014333974570035934 Saving model 3\nEpoch 427, Loss: 0.006162717007100582\nValidation Loss improved to 0.01432009693235159 Saving model 3\nEpoch 428, Loss: 0.004707608837634325\nValidation Loss improved to 0.01431802473962307 Saving model 3\nEpoch 429, Loss: 0.004065755754709244\nValidation Loss improved to 0.0143064484000206 Saving model 3\nEpoch 430, Loss: 0.004329484887421131\nValidation Loss improved to 0.014300905168056488 Saving model 3\nEpoch 431, Loss: 0.006605514790862799\nValidation Loss improved to 0.014283761382102966 Saving model 3\nEpoch 432, Loss: 0.003229129360988736\nValidation Loss improved to 0.01426912471652031 Saving model 3\nEpoch 433, Loss: 0.00620562769472599\nEpoch 434, Loss: 0.006544729229062796\nValidation Loss improved to 0.014260021969676018 Saving model 3\nEpoch 435, Loss: 0.00411482946947217\nValidation Loss improved to 0.01424819603562355 Saving model 3\nEpoch 436, Loss: 0.006018344778567553\nValidation Loss improved to 0.014233875088393688 Saving model 3\nEpoch 437, Loss: 0.003855480346828699\nValidation Loss improved to 0.014223757199943066 Saving model 3\nEpoch 438, Loss: 0.004276346880942583\nValidation Loss improved to 0.014216451905667782 Saving model 3\nEpoch 439, Loss: 0.004323847126215696\nValidation Loss improved to 0.014205294661223888 Saving model 3\nEpoch 440, Loss: 0.004681222606450319\nValidation Loss improved to 0.014203816652297974 Saving model 3\nEpoch 441, Loss: 0.004746291786432266\nEpoch 442, Loss: 0.005356003064662218\nValidation Loss improved to 0.014195703901350498 Saving model 3\nEpoch 443, Loss: 0.004476542118936777\nValidation Loss improved to 0.014192182570695877 Saving model 3\nEpoch 444, Loss: 0.0053398036397993565\nValidation Loss improved to 0.014186663553118706 Saving model 3\nEpoch 445, Loss: 0.003503320971503854\nEpoch 446, Loss: 0.005672137252986431\nValidation Loss improved to 0.01417603436857462 Saving model 3\nEpoch 447, Loss: 0.005661281757056713\nValidation Loss improved to 0.014160620979964733 Saving model 3\nEpoch 448, Loss: 0.005702869035303593\nEpoch 449, Loss: 0.007764479611068964\nValidation Loss improved to 0.014160140417516232 Saving model 3\nEpoch 450, Loss: 0.005451237317174673\nValidation Loss improved to 0.014150787144899368 Saving model 3\nEpoch 451, Loss: 0.003932963591068983\nValidation Loss improved to 0.014142314903438091 Saving model 3\nEpoch 452, Loss: 0.0037496876902878284\nValidation Loss improved to 0.014130820520222187 Saving model 3\nEpoch 453, Loss: 0.003899873234331608\nValidation Loss improved to 0.014121470972895622 Saving model 3\nEpoch 454, Loss: 0.004936971701681614\nValidation Loss improved to 0.014105362817645073 Saving model 3\nEpoch 455, Loss: 0.004296978935599327\nValidation Loss improved to 0.014091871678829193 Saving model 3\nEpoch 456, Loss: 0.0036374058108776808\nValidation Loss improved to 0.014080161228775978 Saving model 3\nEpoch 457, Loss: 0.005031846929341555\nValidation Loss improved to 0.014071924611926079 Saving model 3\nEpoch 458, Loss: 0.004607921000570059\nValidation Loss improved to 0.014063112437725067 Saving model 3\nEpoch 459, Loss: 0.003982659429311752\nValidation Loss improved to 0.01404715608805418 Saving model 3\nEpoch 460, Loss: 0.004261440131813288\nValidation Loss improved to 0.014044484123587608 Saving model 3\nEpoch 461, Loss: 0.0036598339211195707\nValidation Loss improved to 0.01402944978326559 Saving model 3\nEpoch 462, Loss: 0.005670787766575813\nValidation Loss improved to 0.01402753684669733 Saving model 3\nEpoch 463, Loss: 0.0046412223018705845\nValidation Loss improved to 0.014011649414896965 Saving model 3\nEpoch 464, Loss: 0.0038906747940927744\nValidation Loss improved to 0.0139992106705904 Saving model 3\nEpoch 465, Loss: 0.0025282660499215126\nValidation Loss improved to 0.013996672816574574 Saving model 3\nEpoch 466, Loss: 0.004956464283168316\nValidation Loss improved to 0.013978628441691399 Saving model 3\nEpoch 467, Loss: 0.004371977411210537\nValidation Loss improved to 0.01396614033728838 Saving model 3\nEpoch 468, Loss: 0.005047903396189213\nValidation Loss improved to 0.013951192609965801 Saving model 3\nEpoch 469, Loss: 0.005453960504382849\nValidation Loss improved to 0.013936277478933334 Saving model 3\nEpoch 470, Loss: 0.006503026466816664\nValidation Loss improved to 0.013921331614255905 Saving model 3\nEpoch 471, Loss: 0.004193626809865236\nValidation Loss improved to 0.013907821848988533 Saving model 3\nEpoch 472, Loss: 0.0026324372738599777\nValidation Loss improved to 0.013894492760300636 Saving model 3\nEpoch 473, Loss: 0.0024558056611567736\nValidation Loss improved to 0.013893741182982922 Saving model 3\nEpoch 474, Loss: 0.005712485406547785\nValidation Loss improved to 0.013882595114409924 Saving model 3\nEpoch 475, Loss: 0.004725929815322161\nValidation Loss improved to 0.013874984346330166 Saving model 3\nEpoch 476, Loss: 0.005360536277294159\nValidation Loss improved to 0.01386480126529932 Saving model 3\nEpoch 477, Loss: 0.004078798461705446\nEpoch 478, Loss: 0.0076199909672141075\nEpoch 479, Loss: 0.006265611387789249\nEpoch 480, Loss: 0.0045166779309511185\nValidation Loss improved to 0.013856003992259502 Saving model 3\nEpoch 481, Loss: 0.004055301193147898\nValidation Loss improved to 0.013849052600562572 Saving model 3\nEpoch 482, Loss: 0.004158792085945606\nValidation Loss improved to 0.013839231804013252 Saving model 3\nEpoch 483, Loss: 0.005004548467695713\nValidation Loss improved to 0.01382568757981062 Saving model 3\nEpoch 484, Loss: 0.0053724562749266624\nValidation Loss improved to 0.013823704794049263 Saving model 3\nEpoch 485, Loss: 0.0050233821384608746\nValidation Loss improved to 0.013820291496813297 Saving model 3\nEpoch 486, Loss: 0.004706425126641989\nEpoch 487, Loss: 0.006149983033537865\nEpoch 488, Loss: 0.004567871801555157\nValidation Loss improved to 0.013817709870636463 Saving model 3\nEpoch 489, Loss: 0.004912453703582287\nValidation Loss improved to 0.013813459314405918 Saving model 3\nEpoch 490, Loss: 0.0047326539643108845\nValidation Loss improved to 0.013805515132844448 Saving model 3\nEpoch 491, Loss: 0.004605588968843222\nEpoch 492, Loss: 0.003932469058781862\nValidation Loss improved to 0.013802342116832733 Saving model 3\nEpoch 493, Loss: 0.002716989954933524\nValidation Loss improved to 0.013789789751172066 Saving model 3\nEpoch 494, Loss: 0.003974846098572016\nEpoch 495, Loss: 0.0043356893584132195\nValidation Loss improved to 0.013783435337245464 Saving model 3\nEpoch 496, Loss: 0.003088568337261677\nValidation Loss improved to 0.013764884322881699 Saving model 3\nEpoch 497, Loss: 0.004273459780961275\nValidation Loss improved to 0.01375566329807043 Saving model 3\nEpoch 498, Loss: 0.004457115661352873\nValidation Loss improved to 0.013750094920396805 Saving model 3\nEpoch 499, Loss: 0.0037769125774502754\nValidation Loss improved to 0.013740750029683113 Saving model 3\nEpoch 500, Loss: 0.0033060952555388212\nValidation Loss improved to 0.013732918538153172 Saving model 3\nEpoch 501, Loss: 0.003489290364086628\nValidation Loss improved to 0.013721720315515995 Saving model 3\nEpoch 502, Loss: 0.0034276521764695644\nValidation Loss improved to 0.013704800978302956 Saving model 3\nEpoch 503, Loss: 0.0032558967359364033\nValidation Loss improved to 0.013688440434634686 Saving model 3\nEpoch 504, Loss: 0.003446391550824046\nValidation Loss improved to 0.013673814944922924 Saving model 3\nEpoch 505, Loss: 0.007384515833109617\nValidation Loss improved to 0.013671553693711758 Saving model 3\nEpoch 506, Loss: 0.0059108310379087925\nValidation Loss improved to 0.013657188974320889 Saving model 3\nEpoch 507, Loss: 0.0034562991932034492\nValidation Loss improved to 0.013651521876454353 Saving model 3\nEpoch 508, Loss: 0.0031276626978069544\nValidation Loss improved to 0.013642041012644768 Saving model 3\nEpoch 509, Loss: 0.0033890404738485813\nValidation Loss improved to 0.013628480955958366 Saving model 3\nEpoch 510, Loss: 0.0032733057159930468\nValidation Loss improved to 0.01361482311040163 Saving model 3\nEpoch 511, Loss: 0.0055654034949839115\nValidation Loss improved to 0.01360477413982153 Saving model 3\nEpoch 512, Loss: 0.00524489302188158\nValidation Loss improved to 0.013597373850643635 Saving model 3\nEpoch 513, Loss: 0.00457859318703413\nValidation Loss improved to 0.01358405128121376 Saving model 3\nEpoch 514, Loss: 0.003966625779867172\nValidation Loss improved to 0.013573311269283295 Saving model 3\nEpoch 515, Loss: 0.006085391156375408\nValidation Loss improved to 0.013558807782828808 Saving model 3\nEpoch 516, Loss: 0.004325139801949263\nValidation Loss improved to 0.013549458235502243 Saving model 3\nEpoch 517, Loss: 0.0030353746842592955\nValidation Loss improved to 0.013545673340559006 Saving model 3\nEpoch 518, Loss: 0.0046824682503938675\nValidation Loss improved to 0.01353133749216795 Saving model 3\nEpoch 519, Loss: 0.006794882472604513\nValidation Loss improved to 0.013530058786273003 Saving model 3\nEpoch 520, Loss: 0.0054151457734405994\nValidation Loss improved to 0.013516021892428398 Saving model 3\nEpoch 521, Loss: 0.0050417217426002026\nEpoch 522, Loss: 0.006980527192354202\nValidation Loss improved to 0.013510523363947868 Saving model 3\nEpoch 523, Loss: 0.006637554615736008\nValidation Loss improved to 0.013505461625754833 Saving model 3\nEpoch 524, Loss: 0.00516468845307827\nValidation Loss improved to 0.013505387119948864 Saving model 3\nEpoch 525, Loss: 0.004852096550166607\nValidation Loss improved to 0.013492300175130367 Saving model 3\nEpoch 526, Loss: 0.008444379083812237\nEpoch 527, Loss: 0.007938952185213566\nEpoch 528, Loss: 0.008841033093631268\nEpoch 529, Loss: 0.007276538293808699\nEpoch 530, Loss: 0.004010102711617947\nEpoch 531, Loss: 0.0038059439975768328\nValidation Loss improved to 0.013490972109138966 Saving model 3\nEpoch 532, Loss: 0.003329825820401311\nValidation Loss improved to 0.013486731797456741 Saving model 3\nEpoch 533, Loss: 0.005011672154068947\nValidation Loss improved to 0.013475285843014717 Saving model 3\nEpoch 534, Loss: 0.005070506129413843\nEpoch 535, Loss: 0.004921845160424709\nValidation Loss improved to 0.013468315824866295 Saving model 3\nEpoch 536, Loss: 0.004572931677103043\nValidation Loss improved to 0.01346106082201004 Saving model 3\nEpoch 537, Loss: 0.00553160673007369\nValidation Loss improved to 0.013451902195811272 Saving model 3\nEpoch 538, Loss: 0.004121167119592428\nValidation Loss improved to 0.01344335824251175 Saving model 3\nEpoch 539, Loss: 0.004536674357950687\nValidation Loss improved to 0.01343845296651125 Saving model 3\nEpoch 540, Loss: 0.002639273414388299\nValidation Loss improved to 0.013437967747449875 Saving model 3\nEpoch 541, Loss: 0.0028440228197723627\nValidation Loss improved to 0.013425737619400024 Saving model 3\nEpoch 542, Loss: 0.004123368300497532\nValidation Loss improved to 0.013419365510344505 Saving model 3\nEpoch 543, Loss: 0.002817984903231263\nValidation Loss improved to 0.01340712420642376 Saving model 3\nEpoch 544, Loss: 0.003116231644526124\nValidation Loss improved to 0.013398018665611744 Saving model 3\nEpoch 545, Loss: 0.0028448649682104588\nValidation Loss improved to 0.013387681916356087 Saving model 3\nEpoch 546, Loss: 0.003127021947875619\nValidation Loss improved to 0.013376622460782528 Saving model 3\nEpoch 547, Loss: 0.0038750641979277134\nValidation Loss improved to 0.013374731875956059 Saving model 3\nEpoch 548, Loss: 0.0038119645323604345\nValidation Loss improved to 0.013373793102800846 Saving model 3\nEpoch 549, Loss: 0.006118815392255783\nValidation Loss improved to 0.013366620987653732 Saving model 3\nEpoch 550, Loss: 0.004762696102261543\nValidation Loss improved to 0.013350505381822586 Saving model 3\nEpoch 551, Loss: 0.003679838962852955\nValidation Loss improved to 0.01333922054618597 Saving model 3\nEpoch 552, Loss: 0.004189697094261646\nValidation Loss improved to 0.013323519378900528 Saving model 3\nEpoch 553, Loss: 0.00388046121224761\nValidation Loss improved to 0.013315289281308651 Saving model 3\nEpoch 554, Loss: 0.002438064431771636\nValidation Loss improved to 0.013303404673933983 Saving model 3\nEpoch 555, Loss: 0.0031302357092499733\nValidation Loss improved to 0.013293696567416191 Saving model 3\nEpoch 556, Loss: 0.0042543853633105755\nValidation Loss improved to 0.013281974010169506 Saving model 3\nEpoch 557, Loss: 0.005029000341892242\nValidation Loss improved to 0.01327628642320633 Saving model 3\nEpoch 558, Loss: 0.0032638567499816418\nValidation Loss improved to 0.01326238177716732 Saving model 3\nEpoch 559, Loss: 0.003660422982648015\nValidation Loss improved to 0.013251962140202522 Saving model 3\nEpoch 560, Loss: 0.004080977290868759\nValidation Loss improved to 0.013244572095572948 Saving model 3\nEpoch 561, Loss: 0.004150872118771076\nValidation Loss improved to 0.01323364581912756 Saving model 3\nEpoch 562, Loss: 0.0050210533663630486\nValidation Loss improved to 0.013222595676779747 Saving model 3\nEpoch 563, Loss: 0.008945758454501629\nEpoch 564, Loss: 0.005656736437231302\nValidation Loss improved to 0.013219389133155346 Saving model 3\nEpoch 565, Loss: 0.00402086041867733\nValidation Loss improved to 0.013218745589256287 Saving model 3\nEpoch 566, Loss: 0.003185367677360773\nValidation Loss improved to 0.013208615593612194 Saving model 3\nEpoch 567, Loss: 0.0025192569009959698\nValidation Loss improved to 0.013197953812777996 Saving model 3\nEpoch 568, Loss: 0.0034750790800899267\nValidation Loss improved to 0.013186349533498287 Saving model 3\nEpoch 569, Loss: 0.003603750839829445\nValidation Loss improved to 0.013181318528950214 Saving model 3\nEpoch 570, Loss: 0.0037220942322164774\nValidation Loss improved to 0.013173656538128853 Saving model 3\nEpoch 571, Loss: 0.0039031861815601587\nValidation Loss improved to 0.01317177526652813 Saving model 3\nEpoch 572, Loss: 0.0031011770479381084\nValidation Loss improved to 0.013167166151106358 Saving model 3\nEpoch 573, Loss: 0.0040577482432127\nValidation Loss improved to 0.01316263061016798 Saving model 3\nEpoch 574, Loss: 0.002529410645365715\nValidation Loss improved to 0.013156109489500523 Saving model 3\nEpoch 575, Loss: 0.002353929216042161\nValidation Loss improved to 0.013149159029126167 Saving model 3\nEpoch 576, Loss: 0.002625667955726385\nValidation Loss improved to 0.013137221336364746 Saving model 3\nEpoch 577, Loss: 0.004563086200505495\nValidation Loss improved to 0.013130627572536469 Saving model 3\nEpoch 578, Loss: 0.0032539188396185637\nValidation Loss improved to 0.013125428929924965 Saving model 3\nEpoch 579, Loss: 0.004460618831217289\nValidation Loss improved to 0.0131198950111866 Saving model 3\nEpoch 580, Loss: 0.007377611473202705\nEpoch 581, Loss: 0.004527513403445482\nValidation Loss improved to 0.013108016923069954 Saving model 3\nEpoch 582, Loss: 0.003958896268159151\nValidation Loss improved to 0.01310579665005207 Saving model 3\nEpoch 583, Loss: 0.004295424558222294\nValidation Loss improved to 0.013105439953505993 Saving model 3\nEpoch 584, Loss: 0.005949110258370638\nValidation Loss improved to 0.01310085877776146 Saving model 3\nEpoch 585, Loss: 0.0038283562753349543\nValidation Loss improved to 0.01309484988451004 Saving model 3\nEpoch 586, Loss: 0.005389025434851646\nValidation Loss improved to 0.013090993277728558 Saving model 3\nEpoch 587, Loss: 0.003643851727247238\nValidation Loss improved to 0.013083817437291145 Saving model 3\nEpoch 588, Loss: 0.0061032650992274284\nValidation Loss improved to 0.013076232746243477 Saving model 3\nEpoch 589, Loss: 0.003203778760507703\nEpoch 590, Loss: 0.0045531862415373325\nValidation Loss improved to 0.013062631711363792 Saving model 3\nEpoch 591, Loss: 0.003928660415112972\nValidation Loss improved to 0.013054260052740574 Saving model 3\nEpoch 592, Loss: 0.00439043901860714\nValidation Loss improved to 0.013049330562353134 Saving model 3\nEpoch 593, Loss: 0.005586395040154457\nValidation Loss improved to 0.013044459745287895 Saving model 3\nEpoch 594, Loss: 0.005514717195183039\nValidation Loss improved to 0.013034741394221783 Saving model 3\nEpoch 595, Loss: 0.00650996807962656\nValidation Loss improved to 0.013026603497564793 Saving model 3\nEpoch 596, Loss: 0.004267843905836344\nValidation Loss improved to 0.01301780715584755 Saving model 3\nEpoch 597, Loss: 0.00253098551183939\nValidation Loss improved to 0.013010005466639996 Saving model 3\nEpoch 598, Loss: 0.0033849929459393024\nValidation Loss improved to 0.012999147176742554 Saving model 3\nEpoch 599, Loss: 0.0027977859135717154\nValidation Loss improved to 0.012994690798223019 Saving model 3\nEpoch 600, Loss: 0.0032870350405573845\nValidation Loss improved to 0.012988335452973843 Saving model 3\nEpoch 601, Loss: 0.003922146279364824\nValidation Loss improved to 0.012975838035345078 Saving model 3\nEpoch 602, Loss: 0.003355665598064661\nValidation Loss improved to 0.012966386042535305 Saving model 3\nEpoch 603, Loss: 0.0033099083229899406\nValidation Loss improved to 0.012954799458384514 Saving model 3\nEpoch 604, Loss: 0.003984834533184767\nValidation Loss improved to 0.012947184965014458 Saving model 3\nEpoch 605, Loss: 0.003155645914375782\nValidation Loss improved to 0.012940485030412674 Saving model 3\nEpoch 606, Loss: 0.0024843462742865086\nValidation Loss improved to 0.0129338214173913 Saving model 3\nEpoch 607, Loss: 0.0028041615150868893\nValidation Loss improved to 0.012924638576805592 Saving model 3\nEpoch 608, Loss: 0.004230346065014601\nValidation Loss improved to 0.012912783771753311 Saving model 3\nEpoch 609, Loss: 0.0036904066801071167\nValidation Loss improved to 0.012902287766337395 Saving model 3\nEpoch 610, Loss: 0.005115160718560219\nValidation Loss improved to 0.012895312160253525 Saving model 3\nEpoch 611, Loss: 0.004968494176864624\nValidation Loss improved to 0.012887977063655853 Saving model 3\nEpoch 612, Loss: 0.0039167809300124645\nValidation Loss improved to 0.012879283167421818 Saving model 3\nEpoch 613, Loss: 0.004149965476244688\nValidation Loss improved to 0.012873727828264236 Saving model 3\nEpoch 614, Loss: 0.0033175204880535603\nValidation Loss improved to 0.01286822184920311 Saving model 3\nEpoch 615, Loss: 0.004514933563768864\nValidation Loss improved to 0.01286404300481081 Saving model 3\nEpoch 616, Loss: 0.004543475341051817\nValidation Loss improved to 0.012854144908487797 Saving model 3\nEpoch 617, Loss: 0.002802872098982334\nValidation Loss improved to 0.012846436351537704 Saving model 3\nEpoch 618, Loss: 0.005778888240456581\nValidation Loss improved to 0.012835250236093998 Saving model 3\nEpoch 619, Loss: 0.004100989084690809\nValidation Loss improved to 0.012825665064156055 Saving model 3\nEpoch 620, Loss: 0.00454814825206995\nValidation Loss improved to 0.012813637964427471 Saving model 3\nEpoch 621, Loss: 0.004232537932693958\nEpoch 622, Loss: 0.0041575063951313496\nValidation Loss improved to 0.012808240018785 Saving model 3\nEpoch 623, Loss: 0.004626280628144741\nValidation Loss improved to 0.012797608971595764 Saving model 3\nEpoch 624, Loss: 0.003493644529953599\nValidation Loss improved to 0.012787326239049435 Saving model 3\nEpoch 625, Loss: 0.005012789741158485\nEpoch 626, Loss: 0.007328832987695932\nEpoch 627, Loss: 0.003492744406685233\nValidation Loss improved to 0.0127836549654603 Saving model 3\nEpoch 628, Loss: 0.006073379423469305\nValidation Loss improved to 0.012777903117239475 Saving model 3\nEpoch 629, Loss: 0.007940495386719704\nValidation Loss improved to 0.012773729860782623 Saving model 3\nEpoch 630, Loss: 0.009271562099456787\nValidation Loss improved to 0.012768201529979706 Saving model 3\nEpoch 631, Loss: 0.003714197315275669\nValidation Loss improved to 0.012758654542267323 Saving model 3\nEpoch 632, Loss: 0.002789796330034733\nValidation Loss improved to 0.012755238451063633 Saving model 3\nEpoch 633, Loss: 0.0038975507486611605\nValidation Loss improved to 0.012743949890136719 Saving model 3\nEpoch 634, Loss: 0.003692969214171171\nValidation Loss improved to 0.012736289761960506 Saving model 3\nEpoch 635, Loss: 0.002883758395910263\nValidation Loss improved to 0.012728962115943432 Saving model 3\nEpoch 636, Loss: 0.003882327117025852\nValidation Loss improved to 0.01272555347532034 Saving model 3\nEpoch 637, Loss: 0.005589728709310293\nValidation Loss improved to 0.012716223485767841 Saving model 3\nEpoch 638, Loss: 0.005037934985011816\nValidation Loss improved to 0.01271393895149231 Saving model 3\nEpoch 639, Loss: 0.0027425370644778013\nValidation Loss improved to 0.012703833170235157 Saving model 3\nEpoch 640, Loss: 0.003356593893840909\nValidation Loss improved to 0.012692684307694435 Saving model 3\nEpoch 641, Loss: 0.003035596339032054\nValidation Loss improved to 0.012681434862315655 Saving model 3\nEpoch 642, Loss: 0.002691164379939437\nValidation Loss improved to 0.012672509998083115 Saving model 3\nEpoch 643, Loss: 0.005186636000871658\nValidation Loss improved to 0.01266495417803526 Saving model 3\nEpoch 644, Loss: 0.003511411137878895\nValidation Loss improved to 0.01265624724328518 Saving model 3\nEpoch 645, Loss: 0.0034353428054600954\nValidation Loss improved to 0.012644615955650806 Saving model 3\nEpoch 646, Loss: 0.003779233433306217\nValidation Loss improved to 0.012634601444005966 Saving model 3\nEpoch 647, Loss: 0.0044852267019450665\nValidation Loss improved to 0.012625513598322868 Saving model 3\nEpoch 648, Loss: 0.005242325831204653\nValidation Loss improved to 0.01261996291577816 Saving model 3\nEpoch 649, Loss: 0.0063809920102357864\nEpoch 650, Loss: 0.008240538649260998\nValidation Loss improved to 0.012616592459380627 Saving model 3\nEpoch 651, Loss: 0.005401557311415672\nValidation Loss improved to 0.01260826550424099 Saving model 3\nEpoch 652, Loss: 0.006242948584258556\nValidation Loss improved to 0.012597819790244102 Saving model 3\nEpoch 653, Loss: 0.003167746588587761\nValidation Loss improved to 0.012586959637701511 Saving model 3\nEpoch 654, Loss: 0.0028977326583117247\nValidation Loss improved to 0.0125757260248065 Saving model 3\nEpoch 655, Loss: 0.00420377217233181\nValidation Loss improved to 0.012569333426654339 Saving model 3\nEpoch 656, Loss: 0.003283851779997349\nValidation Loss improved to 0.012557793408632278 Saving model 3\nEpoch 657, Loss: 0.0035571767948567867\nValidation Loss improved to 0.012548158876597881 Saving model 3\nEpoch 658, Loss: 0.0039823418483138084\nEpoch 659, Loss: 0.004603676497936249\nValidation Loss improved to 0.012547890655696392 Saving model 3\nEpoch 660, Loss: 0.00471033900976181\nValidation Loss improved to 0.012539960443973541 Saving model 3\nEpoch 661, Loss: 0.0032926159910857677\nValidation Loss improved to 0.012531545013189316 Saving model 3\nEpoch 662, Loss: 0.005411786027252674\nValidation Loss improved to 0.012522128410637379 Saving model 3\nEpoch 663, Loss: 0.005099338013678789\nValidation Loss improved to 0.012521651573479176 Saving model 3\nEpoch 664, Loss: 0.0044340211898088455\nValidation Loss improved to 0.012512093409895897 Saving model 3\nEpoch 665, Loss: 0.003947732038795948\nValidation Loss improved to 0.012504502199590206 Saving model 3\nEpoch 666, Loss: 0.004504453856498003\nValidation Loss improved to 0.0125009436160326 Saving model 3\nEpoch 667, Loss: 0.004510350059717894\nValidation Loss improved to 0.01249690167605877 Saving model 3\nEpoch 668, Loss: 0.0048309206031262875\nValidation Loss improved to 0.012488733977079391 Saving model 3\nEpoch 669, Loss: 0.004498494789004326\nValidation Loss improved to 0.012485227547585964 Saving model 3\nEpoch 670, Loss: 0.0035339565947651863\nValidation Loss improved to 0.012479356490075588 Saving model 3\nEpoch 671, Loss: 0.0021492368541657925\nValidation Loss improved to 0.012472398579120636 Saving model 3\nEpoch 672, Loss: 0.004011992830783129\nValidation Loss improved to 0.012469204142689705 Saving model 3\nEpoch 673, Loss: 0.0036084000021219254\nValidation Loss improved to 0.012461952865123749 Saving model 3\nEpoch 674, Loss: 0.003698789281770587\nValidation Loss improved to 0.012452331371605396 Saving model 3\nEpoch 675, Loss: 0.0035292382817715406\nValidation Loss improved to 0.012451499700546265 Saving model 3\nEpoch 676, Loss: 0.004189500119537115\nValidation Loss improved to 0.01244419813156128 Saving model 3\nEpoch 677, Loss: 0.0028837001882493496\nValidation Loss improved to 0.012441492639482021 Saving model 3\nEpoch 678, Loss: 0.0038821601774543524\nValidation Loss improved to 0.012432283721864223 Saving model 3\nEpoch 679, Loss: 0.0038449743296951056\nValidation Loss improved to 0.012423478066921234 Saving model 3\nEpoch 680, Loss: 0.003822196973487735\nValidation Loss improved to 0.012419975362718105 Saving model 3\nEpoch 681, Loss: 0.0029106386937201023\nValidation Loss improved to 0.012413124553859234 Saving model 3\nEpoch 682, Loss: 0.002994988113641739\nValidation Loss improved to 0.012404771521687508 Saving model 3\nEpoch 683, Loss: 0.002552033867686987\nValidation Loss improved to 0.01239379495382309 Saving model 3\nEpoch 684, Loss: 0.002709790365770459\nValidation Loss improved to 0.012385094538331032 Saving model 3\nEpoch 685, Loss: 0.003735912498086691\nValidation Loss improved to 0.012378980405628681 Saving model 3\nEpoch 686, Loss: 0.003645884105935693\nValidation Loss improved to 0.012368576601147652 Saving model 3\nEpoch 687, Loss: 0.00400944147258997\nValidation Loss improved to 0.012361101806163788 Saving model 3\nEpoch 688, Loss: 0.0033204208593815565\nValidation Loss improved to 0.012358486652374268 Saving model 3\nEpoch 689, Loss: 0.0037277115043252707\nValidation Loss improved to 0.012349842116236687 Saving model 3\nEpoch 690, Loss: 0.004046264104545116\nValidation Loss improved to 0.01234500017017126 Saving model 3\nEpoch 691, Loss: 0.0027651959098875523\nValidation Loss improved to 0.012338326312601566 Saving model 3\nEpoch 692, Loss: 0.003223767504096031\nValidation Loss improved to 0.01232957374304533 Saving model 3\nEpoch 693, Loss: 0.003861964214593172\nValidation Loss improved to 0.012319187633693218 Saving model 3\nEpoch 694, Loss: 0.0037649781443178654\nValidation Loss improved to 0.012311600148677826 Saving model 3\nEpoch 695, Loss: 0.002896810183301568\nValidation Loss improved to 0.01230178214609623 Saving model 3\nEpoch 696, Loss: 0.004337128251791\nValidation Loss improved to 0.012294236570596695 Saving model 3\nEpoch 697, Loss: 0.0030585178174078465\nValidation Loss improved to 0.012288069352507591 Saving model 3\nEpoch 698, Loss: 0.002542973728850484\nValidation Loss improved to 0.012278599664568901 Saving model 3\nEpoch 699, Loss: 0.004647805821150541\nEpoch 700, Loss: 0.004577714949846268\nValidation Loss improved to 0.012272627092897892 Saving model 3\nEpoch 701, Loss: 0.0037365665193647146\nValidation Loss improved to 0.01226356066763401 Saving model 3\nEpoch 702, Loss: 0.006324419751763344\nValidation Loss improved to 0.012261519208550453 Saving model 3\nEpoch 703, Loss: 0.005950822029262781\nValidation Loss improved to 0.012258549220860004 Saving model 3\nEpoch 704, Loss: 0.005768639966845512\nValidation Loss improved to 0.012249555438756943 Saving model 3\nEpoch 705, Loss: 0.005239611025899649\nValidation Loss improved to 0.012249400839209557 Saving model 3\nEpoch 706, Loss: 0.004204577300697565\nValidation Loss improved to 0.012246964499354362 Saving model 3\nEpoch 707, Loss: 0.004204398021101952\nValidation Loss improved to 0.012240721844136715 Saving model 3\nEpoch 708, Loss: 0.0038330419920384884\nValidation Loss improved to 0.012235967442393303 Saving model 3\nEpoch 709, Loss: 0.00307065318338573\nValidation Loss improved to 0.012227145954966545 Saving model 3\nEpoch 710, Loss: 0.0040432787500321865\nValidation Loss improved to 0.012220035307109356 Saving model 3\nEpoch 711, Loss: 0.005503346677869558\nValidation Loss improved to 0.012216612696647644 Saving model 3\nEpoch 712, Loss: 0.0030757514759898186\nValidation Loss improved to 0.012209882959723473 Saving model 3\nEpoch 713, Loss: 0.0026222083251923323\nValidation Loss improved to 0.012202520854771137 Saving model 3\nEpoch 714, Loss: 0.0034965749364346266\nValidation Loss improved to 0.012192848138511181 Saving model 3\nEpoch 715, Loss: 0.004858003929257393\nValidation Loss improved to 0.012192047201097012 Saving model 3\nEpoch 716, Loss: 0.00613141804933548\nValidation Loss improved to 0.012189136818051338 Saving model 3\nEpoch 717, Loss: 0.0051976153627038\nValidation Loss improved to 0.012181383557617664 Saving model 3\nEpoch 718, Loss: 0.004807015880942345\nValidation Loss improved to 0.012172563001513481 Saving model 3\nEpoch 719, Loss: 0.004617712926119566\nValidation Loss improved to 0.012170473113656044 Saving model 3\nEpoch 720, Loss: 0.006503716576844454\nValidation Loss improved to 0.012168541550636292 Saving model 3\nEpoch 721, Loss: 0.007826077751815319\nValidation Loss improved to 0.012158441357314587 Saving model 3\nEpoch 722, Loss: 0.0038480928633362055\nValidation Loss improved to 0.01215487439185381 Saving model 3\nEpoch 723, Loss: 0.002811164129525423\nValidation Loss improved to 0.012147552333772182 Saving model 3\nEpoch 724, Loss: 0.002440021373331547\nValidation Loss improved to 0.012139080092310905 Saving model 3\nEpoch 725, Loss: 0.004314441233873367\nValidation Loss improved to 0.012129324488341808 Saving model 3\nEpoch 726, Loss: 0.003098416142165661\nValidation Loss improved to 0.012122252956032753 Saving model 3\nEpoch 727, Loss: 0.004513836931437254\nValidation Loss improved to 0.012113874778151512 Saving model 3\nEpoch 728, Loss: 0.0027446916792541742\nValidation Loss improved to 0.01210834551602602 Saving model 3\nEpoch 729, Loss: 0.00584065867587924\nValidation Loss improved to 0.012106000445783138 Saving model 3\nEpoch 730, Loss: 0.006835300009697676\nEpoch 731, Loss: 0.004848209675401449\nValidation Loss improved to 0.012098691426217556 Saving model 3\nEpoch 732, Loss: 0.003419981338083744\nValidation Loss improved to 0.01209230162203312 Saving model 3\nEpoch 733, Loss: 0.006030602380633354\nValidation Loss improved to 0.012087996117770672 Saving model 3\nEpoch 734, Loss: 0.0039926692843437195\nValidation Loss improved to 0.012079020030796528 Saving model 3\nEpoch 735, Loss: 0.003309501800686121\nValidation Loss improved to 0.012073126621544361 Saving model 3\nEpoch 736, Loss: 0.0042021567933261395\nValidation Loss improved to 0.012066221795976162 Saving model 3\nEpoch 737, Loss: 0.005186950787901878\nValidation Loss improved to 0.012062497437000275 Saving model 3\nEpoch 738, Loss: 0.005871672183275223\nValidation Loss improved to 0.012054042890667915 Saving model 3\nEpoch 739, Loss: 0.004921652842313051\nValidation Loss improved to 0.012047854252159595 Saving model 3\nEpoch 740, Loss: 0.0068238587118685246\nValidation Loss improved to 0.012040305882692337 Saving model 3\nEpoch 741, Loss: 0.004400282632559538\nValidation Loss improved to 0.012037154287099838 Saving model 3\nEpoch 742, Loss: 0.0036353098694235086\nValidation Loss improved to 0.012033192440867424 Saving model 3\nEpoch 743, Loss: 0.0029868276324123144\nValidation Loss improved to 0.01202427875250578 Saving model 3\nEpoch 744, Loss: 0.005250575952231884\nValidation Loss improved to 0.012022423557937145 Saving model 3\nEpoch 745, Loss: 0.006258777342736721\nValidation Loss improved to 0.012020530179142952 Saving model 3\nEpoch 746, Loss: 0.004020188935101032\nValidation Loss improved to 0.012012704275548458 Saving model 3\nEpoch 747, Loss: 0.0033664351794868708\nValidation Loss improved to 0.012004253454506397 Saving model 3\nEpoch 748, Loss: 0.003973118029534817\nValidation Loss improved to 0.011996234767138958 Saving model 3\nEpoch 749, Loss: 0.0033188415691256523\nValidation Loss improved to 0.011988426558673382 Saving model 3\nEpoch 750, Loss: 0.002872386947274208\nValidation Loss improved to 0.011984926648437977 Saving model 3\nEpoch 751, Loss: 0.003993229474872351\nValidation Loss improved to 0.01197910588234663 Saving model 3\nEpoch 752, Loss: 0.004029470030218363\nValidation Loss improved to 0.01196896843612194 Saving model 3\nEpoch 753, Loss: 0.0028033899143338203\nValidation Loss improved to 0.011959094554185867 Saving model 3\nEpoch 754, Loss: 0.0027878726832568645\nValidation Loss improved to 0.01195134874433279 Saving model 3\nEpoch 755, Loss: 0.007157403510063887\nValidation Loss improved to 0.011944117955863476 Saving model 3\nEpoch 756, Loss: 0.007281099446117878\nValidation Loss improved to 0.011940126307308674 Saving model 3\nEpoch 757, Loss: 0.00641245162114501\nValidation Loss improved to 0.011934994719922543 Saving model 3\nEpoch 758, Loss: 0.003326121950522065\nValidation Loss improved to 0.011931097134947777 Saving model 3\nEpoch 759, Loss: 0.0028266021981835365\nValidation Loss improved to 0.011922575533390045 Saving model 3\nEpoch 760, Loss: 0.002974837552756071\nValidation Loss improved to 0.011919546872377396 Saving model 3\nEpoch 761, Loss: 0.0025954912416636944\nValidation Loss improved to 0.011915802955627441 Saving model 3\nEpoch 762, Loss: 0.0027061044238507748\nValidation Loss improved to 0.011910177767276764 Saving model 3\nEpoch 763, Loss: 0.0024724502582103014\nValidation Loss improved to 0.011904025450348854 Saving model 3\nEpoch 764, Loss: 0.002872965531423688\nValidation Loss improved to 0.011902824975550175 Saving model 3\nEpoch 765, Loss: 0.0038579076062887907\nValidation Loss improved to 0.01189520675688982 Saving model 3\nEpoch 766, Loss: 0.004165790509432554\nValidation Loss improved to 0.011887338943779469 Saving model 3\nEpoch 767, Loss: 0.0024317516945302486\nValidation Loss improved to 0.011884719133377075 Saving model 3\nEpoch 768, Loss: 0.004054853692650795\nValidation Loss improved to 0.011879081837832928 Saving model 3\nEpoch 769, Loss: 0.0018709887517616153\nValidation Loss improved to 0.011870558373630047 Saving model 3\nEpoch 770, Loss: 0.0033959101419895887\nValidation Loss improved to 0.01185972336679697 Saving model 3\nEpoch 771, Loss: 0.005347173660993576\nValidation Loss improved to 0.011851271614432335 Saving model 3\nEpoch 772, Loss: 0.003148171817883849\nValidation Loss improved to 0.011844350025057793 Saving model 3\nEpoch 773, Loss: 0.0054444605484604836\nValidation Loss improved to 0.011835751123726368 Saving model 3\nEpoch 774, Loss: 0.0032859439961612225\nValidation Loss improved to 0.011829761788249016 Saving model 3\nEpoch 775, Loss: 0.003037832910194993\nValidation Loss improved to 0.011821679770946503 Saving model 3\nEpoch 776, Loss: 0.0038379281759262085\nEpoch 777, Loss: 0.004208878614008427\nValidation Loss improved to 0.011813175864517689 Saving model 3\nEpoch 778, Loss: 0.006642521824687719\nValidation Loss improved to 0.011803662404417992 Saving model 3\nEpoch 779, Loss: 0.0028021419420838356\nValidation Loss improved to 0.011798162013292313 Saving model 3\nEpoch 780, Loss: 0.004771007224917412\nValidation Loss improved to 0.01179334707558155 Saving model 3\nEpoch 781, Loss: 0.006305890157818794\nValidation Loss improved to 0.011790224350988865 Saving model 3\nEpoch 782, Loss: 0.006257845088839531\nEpoch 783, Loss: 0.006696332711726427\nValidation Loss improved to 0.011788750067353249 Saving model 3\nEpoch 784, Loss: 0.004300808999687433\nEpoch 785, Loss: 0.005347621161490679\nEpoch 786, Loss: 0.005105056334286928\nEpoch 787, Loss: 0.003783912630751729\nValidation Loss improved to 0.011782780289649963 Saving model 3\nEpoch 788, Loss: 0.003136817831546068\nValidation Loss improved to 0.011778611689805984 Saving model 3\nEpoch 789, Loss: 0.0025871952529996634\nValidation Loss improved to 0.011777537874877453 Saving model 3\nEpoch 790, Loss: 0.0031520449556410313\nValidation Loss improved to 0.011770498938858509 Saving model 3\nEpoch 791, Loss: 0.004282029811292887\nValidation Loss improved to 0.011763992719352245 Saving model 3\nEpoch 792, Loss: 0.004144336562603712\nValidation Loss improved to 0.01175825297832489 Saving model 3\nEpoch 793, Loss: 0.0050907800905406475\nValidation Loss improved to 0.011757592670619488 Saving model 3\nEpoch 794, Loss: 0.004822884686291218\nValidation Loss improved to 0.011754300445318222 Saving model 3\nEpoch 795, Loss: 0.0025020951870828867\nValidation Loss improved to 0.011749348603188992 Saving model 3\nEpoch 796, Loss: 0.002761755371466279\nValidation Loss improved to 0.011741667054593563 Saving model 3\nEpoch 797, Loss: 0.002915398683398962\nValidation Loss improved to 0.01173439435660839 Saving model 3\nEpoch 798, Loss: 0.0031852542888373137\nValidation Loss improved to 0.01172569952905178 Saving model 3\nEpoch 799, Loss: 0.004183506593108177\nValidation Loss improved to 0.011716998182237148 Saving model 3\nEpoch 800, Loss: 0.004957208409905434\nValidation Loss improved to 0.011710993945598602 Saving model 3\nEpoch 801, Loss: 0.004027045331895351\nValidation Loss improved to 0.011704874224960804 Saving model 3\nEpoch 802, Loss: 0.002050688024610281\nValidation Loss improved to 0.011699499562382698 Saving model 3\nEpoch 803, Loss: 0.0021073301322758198\nValidation Loss improved to 0.011690199375152588 Saving model 3\nEpoch 804, Loss: 0.002865387359634042\nValidation Loss improved to 0.011682083830237389 Saving model 3\nEpoch 805, Loss: 0.00329530518501997\nValidation Loss improved to 0.011675928719341755 Saving model 3\nEpoch 806, Loss: 0.0025499514304101467\nValidation Loss improved to 0.011674654670059681 Saving model 3\nEpoch 807, Loss: 0.0053488705307245255\nValidation Loss improved to 0.011668384075164795 Saving model 3\nEpoch 808, Loss: 0.004269272089004517\nValidation Loss improved to 0.01166079193353653 Saving model 3\nEpoch 809, Loss: 0.003958924673497677\nEpoch 810, Loss: 0.007136097177863121\nEpoch 811, Loss: 0.00413521146401763\nEpoch 812, Loss: 0.004684798419475555\nValidation Loss improved to 0.01165345311164856 Saving model 3\nEpoch 813, Loss: 0.004032591823488474\nValidation Loss improved to 0.01165063213557005 Saving model 3\nEpoch 814, Loss: 0.002394347917288542\nValidation Loss improved to 0.011644864454865456 Saving model 3\nEpoch 815, Loss: 0.0022331131622195244\nValidation Loss improved to 0.011641879566013813 Saving model 3\nEpoch 816, Loss: 0.003486619796603918\nValidation Loss improved to 0.011637812480330467 Saving model 3\nEpoch 817, Loss: 0.003348701633512974\nValidation Loss improved to 0.011635063216090202 Saving model 3\nEpoch 818, Loss: 0.003951918799430132\nValidation Loss improved to 0.011629527434706688 Saving model 3\nEpoch 819, Loss: 0.004946541972458363\nValidation Loss improved to 0.011624865233898163 Saving model 3\nEpoch 820, Loss: 0.007699314504861832\nValidation Loss improved to 0.01161964051425457 Saving model 3\nEpoch 821, Loss: 0.004950080066919327\nValidation Loss improved to 0.011615860275924206 Saving model 3\nEpoch 822, Loss: 0.0031516870949417353\nValidation Loss improved to 0.011608557775616646 Saving model 3\nEpoch 823, Loss: 0.005116695538163185\nValidation Loss improved to 0.011606208048760891 Saving model 3\nEpoch 824, Loss: 0.0037204057443886995\nValidation Loss improved to 0.011599558405578136 Saving model 3\nEpoch 825, Loss: 0.002689442364498973\nValidation Loss improved to 0.011591455899178982 Saving model 3\nEpoch 826, Loss: 0.003929356578737497\nValidation Loss improved to 0.011582228355109692 Saving model 3\nEpoch 827, Loss: 0.002802034839987755\nValidation Loss improved to 0.011575301177799702 Saving model 3\nEpoch 828, Loss: 0.0026226709596812725\nValidation Loss improved to 0.011567705310881138 Saving model 3\nEpoch 829, Loss: 0.006007520481944084\nEpoch 830, Loss: 0.005935348104685545\nEpoch 831, Loss: 0.005065192934125662\nValidation Loss improved to 0.011561962775886059 Saving model 3\nEpoch 832, Loss: 0.004457959905266762\nEpoch 833, Loss: 0.006058165337890387\nEpoch 834, Loss: 0.008243945427238941\nEpoch 835, Loss: 0.0051771411672234535\nValidation Loss improved to 0.011559790000319481 Saving model 3\nEpoch 836, Loss: 0.0037519335746765137\nValidation Loss improved to 0.011557312682271004 Saving model 3\nEpoch 837, Loss: 0.0031079116743057966\nValidation Loss improved to 0.011552475392818451 Saving model 3\nEpoch 838, Loss: 0.003101471345871687\nValidation Loss improved to 0.011544501408934593 Saving model 3\nEpoch 839, Loss: 0.00261267414316535\nValidation Loss improved to 0.011541556566953659 Saving model 3\nEpoch 840, Loss: 0.004182497505098581\nValidation Loss improved to 0.011538338847458363 Saving model 3\nEpoch 841, Loss: 0.0045951553620398045\nEpoch 842, Loss: 0.0036924597807228565\nValidation Loss improved to 0.011537295766174793 Saving model 3\nEpoch 843, Loss: 0.0026645734906196594\nValidation Loss improved to 0.011532839387655258 Saving model 3\nEpoch 844, Loss: 0.003441928420215845\nValidation Loss improved to 0.011526569724082947 Saving model 3\nEpoch 845, Loss: 0.0033542246092110872\nValidation Loss improved to 0.011518963612616062 Saving model 3\nEpoch 846, Loss: 0.0026547079905867577\nEpoch 847, Loss: 0.006148835178464651\nEpoch 848, Loss: 0.005454694852232933\nEpoch 849, Loss: 0.004607986658811569\nValidation Loss improved to 0.011515151709318161 Saving model 3\nEpoch 850, Loss: 0.0033168308436870575\nValidation Loss improved to 0.011508464813232422 Saving model 3\nEpoch 851, Loss: 0.0028698802925646305\nValidation Loss improved to 0.011501909233629704 Saving model 3\nEpoch 852, Loss: 0.004432617221027613\nValidation Loss improved to 0.011494389735162258 Saving model 3\nEpoch 853, Loss: 0.0036607100628316402\nValidation Loss improved to 0.011488720774650574 Saving model 3\nEpoch 854, Loss: 0.0028412293177098036\nValidation Loss improved to 0.011481418274343014 Saving model 3\nEpoch 855, Loss: 0.0026961062103509903\nValidation Loss improved to 0.01147559192031622 Saving model 3\nEpoch 856, Loss: 0.004651655443012714\nEpoch 857, Loss: 0.006528889294713736\nEpoch 858, Loss: 0.005084429867565632\nEpoch 859, Loss: 0.0031964892987161875\nValidation Loss improved to 0.01147441752254963 Saving model 3\nEpoch 860, Loss: 0.006368840113282204\nValidation Loss improved to 0.011471240781247616 Saving model 3\nEpoch 861, Loss: 0.0028829413931816816\nValidation Loss improved to 0.011464391835033894 Saving model 3\nEpoch 862, Loss: 0.0024635405279695988\nValidation Loss improved to 0.011460081674158573 Saving model 3\nEpoch 863, Loss: 0.003162638284265995\nValidation Loss improved to 0.011454648338258266 Saving model 3\nEpoch 864, Loss: 0.002260087989270687\nValidation Loss improved to 0.0114509342238307 Saving model 3\nEpoch 865, Loss: 0.00448733102530241\nValidation Loss improved to 0.011444075964391232 Saving model 3\nEpoch 866, Loss: 0.0027223818469792604\nValidation Loss improved to 0.01143635157495737 Saving model 3\nEpoch 867, Loss: 0.001866186154074967\nValidation Loss improved to 0.01142891775816679 Saving model 3\nEpoch 868, Loss: 0.0025973855517804623\nValidation Loss improved to 0.01142236776649952 Saving model 3\nEpoch 869, Loss: 0.0024008643813431263\nValidation Loss improved to 0.011415787972509861 Saving model 3\nEpoch 870, Loss: 0.0020125063601881266\nValidation Loss improved to 0.011409188620746136 Saving model 3\nEpoch 871, Loss: 0.002752584870904684\nValidation Loss improved to 0.011401641182601452 Saving model 3\nEpoch 872, Loss: 0.0038630440831184387\nValidation Loss improved to 0.01140005886554718 Saving model 3\nEpoch 873, Loss: 0.006580167915672064\nValidation Loss improved to 0.011395548470318317 Saving model 3\nEpoch 874, Loss: 0.0071280235424637794\nValidation Loss improved to 0.01139147486537695 Saving model 3\nEpoch 875, Loss: 0.005398152861744165\nValidation Loss improved to 0.01138682384043932 Saving model 3\nEpoch 876, Loss: 0.004171700216829777\nValidation Loss improved to 0.01138109341263771 Saving model 3\nEpoch 877, Loss: 0.0049237580969929695\nValidation Loss improved to 0.011376546695828438 Saving model 3\nEpoch 878, Loss: 0.0037148641422390938\nValidation Loss improved to 0.011369890533387661 Saving model 3\nEpoch 879, Loss: 0.0029491567984223366\nValidation Loss improved to 0.011365347541868687 Saving model 3\nEpoch 880, Loss: 0.004615635611116886\nValidation Loss improved to 0.011359261348843575 Saving model 3\nEpoch 881, Loss: 0.002262356225401163\nValidation Loss improved to 0.011354709044098854 Saving model 3\nEpoch 882, Loss: 0.0024502137675881386\nValidation Loss improved to 0.011348284780979156 Saving model 3\nEpoch 883, Loss: 0.005240993574261665\nValidation Loss improved to 0.01134472619742155 Saving model 3\nEpoch 884, Loss: 0.007759945001453161\nEpoch 885, Loss: 0.004903761204332113\nValidation Loss improved to 0.01134307961910963 Saving model 3\nEpoch 886, Loss: 0.004679858684539795\nValidation Loss improved to 0.011338642798364162 Saving model 3\nEpoch 887, Loss: 0.004133404698222876\nValidation Loss improved to 0.011334632523357868 Saving model 3\nEpoch 888, Loss: 0.0022179994266480207\nValidation Loss improved to 0.011328899301588535 Saving model 3\nEpoch 889, Loss: 0.0025241088587790728\nValidation Loss improved to 0.01132490485906601 Saving model 3\nEpoch 890, Loss: 0.0023895958438515663\nValidation Loss improved to 0.011320783756673336 Saving model 3\nEpoch 891, Loss: 0.0036716631148010492\nValidation Loss improved to 0.011316530406475067 Saving model 3\nEpoch 892, Loss: 0.005397224333137274\nValidation Loss improved to 0.011315228417515755 Saving model 3\nEpoch 893, Loss: 0.00490916520357132\nValidation Loss improved to 0.011311090551316738 Saving model 3\nEpoch 894, Loss: 0.0038196134846657515\nValidation Loss improved to 0.011304972693324089 Saving model 3\nEpoch 895, Loss: 0.003335096873342991\nValidation Loss improved to 0.011299495585262775 Saving model 3\nEpoch 896, Loss: 0.002422505524009466\nValidation Loss improved to 0.01129352580755949 Saving model 3\nEpoch 897, Loss: 0.002954508876428008\nValidation Loss improved to 0.011287067085504532 Saving model 3\nEpoch 898, Loss: 0.007010906469076872\nValidation Loss improved to 0.011281558312475681 Saving model 3\nEpoch 899, Loss: 0.004776573274284601\nValidation Loss improved to 0.011279072612524033 Saving model 3\nEpoch 900, Loss: 0.0049887364730238914\nEpoch 901, Loss: 0.005197548307478428\nValidation Loss improved to 0.011274633929133415 Saving model 3\nEpoch 902, Loss: 0.0029520292300730944\nValidation Loss improved to 0.011270066723227501 Saving model 3\nEpoch 903, Loss: 0.0027401491533964872\nValidation Loss improved to 0.011264498345553875 Saving model 3\nEpoch 904, Loss: 0.002198706613853574\nValidation Loss improved to 0.011259839870035648 Saving model 3\nEpoch 905, Loss: 0.0037258807569742203\nValidation Loss improved to 0.011258767917752266 Saving model 3\nEpoch 906, Loss: 0.003065407043322921\nValidation Loss improved to 0.011250760406255722 Saving model 3\nEpoch 907, Loss: 0.003476999467238784\nValidation Loss improved to 0.011248495429754257 Saving model 3\nEpoch 908, Loss: 0.003375140717253089\nValidation Loss improved to 0.011242245323956013 Saving model 3\nEpoch 909, Loss: 0.0036820608656853437\nValidation Loss improved to 0.011235645040869713 Saving model 3\nEpoch 910, Loss: 0.0027574198320508003\nEpoch 911, Loss: 0.004867770709097385\nValidation Loss improved to 0.011231795884668827 Saving model 3\nEpoch 912, Loss: 0.0048878807574510574\nValidation Loss improved to 0.011227267794311047 Saving model 3\nEpoch 913, Loss: 0.0053492519073188305\nValidation Loss improved to 0.011221811175346375 Saving model 3\nEpoch 914, Loss: 0.005966313183307648\nValidation Loss improved to 0.011215506121516228 Saving model 3\nEpoch 915, Loss: 0.0051276795566082\nValidation Loss improved to 0.01120876707136631 Saving model 3\nEpoch 916, Loss: 0.004502927418798208\nValidation Loss improved to 0.011205189861357212 Saving model 3\nEpoch 917, Loss: 0.0033819866366684437\nValidation Loss improved to 0.011203956790268421 Saving model 3\nEpoch 918, Loss: 0.003183961845934391\nValidation Loss improved to 0.011199072934687138 Saving model 3\nEpoch 919, Loss: 0.00335805956274271\nValidation Loss improved to 0.011191791854798794 Saving model 3\nEpoch 920, Loss: 0.0024571719113737345\nValidation Loss improved to 0.011188020929694176 Saving model 3\nEpoch 921, Loss: 0.004311693366616964\nValidation Loss improved to 0.011183732189238071 Saving model 3\nEpoch 922, Loss: 0.002828077180311084\nValidation Loss improved to 0.01117768045514822 Saving model 3\nEpoch 923, Loss: 0.0029546956066042185\nValidation Loss improved to 0.011172978207468987 Saving model 3\nEpoch 924, Loss: 0.002362418221309781\nValidation Loss improved to 0.011166241019964218 Saving model 3\nEpoch 925, Loss: 0.003998539876192808\nValidation Loss improved to 0.01116031501442194 Saving model 3\nEpoch 926, Loss: 0.0036408293526619673\nValidation Loss improved to 0.011155212298035622 Saving model 3\nEpoch 927, Loss: 0.004373717121779919\nValidation Loss improved to 0.011149155907332897 Saving model 3\nEpoch 928, Loss: 0.0036722237709909678\nValidation Loss improved to 0.01114334911108017 Saving model 3\nEpoch 929, Loss: 0.0026751821860671043\nValidation Loss improved to 0.01113672461360693 Saving model 3\nEpoch 930, Loss: 0.0029400864150375128\nValidation Loss improved to 0.011130093596875668 Saving model 3\nEpoch 931, Loss: 0.0023672792594879866\nValidation Loss improved to 0.011123079806566238 Saving model 3\nEpoch 932, Loss: 0.003083557588979602\nValidation Loss improved to 0.011117188259959221 Saving model 3\nEpoch 933, Loss: 0.003228362649679184\nEpoch 934, Loss: 0.0033177172299474478\nValidation Loss improved to 0.011114750988781452 Saving model 3\nEpoch 935, Loss: 0.002340050647035241\nValidation Loss improved to 0.01111073512583971 Saving model 3\nEpoch 936, Loss: 0.004115828778594732\nValidation Loss improved to 0.011108458042144775 Saving model 3\nEpoch 937, Loss: 0.00541344378143549\nValidation Loss improved to 0.011105915531516075 Saving model 3\nEpoch 938, Loss: 0.0035862657241523266\nValidation Loss improved to 0.01109906006604433 Saving model 3\nEpoch 939, Loss: 0.0038057761266827583\nValidation Loss improved to 0.011094105429947376 Saving model 3\nEpoch 940, Loss: 0.003487576963379979\nValidation Loss improved to 0.01108965277671814 Saving model 3\nEpoch 941, Loss: 0.003941977396607399\nValidation Loss improved to 0.01108459010720253 Saving model 3\nEpoch 942, Loss: 0.0026649360079318285\nValidation Loss improved to 0.011077984236180782 Saving model 3\nEpoch 943, Loss: 0.002204521792009473\nValidation Loss improved to 0.011074023321270943 Saving model 3\nEpoch 944, Loss: 0.0023892668541520834\nValidation Loss improved to 0.011069695465266705 Saving model 3\nEpoch 945, Loss: 0.005020000506192446\nValidation Loss improved to 0.011062045581638813 Saving model 3\nEpoch 946, Loss: 0.003385736607015133\nValidation Loss improved to 0.011055376380681992 Saving model 3\nEpoch 947, Loss: 0.003643388394266367\nValidation Loss improved to 0.011054486967623234 Saving model 3\nEpoch 948, Loss: 0.0034166397526860237\nEpoch 949, Loss: 0.003973590210080147\nValidation Loss improved to 0.011051863431930542 Saving model 3\nEpoch 950, Loss: 0.0030565594788640738\nValidation Loss improved to 0.011044135317206383 Saving model 3\nEpoch 951, Loss: 0.003561764257028699\nValidation Loss improved to 0.011038655415177345 Saving model 3\nEpoch 952, Loss: 0.0036267756950110197\nValidation Loss improved to 0.01103424746543169 Saving model 3\nEpoch 953, Loss: 0.005042904522269964\nValidation Loss improved to 0.011028032749891281 Saving model 3\nEpoch 954, Loss: 0.005153323523700237\nValidation Loss improved to 0.011027161963284016 Saving model 3\nEpoch 955, Loss: 0.004103768616914749\nValidation Loss improved to 0.011020679958164692 Saving model 3\nEpoch 956, Loss: 0.0025622292887419462\nValidation Loss improved to 0.011015488766133785 Saving model 3\nEpoch 957, Loss: 0.002381090074777603\nValidation Loss improved to 0.01100983377546072 Saving model 3\nEpoch 958, Loss: 0.002330765128135681\nValidation Loss improved to 0.011002465151250362 Saving model 3\nEpoch 959, Loss: 0.0034081796184182167\nValidation Loss improved to 0.010996944271028042 Saving model 3\nEpoch 960, Loss: 0.0035945726558566093\nValidation Loss improved to 0.010991804301738739 Saving model 3\nEpoch 961, Loss: 0.004575368948280811\nValidation Loss improved to 0.010986710898578167 Saving model 3\nEpoch 962, Loss: 0.0038289488293230534\nValidation Loss improved to 0.01097959652543068 Saving model 3\nEpoch 963, Loss: 0.0029835666064172983\nValidation Loss improved to 0.010975822806358337 Saving model 3\nEpoch 964, Loss: 0.004308405332267284\nEpoch 965, Loss: 0.00449332082644105\nValidation Loss improved to 0.010970180854201317 Saving model 3\nEpoch 966, Loss: 0.0034362992737442255\nValidation Loss improved to 0.010964050889015198 Saving model 3\nEpoch 967, Loss: 0.003197308862581849\nValidation Loss improved to 0.01095824409276247 Saving model 3\nEpoch 968, Loss: 0.004225315526127815\nValidation Loss improved to 0.0109550841152668 Saving model 3\nEpoch 969, Loss: 0.003973659127950668\nValidation Loss improved to 0.010949010029435158 Saving model 3\nEpoch 970, Loss: 0.003127786796540022\nValidation Loss improved to 0.01094362698495388 Saving model 3\nEpoch 971, Loss: 0.003826125990599394\nValidation Loss improved to 0.010938049294054508 Saving model 3\nEpoch 972, Loss: 0.0031180691439658403\nValidation Loss improved to 0.01093230675905943 Saving model 3\nEpoch 973, Loss: 0.0048159887082874775\nValidation Loss improved to 0.010932235047221184 Saving model 3\nEpoch 974, Loss: 0.0032883761450648308\nValidation Loss improved to 0.010930049233138561 Saving model 3\nEpoch 975, Loss: 0.004734689835458994\nEpoch 976, Loss: 0.008705148473381996\nValidation Loss improved to 0.010929824784398079 Saving model 3\nEpoch 977, Loss: 0.005843997001647949\nValidation Loss improved to 0.010925689712166786 Saving model 3\nEpoch 978, Loss: 0.0038591958582401276\nValidation Loss improved to 0.010923733934760094 Saving model 3\nEpoch 979, Loss: 0.0031859143637120724\nValidation Loss improved to 0.010921096429228783 Saving model 3\nEpoch 980, Loss: 0.004370064940303564\nValidation Loss improved to 0.010917291976511478 Saving model 3\nEpoch 981, Loss: 0.004585825372487307\nValidation Loss improved to 0.010912401601672173 Saving model 3\nEpoch 982, Loss: 0.0047903661616146564\nValidation Loss improved to 0.010906687006354332 Saving model 3\nEpoch 983, Loss: 0.002897528000175953\nValidation Loss improved to 0.010901936329901218 Saving model 3\nEpoch 984, Loss: 0.0023152942303568125\nValidation Loss improved to 0.010895369574427605 Saving model 3\nEpoch 985, Loss: 0.002827714430168271\nValidation Loss improved to 0.010891511105000973 Saving model 3\nEpoch 986, Loss: 0.0021184561774134636\nValidation Loss improved to 0.01088482141494751 Saving model 3\nEpoch 987, Loss: 0.001957333879545331\nValidation Loss improved to 0.010878575034439564 Saving model 3\nEpoch 988, Loss: 0.002624925458803773\nValidation Loss improved to 0.010874304920434952 Saving model 3\nEpoch 989, Loss: 0.004520579241216183\nValidation Loss improved to 0.010870647616684437 Saving model 3\nEpoch 990, Loss: 0.0025698093231767416\nEpoch 991, Loss: 0.003959076479077339\nValidation Loss improved to 0.010866495780646801 Saving model 3\nEpoch 992, Loss: 0.005790536291897297\nValidation Loss improved to 0.01086385827511549 Saving model 3\nEpoch 993, Loss: 0.002952698851004243\nValidation Loss improved to 0.010857505723834038 Saving model 3\nEpoch 994, Loss: 0.002838916378095746\nValidation Loss improved to 0.010851597413420677 Saving model 3\nEpoch 995, Loss: 0.0045676445588469505\nValidation Loss improved to 0.01085004210472107 Saving model 3\nEpoch 996, Loss: 0.004141374956816435\nValidation Loss improved to 0.0108431875705719 Saving model 3\nEpoch 997, Loss: 0.0032375596929341555\nValidation Loss improved to 0.010837346315383911 Saving model 3\nEpoch 998, Loss: 0.005490022711455822\nValidation Loss improved to 0.010833638720214367 Saving model 3\nEpoch 999, Loss: 0.007289257366210222\nValidation Loss improved to 0.010827143676578999 Saving model 3\nEpoch 0, Loss: 0.2513844072818756\nValidation Loss improved to 0.12461718171834946 Saving model 4\nEpoch 1, Loss: 0.11608808487653732\nValidation Loss improved to 0.10352333635091782 Saving model 4\nEpoch 2, Loss: 0.07908538728952408\nValidation Loss improved to 0.08994258940219879 Saving model 4\nEpoch 3, Loss: 0.05853709205985069\nValidation Loss improved to 0.07920892536640167 Saving model 4\nEpoch 4, Loss: 0.036308225244283676\nValidation Loss improved to 0.07005135715007782 Saving model 4\nEpoch 5, Loss: 0.027698975056409836\nValidation Loss improved to 0.06531328707933426 Saving model 4\nEpoch 6, Loss: 0.023432478308677673\nValidation Loss improved to 0.06054430082440376 Saving model 4\nEpoch 7, Loss: 0.018593132495880127\nValidation Loss improved to 0.05633586645126343 Saving model 4\nEpoch 8, Loss: 0.017465811222791672\nValidation Loss improved to 0.05248096585273743 Saving model 4\nEpoch 9, Loss: 0.013418648391962051\nValidation Loss improved to 0.04992020130157471 Saving model 4\nEpoch 10, Loss: 0.012452476657927036\nValidation Loss improved to 0.046835124492645264 Saving model 4\nEpoch 11, Loss: 0.016317060217261314\nValidation Loss improved to 0.045145247131586075 Saving model 4\nEpoch 12, Loss: 0.01750045083463192\nValidation Loss improved to 0.04357806220650673 Saving model 4\nEpoch 13, Loss: 0.013142562471330166\nValidation Loss improved to 0.04220262169837952 Saving model 4\nEpoch 14, Loss: 0.011164330877363682\nValidation Loss improved to 0.04045284911990166 Saving model 4\nEpoch 15, Loss: 0.01718519628047943\nValidation Loss improved to 0.03956812247633934 Saving model 4\nEpoch 16, Loss: 0.014142830856144428\nValidation Loss improved to 0.03866048529744148 Saving model 4\nEpoch 17, Loss: 0.017086192965507507\nValidation Loss improved to 0.03814736753702164 Saving model 4\nEpoch 18, Loss: 0.010573506355285645\nValidation Loss improved to 0.03686390072107315 Saving model 4\nEpoch 19, Loss: 0.011185670271515846\nValidation Loss improved to 0.035777747631073 Saving model 4\nEpoch 20, Loss: 0.015624361112713814\nValidation Loss improved to 0.03462953492999077 Saving model 4\nEpoch 21, Loss: 0.012975119054317474\nValidation Loss improved to 0.03388521820306778 Saving model 4\nEpoch 22, Loss: 0.013825698755681515\nValidation Loss improved to 0.03317176178097725 Saving model 4\nEpoch 23, Loss: 0.012615987099707127\nValidation Loss improved to 0.032360583543777466 Saving model 4\nEpoch 24, Loss: 0.011607255786657333\nValidation Loss improved to 0.031891945749521255 Saving model 4\nEpoch 25, Loss: 0.012680579908192158\nValidation Loss improved to 0.031305957585573196 Saving model 4\nEpoch 26, Loss: 0.012682079337537289\nValidation Loss improved to 0.030634524300694466 Saving model 4\nEpoch 27, Loss: 0.01340724341571331\nValidation Loss improved to 0.030286507681012154 Saving model 4\nEpoch 28, Loss: 0.01078064925968647\nValidation Loss improved to 0.030062351375818253 Saving model 4\nEpoch 29, Loss: 0.01496879756450653\nValidation Loss improved to 0.029349295422434807 Saving model 4\nEpoch 30, Loss: 0.01004267018288374\nEpoch 31, Loss: 0.01830526441335678\nEpoch 32, Loss: 0.009880945086479187\nValidation Loss improved to 0.02914394624531269 Saving model 4\nEpoch 33, Loss: 0.010268441401422024\nValidation Loss improved to 0.028656184673309326 Saving model 4\nEpoch 34, Loss: 0.009840241633355618\nValidation Loss improved to 0.028233690187335014 Saving model 4\nEpoch 35, Loss: 0.008475865237414837\nValidation Loss improved to 0.02775419130921364 Saving model 4\nEpoch 36, Loss: 0.011167694814503193\nValidation Loss improved to 0.027587054297327995 Saving model 4\nEpoch 37, Loss: 0.013050127774477005\nValidation Loss improved to 0.02727399580180645 Saving model 4\nEpoch 38, Loss: 0.011070961132645607\nValidation Loss improved to 0.02686302922666073 Saving model 4\nEpoch 39, Loss: 0.010536647401750088\nValidation Loss improved to 0.026505718007683754 Saving model 4\nEpoch 40, Loss: 0.009144561365246773\nValidation Loss improved to 0.026152146980166435 Saving model 4\nEpoch 41, Loss: 0.00994817540049553\nValidation Loss improved to 0.026033947244286537 Saving model 4\nEpoch 42, Loss: 0.00861191377043724\nValidation Loss improved to 0.02561786212027073 Saving model 4\nEpoch 43, Loss: 0.009430092759430408\nValidation Loss improved to 0.02539392001926899 Saving model 4\nEpoch 44, Loss: 0.007375598885118961\nValidation Loss improved to 0.025129742920398712 Saving model 4\nEpoch 45, Loss: 0.013984174467623234\nEpoch 46, Loss: 0.017881644889712334\nEpoch 47, Loss: 0.014187644235789776\nEpoch 48, Loss: 0.01022293046116829\nEpoch 49, Loss: 0.01022419799119234\nValidation Loss improved to 0.02501467429101467 Saving model 4\nEpoch 50, Loss: 0.00856722891330719\nValidation Loss improved to 0.02485416643321514 Saving model 4\nEpoch 51, Loss: 0.01075923815369606\nValidation Loss improved to 0.02461819536983967 Saving model 4\nEpoch 52, Loss: 0.009442411363124847\nValidation Loss improved to 0.024315599352121353 Saving model 4\nEpoch 53, Loss: 0.007304024416953325\nValidation Loss improved to 0.024167977273464203 Saving model 4\nEpoch 54, Loss: 0.006275704130530357\nValidation Loss improved to 0.02387203276157379 Saving model 4\nEpoch 55, Loss: 0.007164511829614639\nValidation Loss improved to 0.023871442303061485 Saving model 4\nEpoch 56, Loss: 0.01081419549882412\nValidation Loss improved to 0.023715630173683167 Saving model 4\nEpoch 57, Loss: 0.007607984822243452\nValidation Loss improved to 0.02354954369366169 Saving model 4\nEpoch 58, Loss: 0.007302996702492237\nValidation Loss improved to 0.023385055363178253 Saving model 4\nEpoch 59, Loss: 0.008869090117514133\nValidation Loss improved to 0.023125385865569115 Saving model 4\nEpoch 60, Loss: 0.006741393823176622\nEpoch 61, Loss: 0.008955289609730244\nValidation Loss improved to 0.023078516125679016 Saving model 4\nEpoch 62, Loss: 0.00777309387922287\nValidation Loss improved to 0.022913523018360138 Saving model 4\nEpoch 63, Loss: 0.013176401145756245\nValidation Loss improved to 0.022828079760074615 Saving model 4\nEpoch 64, Loss: 0.01130112074315548\nValidation Loss improved to 0.022635595872998238 Saving model 4\nEpoch 65, Loss: 0.006984102074056864\nValidation Loss improved to 0.022488189861178398 Saving model 4\nEpoch 66, Loss: 0.007074556313455105\nValidation Loss improved to 0.022337323054671288 Saving model 4\nEpoch 67, Loss: 0.006235198583453894\nValidation Loss improved to 0.022164063528180122 Saving model 4\nEpoch 68, Loss: 0.005348368547856808\nValidation Loss improved to 0.021987751126289368 Saving model 4\nEpoch 69, Loss: 0.0057240999303758144\nValidation Loss improved to 0.02179908938705921 Saving model 4\nEpoch 70, Loss: 0.00877385213971138\nValidation Loss improved to 0.02167026326060295 Saving model 4\nEpoch 71, Loss: 0.008679380640387535\nValidation Loss improved to 0.02149672619998455 Saving model 4\nEpoch 72, Loss: 0.007675953209400177\nValidation Loss improved to 0.02131526730954647 Saving model 4\nEpoch 73, Loss: 0.011777848936617374\nValidation Loss improved to 0.02118728682398796 Saving model 4\nEpoch 74, Loss: 0.0079582454636693\nValidation Loss improved to 0.0210473220795393 Saving model 4\nEpoch 75, Loss: 0.009620549157261848\nValidation Loss improved to 0.02094200626015663 Saving model 4\nEpoch 76, Loss: 0.009817145764827728\nValidation Loss improved to 0.020831722766160965 Saving model 4\nEpoch 77, Loss: 0.008630996569991112\nValidation Loss improved to 0.02068091556429863 Saving model 4\nEpoch 78, Loss: 0.009291901253163815\nValidation Loss improved to 0.02055523172020912 Saving model 4\nEpoch 79, Loss: 0.009676679037511349\nValidation Loss improved to 0.02051609940826893 Saving model 4\nEpoch 80, Loss: 0.010121270082890987\nValidation Loss improved to 0.020358271896839142 Saving model 4\nEpoch 81, Loss: 0.010572332888841629\nValidation Loss improved to 0.020322322845458984 Saving model 4\nEpoch 82, Loss: 0.008793175220489502\nValidation Loss improved to 0.020195521414279938 Saving model 4\nEpoch 83, Loss: 0.007544435560703278\nEpoch 84, Loss: 0.006474505178630352\nValidation Loss improved to 0.02011856622993946 Saving model 4\nEpoch 85, Loss: 0.007066474296152592\nValidation Loss improved to 0.019959179684519768 Saving model 4\nEpoch 86, Loss: 0.007423419039696455\nValidation Loss improved to 0.019851725548505783 Saving model 4\nEpoch 87, Loss: 0.008561758324503899\nValidation Loss improved to 0.019809098914265633 Saving model 4\nEpoch 88, Loss: 0.008622958324849606\nEpoch 89, Loss: 0.012695856392383575\nValidation Loss improved to 0.01977117545902729 Saving model 4\nEpoch 90, Loss: 0.016593951731920242\nValidation Loss improved to 0.019770577549934387 Saving model 4\nEpoch 91, Loss: 0.00950445793569088\nValidation Loss improved to 0.019638869911432266 Saving model 4\nEpoch 92, Loss: 0.008181417360901833\nValidation Loss improved to 0.019542977213859558 Saving model 4\nEpoch 93, Loss: 0.015006935223937035\nValidation Loss improved to 0.01946188695728779 Saving model 4\nEpoch 94, Loss: 0.008549764752388\nValidation Loss improved to 0.019371479749679565 Saving model 4\nEpoch 95, Loss: 0.0047607868909835815\nValidation Loss improved to 0.019246362149715424 Saving model 4\nEpoch 96, Loss: 0.004461411386728287\nValidation Loss improved to 0.019122876226902008 Saving model 4\nEpoch 97, Loss: 0.005776089150458574\nValidation Loss improved to 0.01905083656311035 Saving model 4\nEpoch 98, Loss: 0.006815905217081308\nValidation Loss improved to 0.018985312432050705 Saving model 4\nEpoch 99, Loss: 0.0070696426555514336\nValidation Loss improved to 0.01890401728451252 Saving model 4\nEpoch 100, Loss: 0.008251466788351536\nEpoch 101, Loss: 0.013790170662105083\nValidation Loss improved to 0.018890975043177605 Saving model 4\nEpoch 102, Loss: 0.011514350771903992\nValidation Loss improved to 0.018873583525419235 Saving model 4\nEpoch 103, Loss: 0.011985343880951405\nValidation Loss improved to 0.018802043050527573 Saving model 4\nEpoch 104, Loss: 0.008447284810245037\nEpoch 105, Loss: 0.00863314513117075\nValidation Loss improved to 0.018783755600452423 Saving model 4\nEpoch 106, Loss: 0.004702035337686539\nValidation Loss improved to 0.01872144639492035 Saving model 4\nEpoch 107, Loss: 0.007643908262252808\nValidation Loss improved to 0.018648918718099594 Saving model 4\nEpoch 108, Loss: 0.010086446069180965\nValidation Loss improved to 0.018579255789518356 Saving model 4\nEpoch 109, Loss: 0.009592050686478615\nValidation Loss improved to 0.018513618037104607 Saving model 4\nEpoch 110, Loss: 0.007492815610021353\nValidation Loss improved to 0.01843727007508278 Saving model 4\nEpoch 111, Loss: 0.004064708948135376\nValidation Loss improved to 0.018319061025977135 Saving model 4\nEpoch 112, Loss: 0.007641250267624855\nValidation Loss improved to 0.018213484436273575 Saving model 4\nEpoch 113, Loss: 0.005397810135036707\nValidation Loss improved to 0.018131688237190247 Saving model 4\nEpoch 114, Loss: 0.003959358669817448\nValidation Loss improved to 0.018054263666272163 Saving model 4\nEpoch 115, Loss: 0.011801964603364468\nValidation Loss improved to 0.018023470416665077 Saving model 4\nEpoch 116, Loss: 0.009675095789134502\nValidation Loss improved to 0.017974557355046272 Saving model 4\nEpoch 117, Loss: 0.008066513575613499\nValidation Loss improved to 0.017881501466035843 Saving model 4\nEpoch 118, Loss: 0.00675229262560606\nValidation Loss improved to 0.017778756096959114 Saving model 4\nEpoch 119, Loss: 0.007361023686826229\nValidation Loss improved to 0.017694834619760513 Saving model 4\nEpoch 120, Loss: 0.005114136263728142\nValidation Loss improved to 0.01760779321193695 Saving model 4\nEpoch 121, Loss: 0.005979097913950682\nValidation Loss improved to 0.017576495185494423 Saving model 4\nEpoch 122, Loss: 0.0052399723790585995\nValidation Loss improved to 0.017499616369605064 Saving model 4\nEpoch 123, Loss: 0.006830284371972084\nValidation Loss improved to 0.01744520291686058 Saving model 4\nEpoch 124, Loss: 0.01007417868822813\nEpoch 125, Loss: 0.0087594510987401\nValidation Loss improved to 0.01737089641392231 Saving model 4\nEpoch 126, Loss: 0.006571116391569376\nValidation Loss improved to 0.017328765243291855 Saving model 4\nEpoch 127, Loss: 0.008640979416668415\nValidation Loss improved to 0.017281271517276764 Saving model 4\nEpoch 128, Loss: 0.0071252151392400265\nValidation Loss improved to 0.017247667536139488 Saving model 4\nEpoch 129, Loss: 0.0049843816086649895\nValidation Loss improved to 0.017169397324323654 Saving model 4\nEpoch 130, Loss: 0.005958663299679756\nValidation Loss improved to 0.017080076038837433 Saving model 4\nEpoch 131, Loss: 0.005616667680442333\nValidation Loss improved to 0.017009470611810684 Saving model 4\nEpoch 132, Loss: 0.005446555092930794\nValidation Loss improved to 0.016965392976999283 Saving model 4\nEpoch 133, Loss: 0.008894660510122776\nValidation Loss improved to 0.016909336671233177 Saving model 4\nEpoch 134, Loss: 0.006946920417249203\nValidation Loss improved to 0.016848819330334663 Saving model 4\nEpoch 135, Loss: 0.009299946017563343\nValidation Loss improved to 0.016774337738752365 Saving model 4\nEpoch 136, Loss: 0.010559633374214172\nValidation Loss improved to 0.016748612746596336 Saving model 4\nEpoch 137, Loss: 0.010782360099256039\nValidation Loss improved to 0.016706427559256554 Saving model 4\nEpoch 138, Loss: 0.011059284210205078\nValidation Loss improved to 0.016694899648427963 Saving model 4\nEpoch 139, Loss: 0.010114699602127075\nValidation Loss improved to 0.016612952575087547 Saving model 4\nEpoch 140, Loss: 0.010604750365018845\nValidation Loss improved to 0.016558121889829636 Saving model 4\nEpoch 141, Loss: 0.011351261287927628\nValidation Loss improved to 0.016536012291908264 Saving model 4\nEpoch 142, Loss: 0.007581961341202259\nValidation Loss improved to 0.016506027430295944 Saving model 4\nEpoch 143, Loss: 0.005863351747393608\nValidation Loss improved to 0.01642722263932228 Saving model 4\nEpoch 144, Loss: 0.005179840140044689\nValidation Loss improved to 0.016375897452235222 Saving model 4\nEpoch 145, Loss: 0.005583208054304123\nValidation Loss improved to 0.016319211572408676 Saving model 4\nEpoch 146, Loss: 0.0048547908663749695\nValidation Loss improved to 0.016238154843449593 Saving model 4\nEpoch 147, Loss: 0.004331681877374649\nEpoch 148, Loss: 0.0067066652700304985\nValidation Loss improved to 0.016166720539331436 Saving model 4\nEpoch 149, Loss: 0.00890774093568325\nValidation Loss improved to 0.01612982712686062 Saving model 4\nEpoch 150, Loss: 0.008208259008824825\nValidation Loss improved to 0.016101615503430367 Saving model 4\nEpoch 151, Loss: 0.010148047469556332\nValidation Loss improved to 0.01604577898979187 Saving model 4\nEpoch 152, Loss: 0.01579887606203556\nValidation Loss improved to 0.016034148633480072 Saving model 4\nEpoch 153, Loss: 0.008177165873348713\nValidation Loss improved to 0.016013555228710175 Saving model 4\nEpoch 154, Loss: 0.008405927568674088\nValidation Loss improved to 0.015981251373887062 Saving model 4\nEpoch 155, Loss: 0.011180141940712929\nValidation Loss improved to 0.01596432365477085 Saving model 4\nEpoch 156, Loss: 0.005182873923331499\nValidation Loss improved to 0.01591319777071476 Saving model 4\nEpoch 157, Loss: 0.00502672279253602\nValidation Loss improved to 0.01585530862212181 Saving model 4\nEpoch 158, Loss: 0.005100570153445005\nValidation Loss improved to 0.015792235732078552 Saving model 4\nEpoch 159, Loss: 0.004526190925389528\nValidation Loss improved to 0.01575636677443981 Saving model 4\nEpoch 160, Loss: 0.004105009604245424\nValidation Loss improved to 0.015695303678512573 Saving model 4\nEpoch 161, Loss: 0.004559622146189213\nValidation Loss improved to 0.015624809078872204 Saving model 4\nEpoch 162, Loss: 0.005265164654701948\nValidation Loss improved to 0.01558147557079792 Saving model 4\nEpoch 163, Loss: 0.00664057582616806\nValidation Loss improved to 0.015573845244944096 Saving model 4\nEpoch 164, Loss: 0.007761868182569742\nValidation Loss improved to 0.015531335026025772 Saving model 4\nEpoch 165, Loss: 0.008906036615371704\nValidation Loss improved to 0.015499090775847435 Saving model 4\nEpoch 166, Loss: 0.006511859130114317\nValidation Loss improved to 0.015433241613209248 Saving model 4\nEpoch 167, Loss: 0.005220054183155298\nValidation Loss improved to 0.01541096344590187 Saving model 4\nEpoch 168, Loss: 0.006137567106634378\nValidation Loss improved to 0.015389141626656055 Saving model 4\nEpoch 169, Loss: 0.00659585278481245\nValidation Loss improved to 0.015359663404524326 Saving model 4\nEpoch 170, Loss: 0.006826178636401892\nEpoch 171, Loss: 0.007406117394566536\nValidation Loss improved to 0.015355359762907028 Saving model 4\nEpoch 172, Loss: 0.010283307172358036\nValidation Loss improved to 0.015319925732910633 Saving model 4\nEpoch 173, Loss: 0.007204335182905197\nEpoch 174, Loss: 0.00614180089905858\nValidation Loss improved to 0.015269964002072811 Saving model 4\nEpoch 175, Loss: 0.004254550207406282\nValidation Loss improved to 0.015217078849673271 Saving model 4\nEpoch 176, Loss: 0.005659105721861124\nValidation Loss improved to 0.015181852504611015 Saving model 4\nEpoch 177, Loss: 0.004946947563439608\nValidation Loss improved to 0.015141892246901989 Saving model 4\nEpoch 178, Loss: 0.0053790356032550335\nValidation Loss improved to 0.015092934481799603 Saving model 4\nEpoch 179, Loss: 0.005729870870709419\nValidation Loss improved to 0.015045969747006893 Saving model 4\nEpoch 180, Loss: 0.005624105222523212\nValidation Loss improved to 0.014996970072388649 Saving model 4\nEpoch 181, Loss: 0.004059234168380499\nValidation Loss improved to 0.014955539256334305 Saving model 4\nEpoch 182, Loss: 0.004504616837948561\nValidation Loss improved to 0.014924002811312675 Saving model 4\nEpoch 183, Loss: 0.006493058986961842\nEpoch 184, Loss: 0.008793550543487072\nValidation Loss improved to 0.01490334328263998 Saving model 4\nEpoch 185, Loss: 0.008350265212357044\nValidation Loss improved to 0.014863566495478153 Saving model 4\nEpoch 186, Loss: 0.0073979925364255905\nValidation Loss improved to 0.014827238395810127 Saving model 4\nEpoch 187, Loss: 0.004336449783295393\nValidation Loss improved to 0.014779828488826752 Saving model 4\nEpoch 188, Loss: 0.0038120036479085684\nValidation Loss improved to 0.014737321063876152 Saving model 4\nEpoch 189, Loss: 0.0062703839503228664\nValidation Loss improved to 0.014715705066919327 Saving model 4\nEpoch 190, Loss: 0.005397754721343517\nValidation Loss improved to 0.014664074406027794 Saving model 4\nEpoch 191, Loss: 0.006341645494103432\nValidation Loss improved to 0.014623099938035011 Saving model 4\nEpoch 192, Loss: 0.006330259144306183\nValidation Loss improved to 0.014612754806876183 Saving model 4\nEpoch 193, Loss: 0.00570293702185154\nValidation Loss improved to 0.014582453295588493 Saving model 4\nEpoch 194, Loss: 0.004266152624040842\nValidation Loss improved to 0.014538206160068512 Saving model 4\nEpoch 195, Loss: 0.00508104357868433\nValidation Loss improved to 0.014505687169730663 Saving model 4\nEpoch 196, Loss: 0.006017144303768873\nValidation Loss improved to 0.014463191851973534 Saving model 4\nEpoch 197, Loss: 0.005979570560157299\nValidation Loss improved to 0.014434580691158772 Saving model 4\nEpoch 198, Loss: 0.005230261012911797\nValidation Loss improved to 0.014395851641893387 Saving model 4\nEpoch 199, Loss: 0.006779070477932692\nEpoch 200, Loss: 0.009748433716595173\nValidation Loss improved to 0.014361382462084293 Saving model 4\nEpoch 201, Loss: 0.004508501850068569\nValidation Loss improved to 0.014334173873066902 Saving model 4\nEpoch 202, Loss: 0.005269024521112442\nValidation Loss improved to 0.01429441012442112 Saving model 4\nEpoch 203, Loss: 0.006083476822823286\nValidation Loss improved to 0.014279613271355629 Saving model 4\nEpoch 204, Loss: 0.006924291606992483\nValidation Loss improved to 0.014238455332815647 Saving model 4\nEpoch 205, Loss: 0.004801654722541571\nValidation Loss improved to 0.014193223789334297 Saving model 4\nEpoch 206, Loss: 0.005491302348673344\nValidation Loss improved to 0.014162393286824226 Saving model 4\nEpoch 207, Loss: 0.007046053186058998\nValidation Loss improved to 0.014162246137857437 Saving model 4\nEpoch 208, Loss: 0.0058236755430698395\nValidation Loss improved to 0.014140218496322632 Saving model 4\nEpoch 209, Loss: 0.0043881479650735855\nValidation Loss improved to 0.014102745801210403 Saving model 4\nEpoch 210, Loss: 0.0041829668916761875\nValidation Loss improved to 0.0140821048989892 Saving model 4\nEpoch 211, Loss: 0.004435968119651079\nValidation Loss improved to 0.014073747210204601 Saving model 4\nEpoch 212, Loss: 0.004821091424673796\nValidation Loss improved to 0.01406425517052412 Saving model 4\nEpoch 213, Loss: 0.00558199780061841\nValidation Loss improved to 0.014028431847691536 Saving model 4\nEpoch 214, Loss: 0.004590935539454222\nValidation Loss improved to 0.013988005928695202 Saving model 4\nEpoch 215, Loss: 0.0035402318462729454\nValidation Loss improved to 0.013941017910838127 Saving model 4\nEpoch 216, Loss: 0.004677605349570513\nValidation Loss improved to 0.013903161510825157 Saving model 4\nEpoch 217, Loss: 0.0038529508747160435\nValidation Loss improved to 0.01387437991797924 Saving model 4\nEpoch 218, Loss: 0.004114317242056131\nValidation Loss improved to 0.013833041302859783 Saving model 4\nEpoch 219, Loss: 0.004459506832063198\nValidation Loss improved to 0.013795723207294941 Saving model 4\nEpoch 220, Loss: 0.0060393414460122585\nValidation Loss improved to 0.013771329075098038 Saving model 4\nEpoch 221, Loss: 0.009084891527891159\nValidation Loss improved to 0.013757779262959957 Saving model 4\nEpoch 222, Loss: 0.008285989984869957\nValidation Loss improved to 0.01374298706650734 Saving model 4\nEpoch 223, Loss: 0.006483656819909811\nValidation Loss improved to 0.01370656955987215 Saving model 4\nEpoch 224, Loss: 0.005022902507334948\nValidation Loss improved to 0.013677636161446571 Saving model 4\nEpoch 225, Loss: 0.004863223992288113\nValidation Loss improved to 0.013655330054461956 Saving model 4\nEpoch 226, Loss: 0.008478685282170773\nValidation Loss improved to 0.013629707507789135 Saving model 4\nEpoch 227, Loss: 0.005835669580847025\nValidation Loss improved to 0.013602065853774548 Saving model 4\nEpoch 228, Loss: 0.008500813506543636\nValidation Loss improved to 0.013567195273935795 Saving model 4\nEpoch 229, Loss: 0.007559198420494795\nValidation Loss improved to 0.01354439090937376 Saving model 4\nEpoch 230, Loss: 0.005487868562340736\nValidation Loss improved to 0.013508818112313747 Saving model 4\nEpoch 231, Loss: 0.006945305969566107\nValidation Loss improved to 0.013493804261088371 Saving model 4\nEpoch 232, Loss: 0.00839360523968935\nValidation Loss improved to 0.013461136259138584 Saving model 4\nEpoch 233, Loss: 0.004993405658751726\nValidation Loss improved to 0.01342618279159069 Saving model 4\nEpoch 234, Loss: 0.004151406232267618\nValidation Loss improved to 0.013393143191933632 Saving model 4\nEpoch 235, Loss: 0.004479206632822752\nValidation Loss improved to 0.013382871635258198 Saving model 4\nEpoch 236, Loss: 0.005452071316540241\nValidation Loss improved to 0.013363342732191086 Saving model 4\nEpoch 237, Loss: 0.0058243065141141415\nValidation Loss improved to 0.013344640843570232 Saving model 4\nEpoch 238, Loss: 0.005788049194961786\nValidation Loss improved to 0.013320030644536018 Saving model 4\nEpoch 239, Loss: 0.0056427400559186935\nValidation Loss improved to 0.013292019255459309 Saving model 4\nEpoch 240, Loss: 0.006570592988282442\nValidation Loss improved to 0.013266402296721935 Saving model 4\nEpoch 241, Loss: 0.003970479127019644\nValidation Loss improved to 0.013248318806290627 Saving model 4\nEpoch 242, Loss: 0.005102801136672497\nValidation Loss improved to 0.013221535831689835 Saving model 4\nEpoch 243, Loss: 0.006028733681887388\nValidation Loss improved to 0.01320741418749094 Saving model 4\nEpoch 244, Loss: 0.005620857700705528\nValidation Loss improved to 0.013178705237805843 Saving model 4\nEpoch 245, Loss: 0.00724363699555397\nValidation Loss improved to 0.013169054873287678 Saving model 4\nEpoch 246, Loss: 0.006625070236623287\nValidation Loss improved to 0.013154328800737858 Saving model 4\nEpoch 247, Loss: 0.012767400592565536\nEpoch 248, Loss: 0.0082085644826293\nEpoch 249, Loss: 0.005259413737803698\nValidation Loss improved to 0.013141913339495659 Saving model 4\nEpoch 250, Loss: 0.004591706674546003\nValidation Loss improved to 0.013110215775668621 Saving model 4\nEpoch 251, Loss: 0.004788519814610481\nValidation Loss improved to 0.013078059069812298 Saving model 4\nEpoch 252, Loss: 0.0038171284832060337\nValidation Loss improved to 0.01304880902171135 Saving model 4\nEpoch 253, Loss: 0.0052712601609528065\nValidation Loss improved to 0.013034217059612274 Saving model 4\nEpoch 254, Loss: 0.007899600081145763\nValidation Loss improved to 0.013023614883422852 Saving model 4\nEpoch 255, Loss: 0.00491967611014843\nValidation Loss improved to 0.01299560721963644 Saving model 4\nEpoch 256, Loss: 0.003933825995773077\nValidation Loss improved to 0.012975087389349937 Saving model 4\nEpoch 257, Loss: 0.005694732069969177\nEpoch 258, Loss: 0.007444406859576702\nValidation Loss improved to 0.01296369731426239 Saving model 4\nEpoch 259, Loss: 0.006701696198433638\nValidation Loss improved to 0.012955614365637302 Saving model 4\nEpoch 260, Loss: 0.011205405928194523\nValidation Loss improved to 0.01295054517686367 Saving model 4\nEpoch 261, Loss: 0.00629330612719059\nValidation Loss improved to 0.012942596338689327 Saving model 4\nEpoch 262, Loss: 0.005104399751871824\nValidation Loss improved to 0.0129201989620924 Saving model 4\nEpoch 263, Loss: 0.0050240810960531235\nValidation Loss improved to 0.012897172011435032 Saving model 4\nEpoch 264, Loss: 0.0045126439072191715\nValidation Loss improved to 0.01286878902465105 Saving model 4\nEpoch 265, Loss: 0.007543858606368303\nValidation Loss improved to 0.012858633883297443 Saving model 4\nEpoch 266, Loss: 0.006235579494386911\nValidation Loss improved to 0.012841274030506611 Saving model 4\nEpoch 267, Loss: 0.0050254808738827705\nValidation Loss improved to 0.012815705500543118 Saving model 4\nEpoch 268, Loss: 0.004033063538372517\nValidation Loss improved to 0.012804722413420677 Saving model 4\nEpoch 269, Loss: 0.003580244490876794\nValidation Loss improved to 0.012780419550836086 Saving model 4\nEpoch 270, Loss: 0.0028762537986040115\nValidation Loss improved to 0.012758121825754642 Saving model 4\nEpoch 271, Loss: 0.00439773965626955\nValidation Loss improved to 0.012752009555697441 Saving model 4\nEpoch 272, Loss: 0.005266577936708927\nValidation Loss improved to 0.012723295949399471 Saving model 4\nEpoch 273, Loss: 0.005330949556082487\nValidation Loss improved to 0.012719569727778435 Saving model 4\nEpoch 274, Loss: 0.005385853350162506\nValidation Loss improved to 0.012703645043075085 Saving model 4\nEpoch 275, Loss: 0.0045572384260594845\nValidation Loss improved to 0.012683030217885971 Saving model 4\nEpoch 276, Loss: 0.006942417472600937\nValidation Loss improved to 0.012674259021878242 Saving model 4\nEpoch 277, Loss: 0.005827256012707949\nValidation Loss improved to 0.012656694278120995 Saving model 4\nEpoch 278, Loss: 0.003964387346059084\nValidation Loss improved to 0.012630707584321499 Saving model 4\nEpoch 279, Loss: 0.004111673682928085\nValidation Loss improved to 0.012627674266695976 Saving model 4\nEpoch 280, Loss: 0.0043455990962684155\nValidation Loss improved to 0.012598450295627117 Saving model 4\nEpoch 281, Loss: 0.0031951216515153646\nValidation Loss improved to 0.012586796656250954 Saving model 4\nEpoch 282, Loss: 0.0036526566836982965\nValidation Loss improved to 0.012564277276396751 Saving model 4\nEpoch 283, Loss: 0.003180029569193721\nValidation Loss improved to 0.012559100985527039 Saving model 4\nEpoch 284, Loss: 0.003365368116647005\nValidation Loss improved to 0.012538707815110683 Saving model 4\nEpoch 285, Loss: 0.003372221952304244\nValidation Loss improved to 0.012528122402727604 Saving model 4\nEpoch 286, Loss: 0.007719013839960098\nValidation Loss improved to 0.012503408826887608 Saving model 4\nEpoch 287, Loss: 0.00692508602514863\nEpoch 288, Loss: 0.005060713738203049\nValidation Loss improved to 0.012485664337873459 Saving model 4\nEpoch 289, Loss: 0.005388996563851833\nValidation Loss improved to 0.012472555972635746 Saving model 4\nEpoch 290, Loss: 0.0065376292914152145\nValidation Loss improved to 0.012465720064938068 Saving model 4\nEpoch 291, Loss: 0.005883569363504648\nValidation Loss improved to 0.012449860572814941 Saving model 4\nEpoch 292, Loss: 0.0066864704713225365\nValidation Loss improved to 0.012430832721292973 Saving model 4\nEpoch 293, Loss: 0.006568034179508686\nEpoch 294, Loss: 0.0058205039240419865\nValidation Loss improved to 0.012422448955476284 Saving model 4\nEpoch 295, Loss: 0.006244906224310398\nValidation Loss improved to 0.012403987348079681 Saving model 4\nEpoch 296, Loss: 0.00456182612106204\nValidation Loss improved to 0.01238767895847559 Saving model 4\nEpoch 297, Loss: 0.004078536760061979\nValidation Loss improved to 0.012373305857181549 Saving model 4\nEpoch 298, Loss: 0.004486983176320791\nValidation Loss improved to 0.012364914640784264 Saving model 4\nEpoch 299, Loss: 0.006078111473470926\nValidation Loss improved to 0.012343114241957664 Saving model 4\nEpoch 300, Loss: 0.004124743863940239\nEpoch 301, Loss: 0.005836299620568752\nValidation Loss improved to 0.012322979047894478 Saving model 4\nEpoch 302, Loss: 0.005038789007812738\nValidation Loss improved to 0.012301654554903507 Saving model 4\nEpoch 303, Loss: 0.003658585250377655\nValidation Loss improved to 0.012278012931346893 Saving model 4\nEpoch 304, Loss: 0.0060308692045509815\nEpoch 305, Loss: 0.00822698138654232\nEpoch 306, Loss: 0.006173380184918642\nEpoch 307, Loss: 0.00484454445540905\nValidation Loss improved to 0.012267502956092358 Saving model 4\nEpoch 308, Loss: 0.004245301242917776\nValidation Loss improved to 0.012241518124938011 Saving model 4\nEpoch 309, Loss: 0.004751401022076607\nValidation Loss improved to 0.012221034616231918 Saving model 4\nEpoch 310, Loss: 0.006084999069571495\nEpoch 311, Loss: 0.005668464116752148\nValidation Loss improved to 0.012209929525852203 Saving model 4\nEpoch 312, Loss: 0.006227645557373762\nValidation Loss improved to 0.012191461399197578 Saving model 4\nEpoch 313, Loss: 0.0027569569647312164\nValidation Loss improved to 0.01217444334179163 Saving model 4\nEpoch 314, Loss: 0.005877349060028791\nValidation Loss improved to 0.012164470739662647 Saving model 4\nEpoch 315, Loss: 0.005887523293495178\nValidation Loss improved to 0.012155577540397644 Saving model 4\nEpoch 316, Loss: 0.007866562344133854\nEpoch 317, Loss: 0.007143271621316671\nValidation Loss improved to 0.012150776572525501 Saving model 4\nEpoch 318, Loss: 0.0045504518784582615\nValidation Loss improved to 0.012128450907766819 Saving model 4\nEpoch 319, Loss: 0.004847084637731314\nValidation Loss improved to 0.012114898301661015 Saving model 4\nEpoch 320, Loss: 0.00620557414367795\nValidation Loss improved to 0.012113400734961033 Saving model 4\nEpoch 321, Loss: 0.006421986967325211\nValidation Loss improved to 0.012096078135073185 Saving model 4\nEpoch 322, Loss: 0.004906860180199146\nValidation Loss improved to 0.012086839415133 Saving model 4\nEpoch 323, Loss: 0.006991873495280743\nValidation Loss improved to 0.012065407820045948 Saving model 4\nEpoch 324, Loss: 0.004098869394510984\nEpoch 325, Loss: 0.008941324427723885\nEpoch 326, Loss: 0.0067691197618842125\nEpoch 327, Loss: 0.005724926944822073\nEpoch 328, Loss: 0.0037058417219668627\nValidation Loss improved to 0.01205661054700613 Saving model 4\nEpoch 329, Loss: 0.0042527224868535995\nValidation Loss improved to 0.012043947353959084 Saving model 4\nEpoch 330, Loss: 0.004968166816979647\nValidation Loss improved to 0.012036927044391632 Saving model 4\nEpoch 331, Loss: 0.005689313169568777\nValidation Loss improved to 0.012032812461256981 Saving model 4\nEpoch 332, Loss: 0.00422630226239562\nValidation Loss improved to 0.01201934926211834 Saving model 4\nEpoch 333, Loss: 0.004074710421264172\nValidation Loss improved to 0.012017110362648964 Saving model 4\nEpoch 334, Loss: 0.006337111350148916\nValidation Loss improved to 0.012002441100776196 Saving model 4\nEpoch 335, Loss: 0.003863787977024913\nValidation Loss improved to 0.011984803713858128 Saving model 4\nEpoch 336, Loss: 0.0032167192548513412\nValidation Loss improved to 0.011969408020377159 Saving model 4\nEpoch 337, Loss: 0.004310824442654848\nValidation Loss improved to 0.011959964409470558 Saving model 4\nEpoch 338, Loss: 0.007787847891449928\nValidation Loss improved to 0.01194559596478939 Saving model 4\nEpoch 339, Loss: 0.006891246419399977\nValidation Loss improved to 0.01194120105355978 Saving model 4\nEpoch 340, Loss: 0.005999065935611725\nValidation Loss improved to 0.01193033717572689 Saving model 4\nEpoch 341, Loss: 0.0050609963946044445\nValidation Loss improved to 0.011916205286979675 Saving model 4\nEpoch 342, Loss: 0.003900032490491867\nValidation Loss improved to 0.011898363940417767 Saving model 4\nEpoch 343, Loss: 0.0036198862362653017\nValidation Loss improved to 0.011882061138749123 Saving model 4\nEpoch 344, Loss: 0.0036197726149111986\nValidation Loss improved to 0.011870454996824265 Saving model 4\nEpoch 345, Loss: 0.007786477450281382\nEpoch 346, Loss: 0.011485048569738865\nEpoch 347, Loss: 0.008485008031129837\nEpoch 348, Loss: 0.007133558392524719\nEpoch 349, Loss: 0.004404819570481777\nEpoch 350, Loss: 0.003067260840907693\nEpoch 351, Loss: 0.0025908395182341337\nValidation Loss improved to 0.01186673529446125 Saving model 4\nEpoch 352, Loss: 0.003460278967395425\nValidation Loss improved to 0.011853818781673908 Saving model 4\nEpoch 353, Loss: 0.00535239465534687\nValidation Loss improved to 0.011837559752166271 Saving model 4\nEpoch 354, Loss: 0.0032724037300795317\nValidation Loss improved to 0.011835025623440742 Saving model 4\nEpoch 355, Loss: 0.005379257258027792\nValidation Loss improved to 0.011821639724075794 Saving model 4\nEpoch 356, Loss: 0.0058357310481369495\nEpoch 357, Loss: 0.0077731069177389145\nEpoch 358, Loss: 0.014160973951220512\nValidation Loss improved to 0.01182053703814745 Saving model 4\nEpoch 359, Loss: 0.005494372453540564\nValidation Loss improved to 0.011802258901298046 Saving model 4\nEpoch 360, Loss: 0.004729636013507843\nValidation Loss improved to 0.011785054579377174 Saving model 4\nEpoch 361, Loss: 0.0041956775821745396\nValidation Loss improved to 0.011778155341744423 Saving model 4\nEpoch 362, Loss: 0.006252064369618893\nValidation Loss improved to 0.011773483827710152 Saving model 4\nEpoch 363, Loss: 0.004319176543504\nValidation Loss improved to 0.011754673905670643 Saving model 4\nEpoch 364, Loss: 0.003464627778157592\nValidation Loss improved to 0.011737124063074589 Saving model 4\nEpoch 365, Loss: 0.003657856024801731\nValidation Loss improved to 0.011718488298356533 Saving model 4\nEpoch 366, Loss: 0.002985184546560049\nValidation Loss improved to 0.011717736721038818 Saving model 4\nEpoch 367, Loss: 0.007532195188105106\nEpoch 368, Loss: 0.00638407189399004\nValidation Loss improved to 0.011708555743098259 Saving model 4\nEpoch 369, Loss: 0.004292414989322424\nValidation Loss improved to 0.011702592484652996 Saving model 4\nEpoch 370, Loss: 0.006791063584387302\nEpoch 371, Loss: 0.00893473718315363\nValidation Loss improved to 0.011698112823069096 Saving model 4\nEpoch 372, Loss: 0.005911136046051979\nEpoch 373, Loss: 0.009271536022424698\nEpoch 374, Loss: 0.005796345416456461\nEpoch 375, Loss: 0.00897189136594534\nEpoch 376, Loss: 0.005722744856029749\nEpoch 377, Loss: 0.0035827269311994314\nValidation Loss improved to 0.011689228937029839 Saving model 4\nEpoch 378, Loss: 0.005412908270955086\nEpoch 379, Loss: 0.007079578936100006\nEpoch 380, Loss: 0.004128190688788891\nValidation Loss improved to 0.011685546487569809 Saving model 4\nEpoch 381, Loss: 0.003565180581063032\nValidation Loss improved to 0.01167306862771511 Saving model 4\nEpoch 382, Loss: 0.0039398809894919395\nValidation Loss improved to 0.011666459031403065 Saving model 4\nEpoch 383, Loss: 0.004936605226248503\nValidation Loss improved to 0.011663977988064289 Saving model 4\nEpoch 384, Loss: 0.005473642610013485\nValidation Loss improved to 0.011655505746603012 Saving model 4\nEpoch 385, Loss: 0.00416545057669282\nValidation Loss improved to 0.011649111285805702 Saving model 4\nEpoch 386, Loss: 0.008308743126690388\nValidation Loss improved to 0.011643577367067337 Saving model 4\nEpoch 387, Loss: 0.0035132942721247673\nEpoch 388, Loss: 0.005808790680021048\nEpoch 389, Loss: 0.004538460168987513\nEpoch 390, Loss: 0.005485864356160164\nEpoch 391, Loss: 0.005782315507531166\nEpoch 392, Loss: 0.005049864295870066\nValidation Loss improved to 0.011640343815088272 Saving model 4\nEpoch 393, Loss: 0.0035209113266319036\nValidation Loss improved to 0.011633804067969322 Saving model 4\nEpoch 394, Loss: 0.005679354537278414\nValidation Loss improved to 0.011624598875641823 Saving model 4\nEpoch 395, Loss: 0.004368145018815994\nValidation Loss improved to 0.011613396927714348 Saving model 4\nEpoch 396, Loss: 0.0033314512111246586\nValidation Loss improved to 0.011601929552853107 Saving model 4\nEpoch 397, Loss: 0.0030581336468458176\nValidation Loss improved to 0.011589374393224716 Saving model 4\nEpoch 398, Loss: 0.003412080928683281\nEpoch 399, Loss: 0.005222461652010679\nEpoch 400, Loss: 0.00777140399441123\nEpoch 401, Loss: 0.008401354774832726\nEpoch 402, Loss: 0.006375596392899752\nValidation Loss improved to 0.011582845821976662 Saving model 4\nEpoch 403, Loss: 0.009684303775429726\nEpoch 404, Loss: 0.008220775984227657\nValidation Loss improved to 0.011582308448851109 Saving model 4\nEpoch 405, Loss: 0.006325387395918369\nEpoch 406, Loss: 0.005144114140421152\nValidation Loss improved to 0.011576502583920956 Saving model 4\nEpoch 407, Loss: 0.0055786860175430775\nValidation Loss improved to 0.011571074835956097 Saving model 4\nEpoch 408, Loss: 0.0052735647186636925\nValidation Loss improved to 0.011564431712031364 Saving model 4\nEpoch 409, Loss: 0.004399855621159077\nValidation Loss improved to 0.011556385084986687 Saving model 4\nEpoch 410, Loss: 0.004888769704848528\nValidation Loss improved to 0.011554117314517498 Saving model 4\nEpoch 411, Loss: 0.003914687316864729\nValidation Loss improved to 0.011548205278813839 Saving model 4\nEpoch 412, Loss: 0.0034240407403558493\nValidation Loss improved to 0.011533591896295547 Saving model 4\nEpoch 413, Loss: 0.00436562392860651\nValidation Loss improved to 0.011530282907187939 Saving model 4\nEpoch 414, Loss: 0.007389403413981199\nValidation Loss improved to 0.011527132242918015 Saving model 4\nEpoch 415, Loss: 0.004436335060745478\nValidation Loss improved to 0.011519511230289936 Saving model 4\nEpoch 416, Loss: 0.0035904650576412678\nValidation Loss improved to 0.011508841067552567 Saving model 4\nEpoch 417, Loss: 0.0035456044133752584\nValidation Loss improved to 0.011498399078845978 Saving model 4\nEpoch 418, Loss: 0.004288924392312765\nValidation Loss improved to 0.01148821972310543 Saving model 4\nEpoch 419, Loss: 0.0022660212125629187\nValidation Loss improved to 0.011479907669126987 Saving model 4\nEpoch 420, Loss: 0.005649092607200146\nValidation Loss improved to 0.011475974693894386 Saving model 4\nEpoch 421, Loss: 0.003817405551671982\nValidation Loss improved to 0.011463942937552929 Saving model 4\nEpoch 422, Loss: 0.003063627053052187\nValidation Loss improved to 0.011460668407380581 Saving model 4\nEpoch 423, Loss: 0.0042059277184307575\nValidation Loss improved to 0.011451144702732563 Saving model 4\nEpoch 424, Loss: 0.0054585677571594715\nValidation Loss improved to 0.011450517922639847 Saving model 4\nEpoch 425, Loss: 0.005110604222863913\nValidation Loss improved to 0.011447813361883163 Saving model 4\nEpoch 426, Loss: 0.007569128647446632\nValidation Loss improved to 0.01144030038267374 Saving model 4\nEpoch 427, Loss: 0.005768471863120794\nEpoch 428, Loss: 0.0033838923554867506\nEpoch 429, Loss: 0.005300871096551418\nValidation Loss improved to 0.011437177658081055 Saving model 4\nEpoch 430, Loss: 0.002946465276181698\nValidation Loss improved to 0.011427768506109715 Saving model 4\nEpoch 431, Loss: 0.0047377306036651134\nValidation Loss improved to 0.011422759853303432 Saving model 4\nEpoch 432, Loss: 0.00672018202021718\nValidation Loss improved to 0.011414855718612671 Saving model 4\nEpoch 433, Loss: 0.0037781535647809505\nValidation Loss improved to 0.01140298880636692 Saving model 4\nEpoch 434, Loss: 0.002954652067273855\nValidation Loss improved to 0.01139373891055584 Saving model 4\nEpoch 435, Loss: 0.0042013623751699924\nEpoch 436, Loss: 0.008820504881441593\nValidation Loss improved to 0.011384570971131325 Saving model 4\nEpoch 437, Loss: 0.003554688533768058\nValidation Loss improved to 0.011376379057765007 Saving model 4\nEpoch 438, Loss: 0.004228940233588219\nValidation Loss improved to 0.011366179212927818 Saving model 4\nEpoch 439, Loss: 0.005675395950675011\nValidation Loss improved to 0.01135345734655857 Saving model 4\nEpoch 440, Loss: 0.003944280557334423\nEpoch 441, Loss: 0.0061403014697134495\nEpoch 442, Loss: 0.005152538418769836\nValidation Loss improved to 0.011350223794579506 Saving model 4\nEpoch 443, Loss: 0.003888484789058566\nValidation Loss improved to 0.011343696154654026 Saving model 4\nEpoch 444, Loss: 0.005853842478245497\nEpoch 445, Loss: 0.003993505146354437\nValidation Loss improved to 0.01133202575147152 Saving model 4\nEpoch 446, Loss: 0.002689091954380274\nValidation Loss improved to 0.011331340298056602 Saving model 4\nEpoch 447, Loss: 0.004877810832113028\nValidation Loss improved to 0.01132509671151638 Saving model 4\nEpoch 448, Loss: 0.004337896592915058\nValidation Loss improved to 0.011317413300275803 Saving model 4\nEpoch 449, Loss: 0.0037088829558342695\nValidation Loss improved to 0.011305182240903378 Saving model 4\nEpoch 450, Loss: 0.0037364664021879435\nValidation Loss improved to 0.011303067207336426 Saving model 4\nEpoch 451, Loss: 0.00366012379527092\nValidation Loss improved to 0.011300772428512573 Saving model 4\nEpoch 452, Loss: 0.003780969651415944\nEpoch 453, Loss: 0.008084854111075401\nEpoch 454, Loss: 0.005914649926126003\nEpoch 455, Loss: 0.005942565854638815\nEpoch 456, Loss: 0.004805874079465866\nEpoch 457, Loss: 0.005847944412380457\nEpoch 458, Loss: 0.009012341499328613\nEpoch 459, Loss: 0.005670367274433374\nEpoch 460, Loss: 0.004710145760327578\nEpoch 461, Loss: 0.003915160428732634\nEpoch 462, Loss: 0.0032626662869006395\nEpoch   463: reducing learning rate of group 0 to 1.5000e-04.\nEpoch 463, Loss: 0.0027794218622148037\nValidation Loss improved to 0.011294156312942505 Saving model 4\nEpoch 464, Loss: 0.0014130603522062302\nValidation Loss improved to 0.011282813735306263 Saving model 4\nEpoch 465, Loss: 0.001195202930830419\nValidation Loss improved to 0.011271580122411251 Saving model 4\nEpoch 466, Loss: 0.0010241639101877809\nValidation Loss improved to 0.0112611697986722 Saving model 4\nEpoch 467, Loss: 0.001012140535749495\nValidation Loss improved to 0.0112523278221488 Saving model 4\nEpoch 468, Loss: 0.0010202346602454782\nValidation Loss improved to 0.011242582462728024 Saving model 4\nEpoch 469, Loss: 0.0009816921083256602\nValidation Loss improved to 0.011230465956032276 Saving model 4\nEpoch 470, Loss: 0.0009385674493387341\nValidation Loss improved to 0.01121954433619976 Saving model 4\nEpoch 471, Loss: 0.0009305433486588299\nValidation Loss improved to 0.011208951473236084 Saving model 4\nEpoch 472, Loss: 0.0008694385178387165\nValidation Loss improved to 0.011197266168892384 Saving model 4\nEpoch 473, Loss: 0.0008638922590762377\nValidation Loss improved to 0.011187488213181496 Saving model 4\nEpoch 474, Loss: 0.00092726806178689\nValidation Loss improved to 0.011176449246704578 Saving model 4\nEpoch 475, Loss: 0.0008446546853519976\nValidation Loss improved to 0.011165875941514969 Saving model 4\nEpoch 476, Loss: 0.0008129282505251467\nValidation Loss improved to 0.01115486677736044 Saving model 4\nEpoch 477, Loss: 0.000831414305139333\nValidation Loss improved to 0.011143104173243046 Saving model 4\nEpoch 478, Loss: 0.0008357794140465558\nValidation Loss improved to 0.011133405379951 Saving model 4\nEpoch 479, Loss: 0.0008972008945420384\nValidation Loss improved to 0.011121705174446106 Saving model 4\nEpoch 480, Loss: 0.0008688236703164876\nValidation Loss improved to 0.011109909042716026 Saving model 4\nEpoch 481, Loss: 0.0008390781003981829\nValidation Loss improved to 0.011099930852651596 Saving model 4\nEpoch 482, Loss: 0.000834885926451534\nValidation Loss improved to 0.011088652536273003 Saving model 4\nEpoch 483, Loss: 0.0008179283468052745\nValidation Loss improved to 0.011078771203756332 Saving model 4\nEpoch 484, Loss: 0.0008017552318051457\nValidation Loss improved to 0.011068456806242466 Saving model 4\nEpoch 485, Loss: 0.0007824718486517668\nValidation Loss improved to 0.011059052310883999 Saving model 4\nEpoch 486, Loss: 0.0008366391411982477\nValidation Loss improved to 0.011047338135540485 Saving model 4\nEpoch 487, Loss: 0.00088410236639902\nValidation Loss improved to 0.011037537828087807 Saving model 4\nEpoch 488, Loss: 0.0007673941436223686\nValidation Loss improved to 0.011026875115931034 Saving model 4\nEpoch 489, Loss: 0.0007579927332699299\nValidation Loss improved to 0.011016828007996082 Saving model 4\nEpoch 490, Loss: 0.0008850896847434342\nValidation Loss improved to 0.011005942709743977 Saving model 4\nEpoch 491, Loss: 0.0008650890085846186\nValidation Loss improved to 0.010995535179972649 Saving model 4\nEpoch 492, Loss: 0.0008453786722384393\nValidation Loss improved to 0.010983879677951336 Saving model 4\nEpoch 493, Loss: 0.0008573625236749649\nValidation Loss improved to 0.01097503025084734 Saving model 4\nEpoch 494, Loss: 0.0008158125565387309\nValidation Loss improved to 0.010963479056954384 Saving model 4\nEpoch 495, Loss: 0.0007946578552946448\nValidation Loss improved to 0.01095286849886179 Saving model 4\nEpoch 496, Loss: 0.000719150819350034\nValidation Loss improved to 0.010943268425762653 Saving model 4\nEpoch 497, Loss: 0.0008110197959467769\nValidation Loss improved to 0.010932191275060177 Saving model 4\nEpoch 498, Loss: 0.000786700053140521\nValidation Loss improved to 0.010921514593064785 Saving model 4\nEpoch 499, Loss: 0.0008043494308367372\nValidation Loss improved to 0.01091050822287798 Saving model 4\nEpoch 500, Loss: 0.0007435104926116765\nValidation Loss improved to 0.010900483466684818 Saving model 4\nEpoch 501, Loss: 0.0007281643920578063\nValidation Loss improved to 0.010890576988458633 Saving model 4\nEpoch 502, Loss: 0.0007984734838828444\nValidation Loss improved to 0.01087965164333582 Saving model 4\nEpoch 503, Loss: 0.0007434553699567914\nValidation Loss improved to 0.010870511643588543 Saving model 4\nEpoch 504, Loss: 0.0007206259178929031\nValidation Loss improved to 0.010860137641429901 Saving model 4\nEpoch 505, Loss: 0.0007001502672210336\nValidation Loss improved to 0.010849317535758018 Saving model 4\nEpoch 506, Loss: 0.0008200136362574995\nValidation Loss improved to 0.010840438306331635 Saving model 4\nEpoch 507, Loss: 0.0007204125286079943\nValidation Loss improved to 0.010830579325556755 Saving model 4\nEpoch 508, Loss: 0.0006418459233827889\nValidation Loss improved to 0.010820722207427025 Saving model 4\nEpoch 509, Loss: 0.0006800834671594203\nValidation Loss improved to 0.010811454616487026 Saving model 4\nEpoch 510, Loss: 0.0006574310828000307\nValidation Loss improved to 0.010802621021866798 Saving model 4\nEpoch 511, Loss: 0.0007897407631389797\nValidation Loss improved to 0.01079188846051693 Saving model 4\nEpoch 512, Loss: 0.0008695319993421435\nValidation Loss improved to 0.01078259851783514 Saving model 4\nEpoch 513, Loss: 0.0007130116573534906\nValidation Loss improved to 0.010772214271128178 Saving model 4\nEpoch 514, Loss: 0.0007404456264339387\nValidation Loss improved to 0.010763357393443584 Saving model 4\nEpoch 515, Loss: 0.000738499453291297\nValidation Loss improved to 0.010752955451607704 Saving model 4\nEpoch 516, Loss: 0.0008456228533759713\nValidation Loss improved to 0.010743916966021061 Saving model 4\nEpoch 517, Loss: 0.0007839089375920594\nValidation Loss improved to 0.010733646340668201 Saving model 4\nEpoch 518, Loss: 0.0006509657250717282\nValidation Loss improved to 0.010724162682890892 Saving model 4\nEpoch 519, Loss: 0.0007323327590711415\nValidation Loss improved to 0.010715903714299202 Saving model 4\nEpoch 520, Loss: 0.0006590844714082778\nValidation Loss improved to 0.01070633064955473 Saving model 4\nEpoch 521, Loss: 0.0007024542428553104\nValidation Loss improved to 0.010695982724428177 Saving model 4\nEpoch 522, Loss: 0.0006821801653131843\nValidation Loss improved to 0.01068769209086895 Saving model 4\nEpoch 523, Loss: 0.0006695398478768766\nValidation Loss improved to 0.010678384453058243 Saving model 4\nEpoch 524, Loss: 0.0006879842258058488\nValidation Loss improved to 0.010668940842151642 Saving model 4\nEpoch 525, Loss: 0.0006437093252316117\nValidation Loss improved to 0.010659952647984028 Saving model 4\nEpoch 526, Loss: 0.0006659068749286234\nValidation Loss improved to 0.010650227777659893 Saving model 4\nEpoch 527, Loss: 0.0006267609423957765\nValidation Loss improved to 0.010640164837241173 Saving model 4\nEpoch 528, Loss: 0.0006223385571502149\nValidation Loss improved to 0.01063102763146162 Saving model 4\nEpoch 529, Loss: 0.000662860693410039\nValidation Loss improved to 0.010622034780681133 Saving model 4\nEpoch 530, Loss: 0.0006185828242450953\nValidation Loss improved to 0.010612459853291512 Saving model 4\nEpoch 531, Loss: 0.0006146128289401531\nValidation Loss improved to 0.010602643713355064 Saving model 4\nEpoch 532, Loss: 0.0006557634333148599\nValidation Loss improved to 0.010592835023999214 Saving model 4\nEpoch 533, Loss: 0.0006766464211978018\nValidation Loss improved to 0.010583270341157913 Saving model 4\nEpoch 534, Loss: 0.0005841874517500401\nValidation Loss improved to 0.010574163869023323 Saving model 4\nEpoch 535, Loss: 0.0006533330888487399\nValidation Loss improved to 0.010565822012722492 Saving model 4\nEpoch 536, Loss: 0.0006300827953964472\nValidation Loss improved to 0.010557285510003567 Saving model 4\nEpoch 537, Loss: 0.0007002520724199712\nValidation Loss improved to 0.010548542253673077 Saving model 4\nEpoch 538, Loss: 0.0006192942382767797\nValidation Loss improved to 0.010539593175053596 Saving model 4\nEpoch 539, Loss: 0.0005806863773614168\nValidation Loss improved to 0.010530333034694195 Saving model 4\nEpoch 540, Loss: 0.0006505848141387105\nValidation Loss improved to 0.010520544834434986 Saving model 4\nEpoch 541, Loss: 0.0006708590080961585\nValidation Loss improved to 0.010511512868106365 Saving model 4\nEpoch 542, Loss: 0.0006343177519738674\nValidation Loss improved to 0.010502763092517853 Saving model 4\nEpoch 543, Loss: 0.0006374713266268373\nValidation Loss improved to 0.010493919253349304 Saving model 4\nEpoch 544, Loss: 0.0008317041792906821\nValidation Loss improved to 0.010484490543603897 Saving model 4\nEpoch 545, Loss: 0.0006724690902046859\nValidation Loss improved to 0.01047508418560028 Saving model 4\nEpoch 546, Loss: 0.0006143213831819594\nValidation Loss improved to 0.010465793311595917 Saving model 4\nEpoch 547, Loss: 0.0008515954832546413\nValidation Loss improved to 0.01045801118016243 Saving model 4\nEpoch 548, Loss: 0.0007526428089477122\nValidation Loss improved to 0.010448970831930637 Saving model 4\nEpoch 549, Loss: 0.0006507760263048112\nValidation Loss improved to 0.010439706966280937 Saving model 4\nEpoch 550, Loss: 0.000645205844193697\nValidation Loss improved to 0.010430955328047276 Saving model 4\nEpoch 551, Loss: 0.0006138031603768468\nValidation Loss improved to 0.010421662591397762 Saving model 4\nEpoch 552, Loss: 0.0006257802015170455\nValidation Loss improved to 0.01041267067193985 Saving model 4\nEpoch 553, Loss: 0.0008187070488929749\nValidation Loss improved to 0.010403897613286972 Saving model 4\nEpoch 554, Loss: 0.0006119475583545864\nValidation Loss improved to 0.010394776239991188 Saving model 4\nEpoch 555, Loss: 0.000607724825385958\nValidation Loss improved to 0.01038551889359951 Saving model 4\nEpoch 556, Loss: 0.0006364620639942586\nValidation Loss improved to 0.010376991704106331 Saving model 4\nEpoch 557, Loss: 0.00064274383476004\nValidation Loss improved to 0.010367964394390583 Saving model 4\nEpoch 558, Loss: 0.0006513138650916517\nValidation Loss improved to 0.010358648374676704 Saving model 4\nEpoch 559, Loss: 0.0007801927858963609\nValidation Loss improved to 0.010350218042731285 Saving model 4\nEpoch 560, Loss: 0.0005764030502177775\nValidation Loss improved to 0.010341398417949677 Saving model 4\nEpoch 561, Loss: 0.000571579672396183\nValidation Loss improved to 0.010333693586289883 Saving model 4\nEpoch 562, Loss: 0.0006007024785503745\nValidation Loss improved to 0.010325487703084946 Saving model 4\nEpoch 563, Loss: 0.0005352405714802444\nValidation Loss improved to 0.010316935367882252 Saving model 4\nEpoch 564, Loss: 0.0005585140315815806\nValidation Loss improved to 0.010308667086064816 Saving model 4\nEpoch 565, Loss: 0.000646278087515384\nValidation Loss improved to 0.010300830937922001 Saving model 4\nEpoch 566, Loss: 0.0005870776949450374\nValidation Loss improved to 0.010292342863976955 Saving model 4\nEpoch 567, Loss: 0.0005258137243799865\nValidation Loss improved to 0.010284348390996456 Saving model 4\nEpoch 568, Loss: 0.0005801363731734455\nValidation Loss improved to 0.010276919230818748 Saving model 4\nEpoch 569, Loss: 0.0006544851348735392\nValidation Loss improved to 0.010269539430737495 Saving model 4\nEpoch 570, Loss: 0.000646396481897682\nValidation Loss improved to 0.010260890237987041 Saving model 4\nEpoch 571, Loss: 0.0006385291926562786\nValidation Loss improved to 0.01025217305868864 Saving model 4\nEpoch 572, Loss: 0.0006529316306114197\nValidation Loss improved to 0.010243500582873821 Saving model 4\nEpoch 573, Loss: 0.0006766925798729062\nValidation Loss improved to 0.010234796442091465 Saving model 4\nEpoch 574, Loss: 0.000747465412132442\nValidation Loss improved to 0.010226971469819546 Saving model 4\nEpoch 575, Loss: 0.0005708329263143241\nValidation Loss improved to 0.010218489915132523 Saving model 4\nEpoch 576, Loss: 0.0006019811844453216\nValidation Loss improved to 0.01021052710711956 Saving model 4\nEpoch 577, Loss: 0.0006915754056535661\nValidation Loss improved to 0.010202013887465 Saving model 4\nEpoch 578, Loss: 0.0007367294165305793\nValidation Loss improved to 0.010193601250648499 Saving model 4\nEpoch 579, Loss: 0.000623225059825927\nValidation Loss improved to 0.010184947401285172 Saving model 4\nEpoch 580, Loss: 0.0006993128918111324\nValidation Loss improved to 0.010176503099501133 Saving model 4\nEpoch 581, Loss: 0.0005882448167540133\nValidation Loss improved to 0.010168436914682388 Saving model 4\nEpoch 582, Loss: 0.0006090519018471241\nValidation Loss improved to 0.010161139070987701 Saving model 4\nEpoch 583, Loss: 0.0005814015166833997\nValidation Loss improved to 0.010152525268495083 Saving model 4\nEpoch 584, Loss: 0.0006553949206136167\nValidation Loss improved to 0.010144482366740704 Saving model 4\nEpoch 585, Loss: 0.0006286795251071453\nValidation Loss improved to 0.010136053897440434 Saving model 4\nEpoch 586, Loss: 0.0006392064969986677\nValidation Loss improved to 0.010127725079655647 Saving model 4\nEpoch 587, Loss: 0.0005909781903028488\nValidation Loss improved to 0.010119939222931862 Saving model 4\nEpoch 588, Loss: 0.0005649365484714508\nValidation Loss improved to 0.01011226512491703 Saving model 4\nEpoch 589, Loss: 0.000510728859808296\nValidation Loss improved to 0.010104378685355186 Saving model 4\nEpoch 590, Loss: 0.0005030903266742826\nValidation Loss improved to 0.010096575133502483 Saving model 4\nEpoch 591, Loss: 0.0005377738852985203\nValidation Loss improved to 0.01008911058306694 Saving model 4\nEpoch 592, Loss: 0.0005380419897846878\nValidation Loss improved to 0.010080987587571144 Saving model 4\nEpoch 593, Loss: 0.0005351246800273657\nValidation Loss improved to 0.01007299218326807 Saving model 4\nEpoch 594, Loss: 0.000510962272528559\nValidation Loss improved to 0.010065432637929916 Saving model 4\nEpoch 595, Loss: 0.0005114625673741102\nValidation Loss improved to 0.010057359002530575 Saving model 4\nEpoch 596, Loss: 0.0005741958739235997\nValidation Loss improved to 0.010049647651612759 Saving model 4\nEpoch 597, Loss: 0.0005234592827036977\nValidation Loss improved to 0.01004198007285595 Saving model 4\nEpoch 598, Loss: 0.0007298784330487251\nValidation Loss improved to 0.010034549050033092 Saving model 4\nEpoch 599, Loss: 0.000519296620041132\nValidation Loss improved to 0.010026856325566769 Saving model 4\nEpoch 600, Loss: 0.0005259641911834478\nValidation Loss improved to 0.010019735433161259 Saving model 4\nEpoch 601, Loss: 0.0005209498922340572\nValidation Loss improved to 0.010012623853981495 Saving model 4\nEpoch 602, Loss: 0.0005607414641417563\nValidation Loss improved to 0.010004917159676552 Saving model 4\nEpoch 603, Loss: 0.0004755979753099382\nValidation Loss improved to 0.00999717228114605 Saving model 4\nEpoch 604, Loss: 0.0005520413396880031\nValidation Loss improved to 0.009989911690354347 Saving model 4\nEpoch 605, Loss: 0.0005140361026860774\nValidation Loss improved to 0.009982745163142681 Saving model 4\nEpoch 606, Loss: 0.0005246299551799893\nValidation Loss improved to 0.00997504498809576 Saving model 4\nEpoch 607, Loss: 0.0005491009796969593\nValidation Loss improved to 0.009968704544007778 Saving model 4\nEpoch 608, Loss: 0.0007391128456220031\nValidation Loss improved to 0.00996180810034275 Saving model 4\nEpoch 609, Loss: 0.0007518857019022107\nValidation Loss improved to 0.009954494424164295 Saving model 4\nEpoch 610, Loss: 0.00052072131074965\nValidation Loss improved to 0.009946908801794052 Saving model 4\nEpoch 611, Loss: 0.0005232520634308457\nValidation Loss improved to 0.009939569979906082 Saving model 4\nEpoch 612, Loss: 0.0005180969601497054\nValidation Loss improved to 0.009932227432727814 Saving model 4\nEpoch 613, Loss: 0.0005408577271737158\nValidation Loss improved to 0.00992431677877903 Saving model 4\nEpoch 614, Loss: 0.000562503410037607\nValidation Loss improved to 0.009916815906763077 Saving model 4\nEpoch 615, Loss: 0.0005021371762268245\nValidation Loss improved to 0.009910098277032375 Saving model 4\nEpoch 616, Loss: 0.0005680620088241994\nValidation Loss improved to 0.009903080761432648 Saving model 4\nEpoch 617, Loss: 0.0004768056096509099\nValidation Loss improved to 0.009895904920995235 Saving model 4\nEpoch 618, Loss: 0.000518814311362803\nValidation Loss improved to 0.009889019653201103 Saving model 4\nEpoch 619, Loss: 0.00047391082625836134\nValidation Loss improved to 0.009882531128823757 Saving model 4\nEpoch 620, Loss: 0.0006073142285458744\nValidation Loss improved to 0.009875476360321045 Saving model 4\nEpoch 621, Loss: 0.0005245590000413358\nValidation Loss improved to 0.009868243709206581 Saving model 4\nEpoch 622, Loss: 0.0005705112707801163\nValidation Loss improved to 0.009861694648861885 Saving model 4\nEpoch 623, Loss: 0.0006100809550844133\nValidation Loss improved to 0.009854831732809544 Saving model 4\nEpoch 624, Loss: 0.000618427584413439\nValidation Loss improved to 0.00984829943627119 Saving model 4\nEpoch 625, Loss: 0.0006009063799865544\nValidation Loss improved to 0.009841577149927616 Saving model 4\nEpoch 626, Loss: 0.0005054128705523908\nValidation Loss improved to 0.009834100492298603 Saving model 4\nEpoch 627, Loss: 0.0005351936561055481\nValidation Loss improved to 0.009827464818954468 Saving model 4\nEpoch 628, Loss: 0.0005929900798946619\nValidation Loss improved to 0.009820480830967426 Saving model 4\nEpoch 629, Loss: 0.0007254723459482193\nValidation Loss improved to 0.00981380045413971 Saving model 4\nEpoch 630, Loss: 0.0009266440174542367\nValidation Loss improved to 0.009806957095861435 Saving model 4\nEpoch 631, Loss: 0.00093833077698946\nValidation Loss improved to 0.009800036437809467 Saving model 4\nEpoch 632, Loss: 0.0005416098283603787\nValidation Loss improved to 0.009792739525437355 Saving model 4\nEpoch 633, Loss: 0.0005476385704241693\nValidation Loss improved to 0.009785419330000877 Saving model 4\nEpoch 634, Loss: 0.0005901200929656625\nValidation Loss improved to 0.009778659790754318 Saving model 4\nEpoch 635, Loss: 0.0006877797422930598\nValidation Loss improved to 0.009771858341991901 Saving model 4\nEpoch 636, Loss: 0.0005494423676282167\nValidation Loss improved to 0.0097647774964571 Saving model 4\nEpoch 637, Loss: 0.0005118806730024517\nValidation Loss improved to 0.009758013300597668 Saving model 4\nEpoch 638, Loss: 0.0005446659633889794\nValidation Loss improved to 0.009751354344189167 Saving model 4\nEpoch 639, Loss: 0.0006058606668375432\nValidation Loss improved to 0.009745379909873009 Saving model 4\nEpoch 640, Loss: 0.000689372536726296\nValidation Loss improved to 0.009739534929394722 Saving model 4\nEpoch 641, Loss: 0.0006583999493159354\nValidation Loss improved to 0.009732617065310478 Saving model 4\nEpoch 642, Loss: 0.0005190687952563167\nValidation Loss improved to 0.00972591433674097 Saving model 4\nEpoch 643, Loss: 0.0005136438412591815\nValidation Loss improved to 0.009718988090753555 Saving model 4\nEpoch 644, Loss: 0.000576157821342349\nValidation Loss improved to 0.00971200130879879 Saving model 4\nEpoch 645, Loss: 0.0005192580865696073\nValidation Loss improved to 0.009704959578812122 Saving model 4\nEpoch 646, Loss: 0.0004584458074532449\nValidation Loss improved to 0.009698120877146721 Saving model 4\nEpoch 647, Loss: 0.0004533040046226233\nValidation Loss improved to 0.009691564366221428 Saving model 4\nEpoch 648, Loss: 0.0005073262727819383\nValidation Loss improved to 0.009684829041361809 Saving model 4\nEpoch 649, Loss: 0.00041709953802637756\nValidation Loss improved to 0.009678013622760773 Saving model 4\nEpoch 650, Loss: 0.0004403420025482774\nValidation Loss improved to 0.00967129785567522 Saving model 4\nEpoch 651, Loss: 0.00045587733620777726\nValidation Loss improved to 0.009664641693234444 Saving model 4\nEpoch 652, Loss: 0.0004815440042875707\nValidation Loss improved to 0.009657848626375198 Saving model 4\nEpoch 653, Loss: 0.00046635832404717803\nValidation Loss improved to 0.009651376865804195 Saving model 4\nEpoch 654, Loss: 0.000508372497279197\nValidation Loss improved to 0.009644769132137299 Saving model 4\nEpoch 655, Loss: 0.00043775225640274584\nValidation Loss improved to 0.00963893998414278 Saving model 4\nEpoch 656, Loss: 0.0006870025536045432\nValidation Loss improved to 0.009632856585085392 Saving model 4\nEpoch 657, Loss: 0.0004525297263171524\nValidation Loss improved to 0.009626083076000214 Saving model 4\nEpoch 658, Loss: 0.0004649898037314415\nValidation Loss improved to 0.00961932260543108 Saving model 4\nEpoch 659, Loss: 0.0004647774330805987\nValidation Loss improved to 0.009612894617021084 Saving model 4\nEpoch 660, Loss: 0.0004850185359828174\nValidation Loss improved to 0.009606339037418365 Saving model 4\nEpoch 661, Loss: 0.0004362660984043032\nValidation Loss improved to 0.009599948301911354 Saving model 4\nEpoch 662, Loss: 0.0004851404810324311\nValidation Loss improved to 0.009593620896339417 Saving model 4\nEpoch 663, Loss: 0.00044100021477788687\nValidation Loss improved to 0.009587016887962818 Saving model 4\nEpoch 664, Loss: 0.0004632468044292182\nValidation Loss improved to 0.009580533020198345 Saving model 4\nEpoch 665, Loss: 0.0005765747046098113\nValidation Loss improved to 0.00957418791949749 Saving model 4\nEpoch 666, Loss: 0.0006167902611196041\nValidation Loss improved to 0.009568518958985806 Saving model 4\nEpoch 667, Loss: 0.0005474741337820888\nValidation Loss improved to 0.009562931023538113 Saving model 4\nEpoch 668, Loss: 0.0006015616818331182\nValidation Loss improved to 0.00955690536648035 Saving model 4\nEpoch 669, Loss: 0.0004934053868055344\nValidation Loss improved to 0.009550248272716999 Saving model 4\nEpoch 670, Loss: 0.00048328674165531993\nValidation Loss improved to 0.009543864987790585 Saving model 4\nEpoch 671, Loss: 0.0004726878250949085\nValidation Loss improved to 0.009537611156702042 Saving model 4\nEpoch 672, Loss: 0.000478952017147094\nValidation Loss improved to 0.009531584568321705 Saving model 4\nEpoch 673, Loss: 0.0004841777263209224\nValidation Loss improved to 0.009525598958134651 Saving model 4\nEpoch 674, Loss: 0.0005426137940958142\nValidation Loss improved to 0.009519225917756557 Saving model 4\nEpoch 675, Loss: 0.0005087822792120278\nValidation Loss improved to 0.009512979537248611 Saving model 4\nEpoch 676, Loss: 0.0007173526100814342\nValidation Loss improved to 0.009507184848189354 Saving model 4\nEpoch 677, Loss: 0.000546931114513427\nValidation Loss improved to 0.009500994347035885 Saving model 4\nEpoch 678, Loss: 0.000491822196636349\nValidation Loss improved to 0.009495035745203495 Saving model 4\nEpoch 679, Loss: 0.0004408688109833747\nValidation Loss improved to 0.009488695301115513 Saving model 4\nEpoch 680, Loss: 0.000447732862085104\nValidation Loss improved to 0.009482367895543575 Saving model 4\nEpoch 681, Loss: 0.00046899914741516113\nValidation Loss improved to 0.009476500563323498 Saving model 4\nEpoch 682, Loss: 0.0004414654686115682\nValidation Loss improved to 0.009470587596297264 Saving model 4\nEpoch 683, Loss: 0.0005492769996635616\nValidation Loss improved to 0.009464552626013756 Saving model 4\nEpoch 684, Loss: 0.0005074499058537185\nValidation Loss improved to 0.009458303451538086 Saving model 4\nEpoch 685, Loss: 0.0005075786611996591\nValidation Loss improved to 0.009452334605157375 Saving model 4\nEpoch 686, Loss: 0.0004396215663291514\nValidation Loss improved to 0.009446761570870876 Saving model 4\nEpoch 687, Loss: 0.000516009924467653\nValidation Loss improved to 0.009440851397812366 Saving model 4\nEpoch 688, Loss: 0.0004077776684425771\nValidation Loss improved to 0.009435001760721207 Saving model 4\nEpoch 689, Loss: 0.0004562001267913729\nValidation Loss improved to 0.009429240599274635 Saving model 4\nEpoch 690, Loss: 0.0006078589358367026\nValidation Loss improved to 0.00942343007773161 Saving model 4\nEpoch 691, Loss: 0.0004538697248790413\nValidation Loss improved to 0.00941749382764101 Saving model 4\nEpoch 692, Loss: 0.0005430338205769658\nValidation Loss improved to 0.009411633014678955 Saving model 4\nEpoch 693, Loss: 0.0005624122568406165\nValidation Loss improved to 0.009405762888491154 Saving model 4\nEpoch 694, Loss: 0.00041752224205993116\nValidation Loss improved to 0.009399780072271824 Saving model 4\nEpoch 695, Loss: 0.000574976671487093\nValidation Loss improved to 0.009393764659762383 Saving model 4\nEpoch 696, Loss: 0.0005392227321863174\nValidation Loss improved to 0.009387812577188015 Saving model 4\nEpoch 697, Loss: 0.0006260560476221144\nValidation Loss improved to 0.009381943382322788 Saving model 4\nEpoch 698, Loss: 0.0004979339428246021\nValidation Loss improved to 0.00937609188258648 Saving model 4\nEpoch 699, Loss: 0.0004459819756448269\nValidation Loss improved to 0.009370182640850544 Saving model 4\nEpoch 700, Loss: 0.000434809917351231\nValidation Loss improved to 0.009364579804241657 Saving model 4\nEpoch 701, Loss: 0.00043502417975105345\nValidation Loss improved to 0.009358829818665981 Saving model 4\nEpoch 702, Loss: 0.0005583266611211002\nValidation Loss improved to 0.009353207424283028 Saving model 4\nEpoch 703, Loss: 0.0004457350878510624\nValidation Loss improved to 0.009347524493932724 Saving model 4\nEpoch 704, Loss: 0.00040300688124261796\nValidation Loss improved to 0.009342084638774395 Saving model 4\nEpoch 705, Loss: 0.00040319698746316135\nValidation Loss improved to 0.009336714632809162 Saving model 4\nEpoch 706, Loss: 0.0004299163992982358\nValidation Loss improved to 0.009330950677394867 Saving model 4\nEpoch 707, Loss: 0.0004552020691335201\nValidation Loss improved to 0.009325327351689339 Saving model 4\nEpoch 708, Loss: 0.0005323823424987495\nValidation Loss improved to 0.009319912642240524 Saving model 4\nEpoch 709, Loss: 0.0004502466763369739\nValidation Loss improved to 0.009314323775470257 Saving model 4\nEpoch 710, Loss: 0.0004404329229146242\nValidation Loss improved to 0.009308467619121075 Saving model 4\nEpoch 711, Loss: 0.0004097282362636179\nValidation Loss improved to 0.00930310133844614 Saving model 4\nEpoch 712, Loss: 0.0004534999607130885\nValidation Loss improved to 0.009297442622482777 Saving model 4\nEpoch 713, Loss: 0.0007013370050117373\nValidation Loss improved to 0.009292379952967167 Saving model 4\nEpoch 714, Loss: 0.000505539181176573\nValidation Loss improved to 0.009286701679229736 Saving model 4\nEpoch 715, Loss: 0.0004495431494433433\nValidation Loss improved to 0.009281214326620102 Saving model 4\nEpoch 716, Loss: 0.0004539320361800492\nValidation Loss improved to 0.009275454096496105 Saving model 4\nEpoch 717, Loss: 0.0005689761019311845\nValidation Loss improved to 0.009269866161048412 Saving model 4\nEpoch 718, Loss: 0.0003892400418408215\nValidation Loss improved to 0.009264163672924042 Saving model 4\nEpoch 719, Loss: 0.0005844913539476693\nValidation Loss improved to 0.009258606471121311 Saving model 4\nEpoch 720, Loss: 0.0005709049291908741\nValidation Loss improved to 0.00925296451896429 Saving model 4\nEpoch 721, Loss: 0.0005178147694095969\nValidation Loss improved to 0.00924744363874197 Saving model 4\nEpoch 722, Loss: 0.00041599030373618007\nValidation Loss improved to 0.009242075495421886 Saving model 4\nEpoch 723, Loss: 0.0004919228958897293\nValidation Loss improved to 0.009236808866262436 Saving model 4\nEpoch 724, Loss: 0.0004580079694278538\nValidation Loss improved to 0.00923159159719944 Saving model 4\nEpoch 725, Loss: 0.0004953414318151772\nValidation Loss improved to 0.009226635098457336 Saving model 4\nEpoch 726, Loss: 0.00046828400809317827\nValidation Loss improved to 0.009220981039106846 Saving model 4\nEpoch 727, Loss: 0.0004997673677280545\nValidation Loss improved to 0.009215754456818104 Saving model 4\nEpoch 728, Loss: 0.00042390465387143195\nValidation Loss improved to 0.009210294112563133 Saving model 4\nEpoch 729, Loss: 0.00040591316064819694\nValidation Loss improved to 0.00920482724905014 Saving model 4\nEpoch 730, Loss: 0.0004545354750007391\nValidation Loss improved to 0.009199857711791992 Saving model 4\nEpoch 731, Loss: 0.0004652096831705421\nValidation Loss improved to 0.009194782935082912 Saving model 4\nEpoch 732, Loss: 0.0004944279207848012\nValidation Loss improved to 0.009189646691083908 Saving model 4\nEpoch 733, Loss: 0.0004519918584264815\nValidation Loss improved to 0.009184317663311958 Saving model 4\nEpoch 734, Loss: 0.00036510475911200047\nValidation Loss improved to 0.009178880602121353 Saving model 4\nEpoch 735, Loss: 0.00044616710511036217\nValidation Loss improved to 0.009173558093607426 Saving model 4\nEpoch 736, Loss: 0.00040631263982504606\nValidation Loss improved to 0.009168210439383984 Saving model 4\nEpoch 737, Loss: 0.0004947022534906864\nValidation Loss improved to 0.009163235314190388 Saving model 4\nEpoch 738, Loss: 0.000455268076620996\nValidation Loss improved to 0.009158295579254627 Saving model 4\nEpoch 739, Loss: 0.000424327066866681\nValidation Loss improved to 0.009153157472610474 Saving model 4\nEpoch 740, Loss: 0.0004915151512250304\nValidation Loss improved to 0.009148203767836094 Saving model 4\nEpoch 741, Loss: 0.00046907729119993746\nValidation Loss improved to 0.00914299301803112 Saving model 4\nEpoch 742, Loss: 0.0006427256157621741\nValidation Loss improved to 0.009137973189353943 Saving model 4\nEpoch 743, Loss: 0.0004313188255764544\nValidation Loss improved to 0.009132852777838707 Saving model 4\nEpoch 744, Loss: 0.0005380091606639326\nValidation Loss improved to 0.00912813562899828 Saving model 4\nEpoch 745, Loss: 0.0004725260951090604\nValidation Loss improved to 0.009122843854129314 Saving model 4\nEpoch 746, Loss: 0.000416519062127918\nValidation Loss improved to 0.009117959067225456 Saving model 4\nEpoch 747, Loss: 0.0004628524766303599\nValidation Loss improved to 0.009112797677516937 Saving model 4\nEpoch 748, Loss: 0.00044577394146472216\nValidation Loss improved to 0.009107524529099464 Saving model 4\nEpoch 749, Loss: 0.0004029866249766201\nValidation Loss improved to 0.0091022290289402 Saving model 4\nEpoch 750, Loss: 0.0004377543809823692\nValidation Loss improved to 0.009097174741327763 Saving model 4\nEpoch 751, Loss: 0.00037166947731748223\nValidation Loss improved to 0.009092169813811779 Saving model 4\nEpoch 752, Loss: 0.0004069973365403712\nValidation Loss improved to 0.009087531827390194 Saving model 4\nEpoch 753, Loss: 0.00047747354255989194\nValidation Loss improved to 0.009082437492907047 Saving model 4\nEpoch 754, Loss: 0.0004312033415772021\nValidation Loss improved to 0.009077401831746101 Saving model 4\nEpoch 755, Loss: 0.0003846182080451399\nValidation Loss improved to 0.009072696790099144 Saving model 4\nEpoch 756, Loss: 0.00042084784945473075\nValidation Loss improved to 0.00906765554100275 Saving model 4\nEpoch 757, Loss: 0.0004943060339428484\nValidation Loss improved to 0.009062716737389565 Saving model 4\nEpoch 758, Loss: 0.0004067633708473295\nValidation Loss improved to 0.009057514369487762 Saving model 4\nEpoch 759, Loss: 0.0004724141617771238\nValidation Loss improved to 0.009052329696714878 Saving model 4\nEpoch 760, Loss: 0.0004645386361517012\nValidation Loss improved to 0.009047379717230797 Saving model 4\nEpoch 761, Loss: 0.00047736032865941525\nValidation Loss improved to 0.009042334742844105 Saving model 4\nEpoch 762, Loss: 0.0004708175838459283\nValidation Loss improved to 0.009037586860358715 Saving model 4\nEpoch 763, Loss: 0.00043855345575138927\nValidation Loss improved to 0.00903255958110094 Saving model 4\nEpoch 764, Loss: 0.000433242239523679\nValidation Loss improved to 0.009027940221130848 Saving model 4\nEpoch 765, Loss: 0.0004783672920893878\nValidation Loss improved to 0.009023069404065609 Saving model 4\nEpoch 766, Loss: 0.000423977617174387\nValidation Loss improved to 0.009018427692353725 Saving model 4\nEpoch 767, Loss: 0.000396836461732164\nValidation Loss improved to 0.009013401344418526 Saving model 4\nEpoch 768, Loss: 0.00042087785550393164\nValidation Loss improved to 0.00900842435657978 Saving model 4\nEpoch 769, Loss: 0.00045226255315355957\nValidation Loss improved to 0.009003736078739166 Saving model 4\nEpoch 770, Loss: 0.0006327475421130657\nValidation Loss improved to 0.008998775854706764 Saving model 4\nEpoch 771, Loss: 0.0006240014336071908\nValidation Loss improved to 0.008993901312351227 Saving model 4\nEpoch 772, Loss: 0.0004429978725966066\nValidation Loss improved to 0.00898885726928711 Saving model 4\nEpoch 773, Loss: 0.00043002195889130235\nValidation Loss improved to 0.008984330110251904 Saving model 4\nEpoch 774, Loss: 0.0005518599064089358\nValidation Loss improved to 0.008979378268122673 Saving model 4\nEpoch 775, Loss: 0.00043238094076514244\nValidation Loss improved to 0.008974570780992508 Saving model 4\nEpoch 776, Loss: 0.0004806773504242301\nValidation Loss improved to 0.00897015631198883 Saving model 4\nEpoch 777, Loss: 0.0004916635225526989\nValidation Loss improved to 0.008965663611888885 Saving model 4\nEpoch 778, Loss: 0.0005133651429787278\nValidation Loss improved to 0.008960968814790249 Saving model 4\nEpoch 779, Loss: 0.0005822506500408053\nValidation Loss improved to 0.00895642675459385 Saving model 4\nEpoch 780, Loss: 0.0005640126182697713\nValidation Loss improved to 0.008951624855399132 Saving model 4\nEpoch 781, Loss: 0.00045004935236647725\nValidation Loss improved to 0.008946910500526428 Saving model 4\nEpoch 782, Loss: 0.0005314839072525501\nValidation Loss improved to 0.008942119777202606 Saving model 4\nEpoch 783, Loss: 0.00041090251761488616\nValidation Loss improved to 0.008937674574553967 Saving model 4\nEpoch 784, Loss: 0.000557705236133188\nValidation Loss improved to 0.00893315114080906 Saving model 4\nEpoch 785, Loss: 0.0004582113469950855\nValidation Loss improved to 0.008928213268518448 Saving model 4\nEpoch 786, Loss: 0.00044163133134134114\nValidation Loss improved to 0.008923973888158798 Saving model 4\nEpoch 787, Loss: 0.0004459668998606503\nValidation Loss improved to 0.008919166401028633 Saving model 4\nEpoch 788, Loss: 0.00040516111766919494\nValidation Loss improved to 0.008914557285606861 Saving model 4\nEpoch 789, Loss: 0.00035549551830627024\nValidation Loss improved to 0.008909997530281544 Saving model 4\nEpoch 790, Loss: 0.00036852501216344535\nValidation Loss improved to 0.008905359543859959 Saving model 4\nEpoch 791, Loss: 0.0005280307377688587\nValidation Loss improved to 0.00890064425766468 Saving model 4\nEpoch 792, Loss: 0.00045303808292374015\nValidation Loss improved to 0.008896413259208202 Saving model 4\nEpoch 793, Loss: 0.0005024293204769492\nValidation Loss improved to 0.008891929872334003 Saving model 4\nEpoch 794, Loss: 0.00042533435043878853\nValidation Loss improved to 0.008887317962944508 Saving model 4\nEpoch 795, Loss: 0.00045014405623078346\nValidation Loss improved to 0.008882730267941952 Saving model 4\nEpoch 796, Loss: 0.0004563629045151174\nValidation Loss improved to 0.008878135122358799 Saving model 4\nEpoch 797, Loss: 0.0005584788741543889\nValidation Loss improved to 0.008874057792127132 Saving model 4\nEpoch 798, Loss: 0.0004544146067928523\nValidation Loss improved to 0.00886965449899435 Saving model 4\nEpoch 799, Loss: 0.0004151473112870008\nValidation Loss improved to 0.008865227922797203 Saving model 4\nEpoch 800, Loss: 0.00042230449616909027\nValidation Loss improved to 0.008860605768859386 Saving model 4\nEpoch 801, Loss: 0.0005341001669876277\nValidation Loss improved to 0.008856523782014847 Saving model 4\nEpoch 802, Loss: 0.0006058187573216856\nValidation Loss improved to 0.008852400816977024 Saving model 4\nEpoch 803, Loss: 0.0005163673777133226\nValidation Loss improved to 0.008848585188388824 Saving model 4\nEpoch 804, Loss: 0.0006467007915489376\nValidation Loss improved to 0.00884406827390194 Saving model 4\nEpoch 805, Loss: 0.00040990649722516537\nValidation Loss improved to 0.008839613758027554 Saving model 4\nEpoch 806, Loss: 0.0003846613399218768\nValidation Loss improved to 0.00883514154702425 Saving model 4\nEpoch 807, Loss: 0.0003572764398995787\nValidation Loss improved to 0.008830878883600235 Saving model 4\nEpoch 808, Loss: 0.0003715037601068616\nValidation Loss improved to 0.00882630329579115 Saving model 4\nEpoch 809, Loss: 0.00039243491482920945\nValidation Loss improved to 0.008821817114949226 Saving model 4\nEpoch 810, Loss: 0.000387383159250021\nValidation Loss improved to 0.008817446418106556 Saving model 4\nEpoch 811, Loss: 0.00032650469802320004\nValidation Loss improved to 0.00881296955049038 Saving model 4\nEpoch 812, Loss: 0.00030630998662672937\nValidation Loss improved to 0.008808473125100136 Saving model 4\nEpoch 813, Loss: 0.0005793693708255887\nValidation Loss improved to 0.00880398415029049 Saving model 4\nEpoch 814, Loss: 0.00043181184446439147\nValidation Loss improved to 0.008799634873867035 Saving model 4\nEpoch 815, Loss: 0.0005162186571396887\nValidation Loss improved to 0.008795150555670261 Saving model 4\nEpoch 816, Loss: 0.0004732903034891933\nValidation Loss improved to 0.008790604770183563 Saving model 4\nEpoch 817, Loss: 0.00039323230157606304\nValidation Loss improved to 0.008786536753177643 Saving model 4\nEpoch 818, Loss: 0.0005825385451316833\nValidation Loss improved to 0.008782480843365192 Saving model 4\nEpoch 819, Loss: 0.0005201703170314431\nValidation Loss improved to 0.008778243325650692 Saving model 4\nEpoch 820, Loss: 0.0004578076768666506\nValidation Loss improved to 0.008774034678936005 Saving model 4\nEpoch 821, Loss: 0.00035044417018070817\nValidation Loss improved to 0.008769674226641655 Saving model 4\nEpoch 822, Loss: 0.00043673597974702716\nValidation Loss improved to 0.008765239268541336 Saving model 4\nEpoch 823, Loss: 0.0004576930368784815\nValidation Loss improved to 0.0087610287591815 Saving model 4\nEpoch 824, Loss: 0.0003817106771748513\nValidation Loss improved to 0.00875663012266159 Saving model 4\nEpoch 825, Loss: 0.00040438250289298594\nValidation Loss improved to 0.008752217516303062 Saving model 4\nEpoch 826, Loss: 0.0005805340479128063\nValidation Loss improved to 0.008747822605073452 Saving model 4\nEpoch 827, Loss: 0.0005530185881070793\nValidation Loss improved to 0.00874351430684328 Saving model 4\nEpoch 828, Loss: 0.0004442814097274095\nValidation Loss improved to 0.008739198558032513 Saving model 4\nEpoch 829, Loss: 0.00046727413428016007\nValidation Loss improved to 0.008735112845897675 Saving model 4\nEpoch 830, Loss: 0.00044868930126540363\nValidation Loss improved to 0.008731161244213581 Saving model 4\nEpoch 831, Loss: 0.0004005384398624301\nValidation Loss improved to 0.008726847358047962 Saving model 4\nEpoch 832, Loss: 0.00046744916471652687\nValidation Loss improved to 0.008722945116460323 Saving model 4\nEpoch 833, Loss: 0.00041562929982319474\nValidation Loss improved to 0.008718837983906269 Saving model 4\nEpoch 834, Loss: 0.00036864844150841236\nValidation Loss improved to 0.008714742958545685 Saving model 4\nEpoch 835, Loss: 0.0003736305225174874\nValidation Loss improved to 0.008710545487701893 Saving model 4\nEpoch 836, Loss: 0.0003945602511521429\nValidation Loss improved to 0.008706525899469852 Saving model 4\nEpoch 837, Loss: 0.0003842374717351049\nValidation Loss improved to 0.008702280931174755 Saving model 4\nEpoch 838, Loss: 0.0003527929657138884\nValidation Loss improved to 0.008698138408362865 Saving model 4\nEpoch 839, Loss: 0.00047260543215088546\nValidation Loss improved to 0.008693944662809372 Saving model 4\nEpoch 840, Loss: 0.000676358409691602\nValidation Loss improved to 0.008689924143254757 Saving model 4\nEpoch 841, Loss: 0.0005217909929342568\nValidation Loss improved to 0.008685845881700516 Saving model 4\nEpoch 842, Loss: 0.000447164464276284\nValidation Loss improved to 0.008681817911565304 Saving model 4\nEpoch 843, Loss: 0.00035734043922275305\nValidation Loss improved to 0.008677949197590351 Saving model 4\nEpoch 844, Loss: 0.0003550219989847392\nValidation Loss improved to 0.008673821575939655 Saving model 4\nEpoch 845, Loss: 0.0004081745573785156\nValidation Loss improved to 0.008669739589095116 Saving model 4\nEpoch 846, Loss: 0.0003644407552201301\nValidation Loss improved to 0.008665816858410835 Saving model 4\nEpoch 847, Loss: 0.0004510407743509859\nValidation Loss improved to 0.008661746978759766 Saving model 4\nEpoch 848, Loss: 0.0005526512977667153\nValidation Loss improved to 0.008657771162688732 Saving model 4\nEpoch 849, Loss: 0.00041982694528996944\nValidation Loss improved to 0.008653881959617138 Saving model 4\nEpoch 850, Loss: 0.0004658444377128035\nValidation Loss improved to 0.00864984281361103 Saving model 4\nEpoch 851, Loss: 0.00035966371069662273\nValidation Loss improved to 0.008645760826766491 Saving model 4\nEpoch 852, Loss: 0.0003592272405512631\nValidation Loss improved to 0.008641727268695831 Saving model 4\nEpoch 853, Loss: 0.00038811645936220884\nValidation Loss improved to 0.008637621998786926 Saving model 4\nEpoch 854, Loss: 0.0003773879143409431\nValidation Loss improved to 0.008633581921458244 Saving model 4\nEpoch 855, Loss: 0.00031633529579266906\nValidation Loss improved to 0.008629504591226578 Saving model 4\nEpoch 856, Loss: 0.00041608381434343755\nValidation Loss improved to 0.008625403046607971 Saving model 4\nEpoch 857, Loss: 0.0006133720162324607\nValidation Loss improved to 0.008621457032859325 Saving model 4\nEpoch 858, Loss: 0.0005299467593431473\nValidation Loss improved to 0.008617427200078964 Saving model 4\nEpoch 859, Loss: 0.00036117970012128353\nValidation Loss improved to 0.008613428100943565 Saving model 4\nEpoch 860, Loss: 0.0004004470829386264\nValidation Loss improved to 0.00860940758138895 Saving model 4\nEpoch 861, Loss: 0.00041708676144480705\nValidation Loss improved to 0.008605501614511013 Saving model 4\nEpoch 862, Loss: 0.0004087160050403327\nValidation Loss improved to 0.008601486682891846 Saving model 4\nEpoch 863, Loss: 0.00043964190990664065\nValidation Loss improved to 0.008597513660788536 Saving model 4\nEpoch 864, Loss: 0.00041668256744742393\nValidation Loss improved to 0.00859344657510519 Saving model 4\nEpoch 865, Loss: 0.00038275346742011607\nValidation Loss improved to 0.008589831180870533 Saving model 4\nEpoch 866, Loss: 0.0004557196225505322\nValidation Loss improved to 0.00858592800796032 Saving model 4\nEpoch 867, Loss: 0.0003798846446443349\nValidation Loss improved to 0.008582229726016521 Saving model 4\nEpoch 868, Loss: 0.00035950905294157565\nValidation Loss improved to 0.008578302338719368 Saving model 4\nEpoch 869, Loss: 0.0003495371784083545\nValidation Loss improved to 0.008574597537517548 Saving model 4\nEpoch 870, Loss: 0.0003506501961965114\nValidation Loss improved to 0.008570721372961998 Saving model 4\nEpoch 871, Loss: 0.0003633523592725396\nValidation Loss improved to 0.00856710597872734 Saving model 4\nEpoch 872, Loss: 0.0005308652180247009\nValidation Loss improved to 0.008563998155295849 Saving model 4\nEpoch 873, Loss: 0.0005508153117261827\nValidation Loss improved to 0.008560321293771267 Saving model 4\nEpoch 874, Loss: 0.00036267662653699517\nValidation Loss improved to 0.008556450717151165 Saving model 4\nEpoch 875, Loss: 0.0005050585605204105\nValidation Loss improved to 0.008552664890885353 Saving model 4\nEpoch 876, Loss: 0.00038534283521585166\nValidation Loss improved to 0.008548886515200138 Saving model 4\nEpoch 877, Loss: 0.0003622146905399859\nValidation Loss improved to 0.008545199409127235 Saving model 4\nEpoch 878, Loss: 0.00033753682509995997\nValidation Loss improved to 0.008541340008378029 Saving model 4\nEpoch 879, Loss: 0.0003364828589837998\nValidation Loss improved to 0.008537475019693375 Saving model 4\nEpoch 880, Loss: 0.00034697982482612133\nValidation Loss improved to 0.008533797226846218 Saving model 4\nEpoch 881, Loss: 0.0003610116255003959\nValidation Loss improved to 0.008530175313353539 Saving model 4\nEpoch 882, Loss: 0.00035162302083335817\nValidation Loss improved to 0.008526308462023735 Saving model 4\nEpoch 883, Loss: 0.0003292413312010467\nValidation Loss improved to 0.008522543124854565 Saving model 4\nEpoch 884, Loss: 0.0004138943913858384\nValidation Loss improved to 0.008518917486071587 Saving model 4\nEpoch 885, Loss: 0.0003804955631494522\nValidation Loss improved to 0.008515146560966969 Saving model 4\nEpoch 886, Loss: 0.0003626902471296489\nValidation Loss improved to 0.008511585183441639 Saving model 4\nEpoch 887, Loss: 0.0004150106105953455\nValidation Loss improved to 0.008507930673658848 Saving model 4\nEpoch 888, Loss: 0.0003168654511682689\nValidation Loss improved to 0.008504238910973072 Saving model 4\nEpoch 889, Loss: 0.0003339131362736225\nValidation Loss improved to 0.008500644005835056 Saving model 4\nEpoch 890, Loss: 0.0003963349445257336\nValidation Loss improved to 0.008496908470988274 Saving model 4\nEpoch 891, Loss: 0.000408350198995322\nValidation Loss improved to 0.008493145927786827 Saving model 4\nEpoch 892, Loss: 0.00038940709782764316\nValidation Loss improved to 0.008489458821713924 Saving model 4\nEpoch 893, Loss: 0.00040842071757651865\nValidation Loss improved to 0.00848589651286602 Saving model 4\nEpoch 894, Loss: 0.0004445669474080205\nValidation Loss improved to 0.008482157252728939 Saving model 4\nEpoch 895, Loss: 0.0005034571513533592\nValidation Loss improved to 0.00847858376801014 Saving model 4\nEpoch 896, Loss: 0.0004167421720921993\nValidation Loss improved to 0.00847481470555067 Saving model 4\nEpoch 897, Loss: 0.00032486021518707275\nValidation Loss improved to 0.008471098728477955 Saving model 4\nEpoch 898, Loss: 0.00034456257708370686\nValidation Loss improved to 0.008467386476695538 Saving model 4\nEpoch 899, Loss: 0.0003427084884606302\nValidation Loss improved to 0.008463628590106964 Saving model 4\nEpoch 900, Loss: 0.00036273454315960407\nValidation Loss improved to 0.008460115641355515 Saving model 4\nEpoch 901, Loss: 0.00033595552667975426\nValidation Loss improved to 0.008456573821604252 Saving model 4\nEpoch 902, Loss: 0.0003783497086260468\nValidation Loss improved to 0.008452772162854671 Saving model 4\nEpoch 903, Loss: 0.0004179409006610513\nValidation Loss improved to 0.008449048735201359 Saving model 4\nEpoch 904, Loss: 0.00035645393654704094\nValidation Loss improved to 0.00844530574977398 Saving model 4\nEpoch 905, Loss: 0.00045115684042684734\nValidation Loss improved to 0.008442213758826256 Saving model 4\nEpoch 906, Loss: 0.0005417268257588148\nValidation Loss improved to 0.008438517339527607 Saving model 4\nEpoch 907, Loss: 0.0004866506496910006\nValidation Loss improved to 0.008434935472905636 Saving model 4\nEpoch 908, Loss: 0.0004297301056794822\nValidation Loss improved to 0.00843135081231594 Saving model 4\nEpoch 909, Loss: 0.0004343604377936572\nValidation Loss improved to 0.008427685126662254 Saving model 4\nEpoch 910, Loss: 0.0004794819396920502\nValidation Loss improved to 0.008424384519457817 Saving model 4\nEpoch 911, Loss: 0.0004185682628303766\nValidation Loss improved to 0.008420865051448345 Saving model 4\nEpoch 912, Loss: 0.00037654631887562573\nValidation Loss improved to 0.008417189121246338 Saving model 4\nEpoch 913, Loss: 0.00031750378548167646\nValidation Loss improved to 0.008413601666688919 Saving model 4\nEpoch 914, Loss: 0.00035550922621041536\nValidation Loss improved to 0.008409897796809673 Saving model 4\nEpoch 915, Loss: 0.0003575574664864689\nValidation Loss improved to 0.008406348526477814 Saving model 4\nEpoch 916, Loss: 0.00036709479172714055\nValidation Loss improved to 0.00840276200324297 Saving model 4\nEpoch 917, Loss: 0.0003307106962893158\nValidation Loss improved to 0.00839910190552473 Saving model 4\nEpoch 918, Loss: 0.0003502912004478276\nValidation Loss improved to 0.008395548909902573 Saving model 4\nEpoch 919, Loss: 0.0003734225465450436\nValidation Loss improved to 0.008391998708248138 Saving model 4\nEpoch 920, Loss: 0.00031259431852959096\nValidation Loss improved to 0.008388720452785492 Saving model 4\nEpoch 921, Loss: 0.0004620123072527349\nValidation Loss improved to 0.008385172113776207 Saving model 4\nEpoch 922, Loss: 0.0003312688786536455\nValidation Loss improved to 0.008381800726056099 Saving model 4\nEpoch 923, Loss: 0.00041898852214217186\nValidation Loss improved to 0.0083782859146595 Saving model 4\nEpoch 924, Loss: 0.0004765794728882611\nValidation Loss improved to 0.008374804630875587 Saving model 4\nEpoch 925, Loss: 0.00034357167896814644\nValidation Loss improved to 0.008371230214834213 Saving model 4\nEpoch 926, Loss: 0.0003129080287180841\nValidation Loss improved to 0.008367662318050861 Saving model 4\nEpoch 927, Loss: 0.00036504995659925044\nValidation Loss improved to 0.008364247158169746 Saving model 4\nEpoch 928, Loss: 0.00034494177089072764\nValidation Loss improved to 0.00836087018251419 Saving model 4\nEpoch 929, Loss: 0.0003101424954365939\nValidation Loss improved to 0.00835732463747263 Saving model 4\nEpoch 930, Loss: 0.00034139613853767514\nValidation Loss improved to 0.008353839628398418 Saving model 4\nEpoch 931, Loss: 0.0003299917734693736\nValidation Loss improved to 0.008350368589162827 Saving model 4\nEpoch 932, Loss: 0.0003362766874488443\nValidation Loss improved to 0.008346965536475182 Saving model 4\nEpoch 933, Loss: 0.0003474580298643559\nValidation Loss improved to 0.008343508467078209 Saving model 4\nEpoch 934, Loss: 0.0003279187367297709\nValidation Loss improved to 0.00834005419164896 Saving model 4\nEpoch 935, Loss: 0.00035142997512593865\nValidation Loss improved to 0.008336519822478294 Saving model 4\nEpoch 936, Loss: 0.0004259906418155879\nValidation Loss improved to 0.00833335891366005 Saving model 4\nEpoch 937, Loss: 0.0004538161738310009\nValidation Loss improved to 0.008330000564455986 Saving model 4\nEpoch 938, Loss: 0.0005041377735324204\nValidation Loss improved to 0.008326909504830837 Saving model 4\nEpoch 939, Loss: 0.0004272901569493115\nValidation Loss improved to 0.008323827758431435 Saving model 4\nEpoch 940, Loss: 0.00042271800339221954\nValidation Loss improved to 0.008320403285324574 Saving model 4\nEpoch 941, Loss: 0.0003986860974691808\nValidation Loss improved to 0.008317003026604652 Saving model 4\nEpoch 942, Loss: 0.0003826308820862323\nValidation Loss improved to 0.008314142003655434 Saving model 4\nEpoch 943, Loss: 0.0007139982772059739\nValidation Loss improved to 0.008310801349580288 Saving model 4\nEpoch 944, Loss: 0.0005130809149704874\nValidation Loss improved to 0.008307370357215405 Saving model 4\nEpoch 945, Loss: 0.00047176037332974374\nValidation Loss improved to 0.008304066024720669 Saving model 4\nEpoch 946, Loss: 0.00034422113094478846\nValidation Loss improved to 0.008300655521452427 Saving model 4\nEpoch 947, Loss: 0.0003089897509198636\nValidation Loss improved to 0.008297193795442581 Saving model 4\nEpoch 948, Loss: 0.0003571989363990724\nValidation Loss improved to 0.00829377118498087 Saving model 4\nEpoch 949, Loss: 0.00038221024442464113\nValidation Loss improved to 0.008290532045066357 Saving model 4\nEpoch 950, Loss: 0.00038166571175679564\nValidation Loss improved to 0.008287208154797554 Saving model 4\nEpoch 951, Loss: 0.0003631280269473791\nValidation Loss improved to 0.008283855393528938 Saving model 4\nEpoch 952, Loss: 0.0003609310952015221\nValidation Loss improved to 0.008280734531581402 Saving model 4\nEpoch 953, Loss: 0.0003604257362894714\nValidation Loss improved to 0.008277528919279575 Saving model 4\nEpoch 954, Loss: 0.0005838365759700537\nValidation Loss improved to 0.008274482563138008 Saving model 4\nEpoch 955, Loss: 0.0004380520840641111\nValidation Loss improved to 0.008271127939224243 Saving model 4\nEpoch 956, Loss: 0.0003474573022685945\nValidation Loss improved to 0.00826783012598753 Saving model 4\nEpoch 957, Loss: 0.0003906449128407985\nValidation Loss improved to 0.008264743722975254 Saving model 4\nEpoch 958, Loss: 0.0003247752320021391\nValidation Loss improved to 0.008261359296739101 Saving model 4\nEpoch 959, Loss: 0.0002779028145596385\nValidation Loss improved to 0.008257986046373844 Saving model 4\nEpoch 960, Loss: 0.000327485118759796\nValidation Loss improved to 0.008254718966782093 Saving model 4\nEpoch 961, Loss: 0.0003329843748360872\nValidation Loss improved to 0.00825160276144743 Saving model 4\nEpoch 962, Loss: 0.00032631494104862213\nValidation Loss improved to 0.008248301222920418 Saving model 4\nEpoch 963, Loss: 0.0002809520228765905\nValidation Loss improved to 0.008245320059359074 Saving model 4\nEpoch 964, Loss: 0.0004694049130193889\nValidation Loss improved to 0.00824224017560482 Saving model 4\nEpoch 965, Loss: 0.0004029640113003552\nValidation Loss improved to 0.008239107206463814 Saving model 4\nEpoch 966, Loss: 0.0004876593593508005\nValidation Loss improved to 0.008235949091613293 Saving model 4\nEpoch 967, Loss: 0.00041661300929263234\nValidation Loss improved to 0.008232587948441505 Saving model 4\nEpoch 968, Loss: 0.0004124377737753093\nValidation Loss improved to 0.00822949968278408 Saving model 4\nEpoch 969, Loss: 0.0003331576008349657\nValidation Loss improved to 0.008226296864449978 Saving model 4\nEpoch 970, Loss: 0.0003221242513973266\nValidation Loss improved to 0.008223206736147404 Saving model 4\nEpoch 971, Loss: 0.00025942258071154356\nValidation Loss improved to 0.008219962008297443 Saving model 4\nEpoch 972, Loss: 0.0003554268041625619\nValidation Loss improved to 0.00821673683822155 Saving model 4\nEpoch 973, Loss: 0.0004735615511890501\nValidation Loss improved to 0.008213500492274761 Saving model 4\nEpoch 974, Loss: 0.0003730481257662177\nValidation Loss improved to 0.008210316300392151 Saving model 4\nEpoch 975, Loss: 0.00039234478026628494\nValidation Loss improved to 0.00820751953870058 Saving model 4\nEpoch 976, Loss: 0.00034702723496593535\nValidation Loss improved to 0.008204326964914799 Saving model 4\nEpoch 977, Loss: 0.0003521704929880798\nValidation Loss improved to 0.008201147429645061 Saving model 4\nEpoch 978, Loss: 0.00034830509684979916\nValidation Loss improved to 0.008198240771889687 Saving model 4\nEpoch 979, Loss: 0.00041173832141794264\nValidation Loss improved to 0.008195120841264725 Saving model 4\nEpoch 980, Loss: 0.0002768209669739008\nValidation Loss improved to 0.008192270062863827 Saving model 4\nEpoch 981, Loss: 0.0004316178092267364\nValidation Loss improved to 0.008189198561012745 Saving model 4\nEpoch 982, Loss: 0.0003030424122698605\nValidation Loss improved to 0.008186093531548977 Saving model 4\nEpoch 983, Loss: 0.0003469540970399976\nValidation Loss improved to 0.008182968944311142 Saving model 4\nEpoch 984, Loss: 0.00028181253583170474\nValidation Loss improved to 0.008179860189557076 Saving model 4\nEpoch 985, Loss: 0.00039247662061825395\nValidation Loss improved to 0.008176719769835472 Saving model 4\nEpoch 986, Loss: 0.0003784037835430354\nValidation Loss improved to 0.008173603564500809 Saving model 4\nEpoch 987, Loss: 0.00038869839045219123\nValidation Loss improved to 0.008170628920197487 Saving model 4\nEpoch 988, Loss: 0.0003656532207969576\nValidation Loss improved to 0.008167573250830173 Saving model 4\nEpoch 989, Loss: 0.00035331485560163856\nValidation Loss improved to 0.008164443075656891 Saving model 4\nEpoch 990, Loss: 0.0003323622513562441\nValidation Loss improved to 0.008161271922290325 Saving model 4\nEpoch 991, Loss: 0.00033811715547926724\nValidation Loss improved to 0.008158095180988312 Saving model 4\nEpoch 992, Loss: 0.00039944861782714725\nValidation Loss improved to 0.008155140094459057 Saving model 4\nEpoch 993, Loss: 0.00036674333387054503\nValidation Loss improved to 0.008152078837156296 Saving model 4\nEpoch 994, Loss: 0.00040082758641801775\nValidation Loss improved to 0.008149124681949615 Saving model 4\nEpoch 995, Loss: 0.000327237241435796\nValidation Loss improved to 0.008146225474774837 Saving model 4\nEpoch 996, Loss: 0.0003545393410604447\nValidation Loss improved to 0.008143042214214802 Saving model 4\nEpoch 997, Loss: 0.0003124671638943255\nValidation Loss improved to 0.008139917626976967 Saving model 4\nEpoch 998, Loss: 0.0003169934789184481\nValidation Loss improved to 0.008136853575706482 Saving model 4\nEpoch 999, Loss: 0.0003447145572863519\nValidation Loss improved to 0.008133797906339169 Saving model 4\n"
    }
   ],
   "source": [
    "for fold, (train_index, test_index) in enumerate(kf.split(TRAIN_PATIENTS)):\n",
    "    model = PulmonaryModel(len(FV))\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    df_train = train_df.iloc[train_index].reset_index(drop=True)\n",
    "    df_valid = train_df.iloc[test_index].reset_index(drop=True)\n",
    "\n",
    "    train_dataset = PulmonaryDataset(df_train, FV)\n",
    "    valid_dataset = PulmonaryDataset(df_valid, FV)\n",
    "\n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=10,\n",
    "        shuffle=True,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    valid_data_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=4,\n",
    "        shuffle=False,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.05, verbose=True)\n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "    valid_loss = AverageMeter()\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_one_epoch(model, train_data_loader, optimizer, epoch)\n",
    "        eval_one_epoch(model, valid_data_loader, valid_loss, lr_scheduler)\n",
    "\n",
    "        if valid_loss.avg < best_valid_loss:\n",
    "            best_valid_loss = valid_loss.avg\n",
    "            print(f'Validation Loss improved to {valid_loss.avg } Saving model {fold}')\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, os.path.join(CONFIG.CFG.DATA.MODELS_OUT, f\"model_fold_{fold}.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG.upload_to_kaggle(\"osicqrmodel\", \"OSIC QR Model\", new=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for fold in range(K_FOLDS):\n",
    "    model = PulmonaryModel(len(FV))\n",
    "    model = model.to(DEVICE)\n",
    "    checkpoint = torch.load(os.path.join(CONFIG.CFG.DATA.MODELS_OUT, f\"model_fold_{fold}.pt\"))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = PulmonaryDataset(sub_df, FV)\n",
    "test_data_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "avg_preds = np.zeros((len(test_dataset), len(QUANTILES)))\n",
    "with torch.no_grad():\n",
    "    for model in models:\n",
    "        preds = []\n",
    "        for j, test_data in enumerate(test_data_loader):\n",
    "            features = test_data['features']\n",
    "            targets = test_data['target']\n",
    "\n",
    "            features = features.to(DEVICE).float()\n",
    "            targets = targets.to(DEVICE).float()\n",
    "\n",
    "            out = model(features)\n",
    "            preds.append(out)\n",
    "        preds = torch.cat(preds, dim=0).cpu().numpy()\n",
    "        avg_preds += preds\n",
    "avg_preds /= len(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0.38153635, 0.38537843, 0.38942896],\n       [0.38147265, 0.38530883, 0.38937286],\n       [0.38144171, 0.38527464, 0.38934845],\n       ...,\n       [0.37112791, 0.37638941, 0.38357923],\n       [0.37111468, 0.37639861, 0.38358554],\n       [0.37112247, 0.37642929, 0.38361248]])"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "avg_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse the scaling operation for FVC\n",
    "avg_preds -= MIN_MAX_SCALER.min_[SCALE_COLUMNS.index('FVC')]\n",
    "avg_preds /= MIN_MAX_SCALER.scale_[SCALE_COLUMNS.index('FVC')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[2952.92055688, 2974.32862463, 2996.89817224],\n       [2952.56562335, 2973.9407783 , 2996.58555043],\n       [2952.39318817, 2973.75027595, 2996.44958169],\n       ...,\n       [2894.9247113 , 2924.24181521, 2964.30347128],\n       [2894.85098131, 2924.29306087, 2964.33864248],\n       [2894.89438901, 2924.46400154, 2964.48872619]])"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "avg_preds[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}