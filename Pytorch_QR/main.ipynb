{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "if os.path.abspath(os.pardir) not in sys.path:\n",
    "    sys.path.insert(0, os.path.abspath(os.pardir))\n",
    "import CONFIG\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torch.nn.functional as F\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = CONFIG.CFG.DATA.BASE\n",
    "K_FOLDS = 5\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 3e-3\n",
    "NUM_EPOCHS = 1000\n",
    "ES_PATIENCE = 20\n",
    "QUANTILES = (0.2, 0.5, 0.8)\n",
    "SCALE_COLUMNS = ['Weeks', 'FVC', 'Percent', 'Age']\n",
    "SEX_COLUMNS = ['Male', 'Female']\n",
    "SMOKING_STATUS_COLUMNS = ['Currently smokes', 'Ex-smoker', 'Never smoked']\n",
    "FV = SEX_COLUMNS + SMOKING_STATUS_COLUMNS + SCALE_COLUMNS\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_MAX_SCALER = preprocessing.MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\"))\n",
    "train_df.drop_duplicates(keep=False, inplace=True, subset=['Patient', 'Weeks'])\n",
    "test_df = pd.read_csv(os.path.join(DATA_DIR, \"test.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[SCALE_COLUMNS] = MIN_MAX_SCALER.fit_transform(train_df[SCALE_COLUMNS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert categoricals into dummies\n",
    "train_df['Sex'] = pd.Categorical(train_df['Sex'], categories=SEX_COLUMNS)\n",
    "train_df['SmokingStatus'] = pd.Categorical(train_df['SmokingStatus'], categories=SMOKING_STATUS_COLUMNS)\n",
    "train_df = train_df.join(pd.get_dummies(train_df['Sex']))\n",
    "train_df = train_df.join(pd.get_dummies(train_df['SmokingStatus']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# REMOVE THE ONES FROM THE TRAIN_DF THAT ARE PRESENT IN TEST_DF AS WELL\n",
    "TEST_PATIENTS = test_df['Patient'].unique().tolist()\n",
    "valid_df = train_df[train_df['Patient'].isin(TEST_PATIENTS)]\n",
    "train_df = train_df[~train_df['Patient'].isin(TEST_PATIENTS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Patient</th>\n      <th>Weeks</th>\n      <th>FVC</th>\n      <th>Percent</th>\n      <th>Age</th>\n      <th>Sex</th>\n      <th>SmokingStatus</th>\n      <th>Male</th>\n      <th>Female</th>\n      <th>Currently smokes</th>\n      <th>Ex-smoker</th>\n      <th>Never smoked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1504</th>\n      <td>ID00419637202311204720264</td>\n      <td>0.079710</td>\n      <td>0.393575</td>\n      <td>0.332421</td>\n      <td>0.615385</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1505</th>\n      <td>ID00419637202311204720264</td>\n      <td>0.086957</td>\n      <td>0.364681</td>\n      <td>0.302311</td>\n      <td>0.615385</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1506</th>\n      <td>ID00419637202311204720264</td>\n      <td>0.101449</td>\n      <td>0.351041</td>\n      <td>0.288097</td>\n      <td>0.615385</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1507</th>\n      <td>ID00419637202311204720264</td>\n      <td>0.108696</td>\n      <td>0.339555</td>\n      <td>0.276128</td>\n      <td>0.615385</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1508</th>\n      <td>ID00419637202311204720264</td>\n      <td>0.130435</td>\n      <td>0.342965</td>\n      <td>0.279682</td>\n      <td>0.615385</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                        Patient     Weeks  ...  Ex-smoker  Never smoked\n1504  ID00419637202311204720264  0.079710  ...          1             0\n1505  ID00419637202311204720264  0.086957  ...          1             0\n1506  ID00419637202311204720264  0.101449  ...          1             0\n1507  ID00419637202311204720264  0.108696  ...          1             0\n1508  ID00419637202311204720264  0.130435  ...          1             0\n\n[5 rows x 12 columns]"
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "valid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Patient</th>\n      <th>Weeks</th>\n      <th>FVC</th>\n      <th>Percent</th>\n      <th>Age</th>\n      <th>Sex</th>\n      <th>SmokingStatus</th>\n      <th>Male</th>\n      <th>Female</th>\n      <th>Currently smokes</th>\n      <th>Ex-smoker</th>\n      <th>Never smoked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ID00007637202177411956430</td>\n      <td>0.007246</td>\n      <td>0.267050</td>\n      <td>0.236393</td>\n      <td>0.769231</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ID00007637202177411956430</td>\n      <td>0.072464</td>\n      <td>0.248923</td>\n      <td>0.215941</td>\n      <td>0.769231</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ID00007637202177411956430</td>\n      <td>0.086957</td>\n      <td>0.221464</td>\n      <td>0.184960</td>\n      <td>0.769231</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ID00007637202177411956430</td>\n      <td>0.101449</td>\n      <td>0.236360</td>\n      <td>0.201767</td>\n      <td>0.769231</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ID00007637202177411956430</td>\n      <td>0.115942</td>\n      <td>0.222900</td>\n      <td>0.186580</td>\n      <td>0.769231</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                     Patient     Weeks  ...  Ex-smoker  Never smoked\n0  ID00007637202177411956430  0.007246  ...          1             0\n1  ID00007637202177411956430  0.072464  ...          1             0\n2  ID00007637202177411956430  0.086957  ...          1             0\n3  ID00007637202177411956430  0.101449  ...          1             0\n4  ID00007637202177411956430  0.115942  ...          1             0\n\n[5 rows x 12 columns]"
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Patient_Week</th>\n      <th>FVC</th>\n      <th>Confidence</th>\n      <th>Patient</th>\n      <th>Weeks</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ID00419637202311204720264_-12</td>\n      <td>2000</td>\n      <td>100</td>\n      <td>ID00419637202311204720264</td>\n      <td>-12</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ID00421637202311550012437_-12</td>\n      <td>2000</td>\n      <td>100</td>\n      <td>ID00421637202311550012437</td>\n      <td>-12</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ID00422637202311677017371_-12</td>\n      <td>2000</td>\n      <td>100</td>\n      <td>ID00422637202311677017371</td>\n      <td>-12</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ID00423637202312137826377_-12</td>\n      <td>2000</td>\n      <td>100</td>\n      <td>ID00423637202312137826377</td>\n      <td>-12</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ID00426637202313170790466_-12</td>\n      <td>2000</td>\n      <td>100</td>\n      <td>ID00426637202313170790466</td>\n      <td>-12</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                    Patient_Week   FVC  ...                    Patient Weeks\n0  ID00419637202311204720264_-12  2000  ...  ID00419637202311204720264   -12\n1  ID00421637202311550012437_-12  2000  ...  ID00421637202311550012437   -12\n2  ID00422637202311677017371_-12  2000  ...  ID00422637202311677017371   -12\n3  ID00423637202312137826377_-12  2000  ...  ID00423637202312137826377   -12\n4  ID00426637202313170790466_-12  2000  ...  ID00426637202313170790466   -12\n\n[5 rows x 5 columns]"
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "sub_df = pd.read_csv(os.path.join(DATA_DIR, \"sample_submission.csv\"))\n",
    "sub_df['Patient'] = sub_df['Patient_Week'].apply(lambda x: x.split('_')[0])\n",
    "sub_df['Weeks'] = sub_df['Patient_Week'].apply(lambda x: int(x.split('_')[-1]))\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = sub_df.drop('FVC', axis=1).merge(test_df.drop('Weeks', axis=1), on='Patient')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have to make it categorical coz sub's sex column has males only\n",
    "sub_df['Sex'] = pd.Categorical(sub_df['Sex'], categories=SEX_COLUMNS)\n",
    "sub_df['SmokingStatus'] = pd.Categorical(sub_df['SmokingStatus'], categories=SMOKING_STATUS_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = sub_df.join(pd.get_dummies(sub_df['Sex']))\n",
    "sub_df = sub_df.join(pd.get_dummies(sub_df['SmokingStatus']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Patient_Week</th>\n      <th>Confidence</th>\n      <th>Patient</th>\n      <th>Weeks</th>\n      <th>FVC</th>\n      <th>Percent</th>\n      <th>Age</th>\n      <th>Sex</th>\n      <th>SmokingStatus</th>\n      <th>Male</th>\n      <th>Female</th>\n      <th>Currently smokes</th>\n      <th>Ex-smoker</th>\n      <th>Never smoked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ID00419637202311204720264_-12</td>\n      <td>100</td>\n      <td>ID00419637202311204720264</td>\n      <td>-0.050725</td>\n      <td>0.393575</td>\n      <td>0.332421</td>\n      <td>0.615385</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ID00419637202311204720264_-11</td>\n      <td>100</td>\n      <td>ID00419637202311204720264</td>\n      <td>-0.043478</td>\n      <td>0.393575</td>\n      <td>0.332421</td>\n      <td>0.615385</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ID00419637202311204720264_-10</td>\n      <td>100</td>\n      <td>ID00419637202311204720264</td>\n      <td>-0.036232</td>\n      <td>0.393575</td>\n      <td>0.332421</td>\n      <td>0.615385</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ID00419637202311204720264_-9</td>\n      <td>100</td>\n      <td>ID00419637202311204720264</td>\n      <td>-0.028986</td>\n      <td>0.393575</td>\n      <td>0.332421</td>\n      <td>0.615385</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ID00419637202311204720264_-8</td>\n      <td>100</td>\n      <td>ID00419637202311204720264</td>\n      <td>-0.021739</td>\n      <td>0.393575</td>\n      <td>0.332421</td>\n      <td>0.615385</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                    Patient_Week  Confidence  ... Ex-smoker  Never smoked\n0  ID00419637202311204720264_-12         100  ...         1             0\n1  ID00419637202311204720264_-11         100  ...         1             0\n2  ID00419637202311204720264_-10         100  ...         1             0\n3   ID00419637202311204720264_-9         100  ...         1             0\n4   ID00419637202311204720264_-8         100  ...         1             0\n\n[5 rows x 14 columns]"
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "sub_df[SCALE_COLUMNS] = MIN_MAX_SCALER.transform(sub_df[SCALE_COLUMNS])\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PulmonaryDataset(Dataset):\n",
    "    def __init__(self, df, FV, test=False):\n",
    "        self.df = df\n",
    "        self.test = test\n",
    "        self.FV = FV\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'features': torch.tensor(self.df[self.FV].iloc[idx].values),\n",
    "            'target': torch.tensor(self.df['FVC'].iloc[idx])\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PulmonaryModel(nn.Module):\n",
    "    def __init__(self, in_features=9, out_quantiles=3):\n",
    "        super(PulmonaryModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features, 100)\n",
    "        self.fc2 = nn.Linear(100, 100)\n",
    "        self.fc3 = nn.Linear(100, out_quantiles)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_loss(preds, target, quantiles):\n",
    "    assert not target.requires_grad\n",
    "    assert preds.size(0) == target.size(0)\n",
    "    losses = []\n",
    "    for i, q in enumerate(quantiles):\n",
    "        errors = target - preds[:, i]\n",
    "        losses.append(torch.max((q - 1) * errors, q * errors).unsqueeze(1))\n",
    "    loss = torch.mean(torch.sum(torch.cat(losses, dim=1), dim=1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PulmonaryDataset(train_df, FV)\n",
    "valid_dataset = PulmonaryDataset(valid_df, FV)\n",
    "test_dataset = PulmonaryDataset(sub_df, FV)\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=10,\n",
    "    drop_last=False,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")\n",
    "valid_data_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=2\n",
    ")\n",
    "test_data_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=10,\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "PulmonaryModel(\n  (fc1): Linear(in_features=9, out_features=100, bias=True)\n  (fc2): Linear(in_features=100, out_features=100, bias=True)\n  (fc3): Linear(in_features=100, out_features=3, bias=True)\n)"
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "source": [
    "model = PulmonaryModel(len(FV))\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.05, verbose=True)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 0, Loss 0.08648233115673065 \n Val Loss, 0.5228700637817383\nEpoch 1, Loss 0.02328229509294033 \n Val Loss, 0.30478763580322266\nEpoch 2, Loss 0.0176162701100111 \n Val Loss, 0.24272899329662323\nEpoch 3, Loss 0.011500070802867413 \n Val Loss, 0.08962269127368927\nEpoch 4, Loss 0.008095852099359035 \n Val Loss, 0.06807706505060196\nEpoch 5, Loss 0.007904370315372944 \n Val Loss, 0.09128690510988235\nEpoch 6, Loss 0.007913289591670036 \n Val Loss, 0.03876123204827309\nEpoch 7, Loss 0.0073406752198934555 \n Val Loss, 0.11662084609270096\nEpoch 8, Loss 0.006621276494115591 \n Val Loss, 0.0756915807723999\nEpoch 9, Loss 0.005653091240674257 \n Val Loss, 0.027989469468593597\nEpoch 10, Loss 0.005888788960874081 \n Val Loss, 0.04633215069770813\nEpoch 11, Loss 0.005700851324945688 \n Val Loss, 0.025716617703437805\nEpoch 12, Loss 0.004757427144795656 \n Val Loss, 0.036921191960573196\nEpoch 13, Loss 0.004415890201926231 \n Val Loss, 0.03669958934187889\nEpoch 14, Loss 0.004617221653461456 \n Val Loss, 0.02638453058898449\nEpoch 15, Loss 0.004904172848910093 \n Val Loss, 0.028316538780927658\nEpoch 16, Loss 0.005336792673915625 \n Val Loss, 0.06642104685306549\nEpoch 17, Loss 0.005238730926066637 \n Val Loss, 0.018803726881742477\nEpoch 18, Loss 0.004888331517577171 \n Val Loss, 0.050837937742471695\nEpoch 19, Loss 0.0042222230695188046 \n Val Loss, 0.05486200377345085\nEpoch 20, Loss 0.003932767082005739 \n Val Loss, 0.018765445798635483\nEpoch 21, Loss 0.003930370323359966 \n Val Loss, 0.08110098540782928\nEpoch 22, Loss 0.003944167401641607 \n Val Loss, 0.06865426898002625\nEpoch 23, Loss 0.00405733659863472 \n Val Loss, 0.15298041701316833\nEpoch 24, Loss 0.005464112851768732 \n Val Loss, 0.06140630319714546\nEpoch 25, Loss 0.00424172030761838 \n Val Loss, 0.030370905995368958\nEpoch 26, Loss 0.0036187823861837387 \n Val Loss, 0.070359006524086\nEpoch 27, Loss 0.004787954967468977 \n Val Loss, 0.032838374376297\nEpoch 28, Loss 0.0038986867293715477 \n Val Loss, 0.054415807127952576\nEpoch 29, Loss 0.0039039505645632744 \n Val Loss, 0.07622150331735611\nEpoch 30, Loss 0.004158083815127611 \n Val Loss, 0.09631292521953583\nEpoch    32: reducing learning rate of group 0 to 5.0000e-05.\nEpoch 31, Loss 0.004571264144033194 \n Val Loss, 0.064828060567379\nEpoch 32, Loss 0.0015565481735393405 \n Val Loss, 0.005785513203591108\nEpoch 33, Loss 0.0008605583570897579 \n Val Loss, 0.006217324640601873\nEpoch 34, Loss 0.0007977323839440942 \n Val Loss, 0.007573210634291172\nEpoch 35, Loss 0.0007319771684706211 \n Val Loss, 0.005220702383667231\nEpoch 36, Loss 0.0006976058939471841 \n Val Loss, 0.005117535591125488\nEpoch 37, Loss 0.0006904061883687973 \n Val Loss, 0.004421302117407322\nEpoch 38, Loss 0.000662890262901783 \n Val Loss, 0.00525357760488987\nEpoch 39, Loss 0.0006754457717761397 \n Val Loss, 0.005112443119287491\nEpoch 40, Loss 0.0006423626327887177 \n Val Loss, 0.006757201626896858\nEpoch 41, Loss 0.0006232233718037605 \n Val Loss, 0.009576171636581421\nEpoch 42, Loss 0.0006921864114701748 \n Val Loss, 0.006587809883058071\nEpoch 43, Loss 0.0006140625337138772 \n Val Loss, 0.007761032320559025\nEpoch 44, Loss 0.0006192837026901543 \n Val Loss, 0.004800319671630859\nEpoch 45, Loss 0.0005901696276850998 \n Val Loss, 0.004420429468154907\nEpoch 46, Loss 0.0006167645333334804 \n Val Loss, 0.00548184011131525\nEpoch 47, Loss 0.0005675225402228534 \n Val Loss, 0.006134687922894955\nEpoch 48, Loss 0.0005947432364337146 \n Val Loss, 0.0065463837236166\nEpoch 49, Loss 0.0005967012839391828 \n Val Loss, 0.00574359530583024\nEpoch 50, Loss 0.0005567075568251312 \n Val Loss, 0.004994420800358057\nEpoch 51, Loss 0.0005731075070798397 \n Val Loss, 0.009270962327718735\nEpoch 52, Loss 0.0005569086060859263 \n Val Loss, 0.006124748848378658\nEpoch 53, Loss 0.0005491647752933204 \n Val Loss, 0.005776859819889069\nEpoch 54, Loss 0.0006064880290068686 \n Val Loss, 0.004365061409771442\nEpoch 55, Loss 0.0005350290448404849 \n Val Loss, 0.005326883867383003\nEpoch 56, Loss 0.0006143884966149926 \n Val Loss, 0.007739819586277008\nEpoch 57, Loss 0.0005291105480864644 \n Val Loss, 0.004380787722766399\nEpoch 58, Loss 0.0005285986117087305 \n Val Loss, 0.006224237382411957\nEpoch 59, Loss 0.0005302311619743705 \n Val Loss, 0.005309702828526497\nEpoch 60, Loss 0.0005480074905790389 \n Val Loss, 0.005441767163574696\nEpoch 61, Loss 0.0005307444371283054 \n Val Loss, 0.007289452478289604\nEpoch 62, Loss 0.0005644065677188337 \n Val Loss, 0.004620020277798176\nEpoch 63, Loss 0.0005810043076053262 \n Val Loss, 0.00712721748277545\nEpoch 64, Loss 0.0005023326375521719 \n Val Loss, 0.0048396997153759\nEpoch    66: reducing learning rate of group 0 to 2.5000e-06.\nEpoch 65, Loss 0.0004995748749934137 \n Val Loss, 0.004540800116956234\nEpoch 66, Loss 0.0004050324787385762 \n Val Loss, 0.00360321719199419\nEpoch 67, Loss 0.00037238930235616863 \n Val Loss, 0.0038461238145828247\nEpoch 68, Loss 0.00037278985837474465 \n Val Loss, 0.0038922864478081465\nEpoch 69, Loss 0.00036940432619303465 \n Val Loss, 0.0036808662116527557\nEpoch 70, Loss 0.00036763426032848656 \n Val Loss, 0.0035923877730965614\nEpoch 71, Loss 0.0003647837438620627 \n Val Loss, 0.0038592624478042126\nEpoch 72, Loss 0.0003687366552185267 \n Val Loss, 0.003736758604645729\nEpoch 73, Loss 0.0003656492626760155 \n Val Loss, 0.003726027440279722\nEpoch 74, Loss 0.0003624334931373596 \n Val Loss, 0.003917704802006483\nEpoch 75, Loss 0.0003668801218736917 \n Val Loss, 0.003628962906077504\nEpoch 76, Loss 0.0003630555875133723 \n Val Loss, 0.0037038023583590984\nEpoch 77, Loss 0.00036714624729938805 \n Val Loss, 0.003617499489337206\nEpoch 78, Loss 0.00036503589944913983 \n Val Loss, 0.003874466521665454\nEpoch 79, Loss 0.00036387445288710296 \n Val Loss, 0.004022940061986446\nEpoch 80, Loss 0.0003633424930740148 \n Val Loss, 0.0035367661621421576\nEpoch 81, Loss 0.00036893432843498886 \n Val Loss, 0.004029059316962957\nEpoch 82, Loss 0.00036764066317118704 \n Val Loss, 0.0037965569645166397\nEpoch 83, Loss 0.0003653567691799253 \n Val Loss, 0.0035979279782623053\nEpoch 84, Loss 0.00036684490623883903 \n Val Loss, 0.0034670529421418905\nEpoch 85, Loss 0.0003597843460738659 \n Val Loss, 0.004056805744767189\nEpoch 86, Loss 0.00035807336098514497 \n Val Loss, 0.003745432011783123\nEpoch 87, Loss 0.00035968906013295054 \n Val Loss, 0.004011503420770168\nEpoch 88, Loss 0.0003596974129322916 \n Val Loss, 0.003716494422405958\nEpoch 89, Loss 0.0003588473773561418 \n Val Loss, 0.0036701320204883814\nEpoch 90, Loss 0.0003572312998585403 \n Val Loss, 0.0035100027453154325\nEpoch 91, Loss 0.0003574146830942482 \n Val Loss, 0.003648967482149601\nEpoch 92, Loss 0.0003621938230935484 \n Val Loss, 0.0038433547597378492\nEpoch 93, Loss 0.00035694928374141455 \n Val Loss, 0.00408987607806921\nEpoch 94, Loss 0.00035766998189501464 \n Val Loss, 0.003804982639849186\nEpoch    96: reducing learning rate of group 0 to 1.2500e-07.\nEpoch 95, Loss 0.0003573641588445753 \n Val Loss, 0.003938596695661545\nEpoch 96, Loss 0.0003523163904901594 \n Val Loss, 0.0035839728079736233\nEpoch 97, Loss 0.0003497700672596693 \n Val Loss, 0.0036022651474922895\nEpoch 98, Loss 0.0003526225336827338 \n Val Loss, 0.0036308178678154945\nEpoch 99, Loss 0.00034938284079544246 \n Val Loss, 0.0036252199206501245\nEpoch 100, Loss 0.00034937995951622725 \n Val Loss, 0.0035912555176764727\nEpoch 101, Loss 0.0003542898048181087 \n Val Loss, 0.003635295433923602\nEpoch 102, Loss 0.00034835762926377356 \n Val Loss, 0.0036452999338507652\nEpoch 103, Loss 0.000348881323589012 \n Val Loss, 0.0035935777705162764\nEpoch 104, Loss 0.0003498559526633471 \n Val Loss, 0.003607093123719096\nEpoch 105, Loss 0.0003491066745482385 \n Val Loss, 0.003632043022662401\nEpoch   107: reducing learning rate of group 0 to 6.2500e-09.\nEpoch 106, Loss 0.0003486561472527683 \n Val Loss, 0.0036488212645053864\nEpoch 107, Loss 0.0003496817371342331 \n Val Loss, 0.0036408239975571632\nEpoch 108, Loss 0.00034794057137332857 \n Val Loss, 0.0036330611910670996\nEpoch 109, Loss 0.00034819095162674785 \n Val Loss, 0.0036283936351537704\nEpoch 110, Loss 0.0003498779551591724 \n Val Loss, 0.0036247186362743378\nEpoch 111, Loss 0.00034896115539595485 \n Val Loss, 0.0036220939364284277\nEpoch 112, Loss 0.0003504026972223073 \n Val Loss, 0.0036195726133883\nEpoch 113, Loss 0.00034822773886844516 \n Val Loss, 0.003617056179791689\nEpoch 114, Loss 0.00034884619526565075 \n Val Loss, 0.0036162647884339094\nEpoch 115, Loss 0.000347825582139194 \n Val Loss, 0.00361489481292665\nEpoch 116, Loss 0.00034763332223519683 \n Val Loss, 0.003614397021010518\nEpoch 117, Loss 0.0003485994238872081 \n Val Loss, 0.00361269386485219\nEpoch 118, Loss 0.00034823419991880655 \n Val Loss, 0.0036139620933681726\nEpoch 119, Loss 0.0003483300388325006 \n Val Loss, 0.0036135991103947163\nEpoch 120, Loss 0.0003482330357655883 \n Val Loss, 0.003612636122852564\nEpoch 121, Loss 0.0003508549416437745 \n Val Loss, 0.0036124205216765404\nEpoch 122, Loss 0.0003525356587488204 \n Val Loss, 0.0036113092210143805\nEpoch 123, Loss 0.00034976168535649776 \n Val Loss, 0.0036114505492150784\nEpoch 124, Loss 0.0003478744183667004 \n Val Loss, 0.0036122086457908154\nEpoch 125, Loss 0.0003486553323455155 \n Val Loss, 0.003612208878621459\nEpoch 126, Loss 0.0003547319211065769 \n Val Loss, 0.0036119073629379272\nEpoch 127, Loss 0.0003499794693198055 \n Val Loss, 0.003612648230046034\nEpoch 128, Loss 0.00034934430732391775 \n Val Loss, 0.0036123821046203375\nEpoch 129, Loss 0.0003478885628283024 \n Val Loss, 0.0036117653362452984\nEpoch 130, Loss 0.00035064216353930533 \n Val Loss, 0.0036121816374361515\nEpoch 131, Loss 0.00034891098039224744 \n Val Loss, 0.0036120403092354536\nEpoch 132, Loss 0.0003512991825118661 \n Val Loss, 0.003611311549320817\nEpoch 133, Loss 0.0003487687499728054 \n Val Loss, 0.0036113113164901733\nEpoch 134, Loss 0.00034879075246863067 \n Val Loss, 0.0036112486850470304\nEpoch 135, Loss 0.00034777753171510994 \n Val Loss, 0.0036121176090091467\nEpoch 136, Loss 0.0003477999707683921 \n Val Loss, 0.003612273372709751\nEpoch 137, Loss 0.0003477825375739485 \n Val Loss, 0.003611872438341379\nEpoch 138, Loss 0.00034877005964517593 \n Val Loss, 0.003612402593716979\nEpoch 139, Loss 0.0003484210465103388 \n Val Loss, 0.003611535532400012\nEpoch 140, Loss 0.00034830847289413214 \n Val Loss, 0.0036119220312684774\nEpoch 141, Loss 0.00034787875483743846 \n Val Loss, 0.0036116924602538347\nEpoch 142, Loss 0.0003476949059404433 \n Val Loss, 0.0036127292551100254\nEpoch 143, Loss 0.00035040636430494487 \n Val Loss, 0.0036124391481280327\nEpoch 144, Loss 0.0003479185688775033 \n Val Loss, 0.003612332511693239\nEpoch 145, Loss 0.0003528293455019593 \n Val Loss, 0.0036124279722571373\nEpoch 146, Loss 0.0003475750854704529 \n Val Loss, 0.003613722510635853\nEpoch 147, Loss 0.00034841333399526775 \n Val Loss, 0.0036138116847723722\nEpoch 148, Loss 0.0003523043415043503 \n Val Loss, 0.0036129718646407127\nEpoch 149, Loss 0.0003478577418718487 \n Val Loss, 0.003612363710999489\nEpoch 150, Loss 0.00034854316618293524 \n Val Loss, 0.0036133667454123497\nEpoch 151, Loss 0.00034841621527448297 \n Val Loss, 0.003612958826124668\nEpoch 152, Loss 0.0003493895346764475 \n Val Loss, 0.0036137800198048353\nEpoch 153, Loss 0.0003495535929687321 \n Val Loss, 0.0036128652282059193\nEpoch 154, Loss 0.0003491187817417085 \n Val Loss, 0.0036132168024778366\nEpoch 155, Loss 0.0003492128162179142 \n Val Loss, 0.0036132761742919683\nEpoch 156, Loss 0.0003503809275571257 \n Val Loss, 0.0036139138974249363\nEpoch 157, Loss 0.00034763693111017346 \n Val Loss, 0.0036142030730843544\nEpoch 158, Loss 0.0003500432358123362 \n Val Loss, 0.003614044515416026\nEpoch 159, Loss 0.00034879869781434536 \n Val Loss, 0.0036136009730398655\nEpoch 160, Loss 0.0003494365664664656 \n Val Loss, 0.0036133897956460714\nEpoch 161, Loss 0.00034858318394981325 \n Val Loss, 0.0036143995821475983\nEpoch 162, Loss 0.0003484119661152363 \n Val Loss, 0.003613741137087345\nEpoch 163, Loss 0.00035066460259258747 \n Val Loss, 0.003613254986703396\nEpoch 164, Loss 0.00035017638583667576 \n Val Loss, 0.0036122493911534548\nEpoch 165, Loss 0.0003496353165246546 \n Val Loss, 0.0036131422966718674\nEpoch 166, Loss 0.0003493119729682803 \n Val Loss, 0.0036132759414613247\nEpoch 167, Loss 0.0003476146957837045 \n Val Loss, 0.0036136084236204624\nEpoch 168, Loss 0.0003489358932711184 \n Val Loss, 0.0036133481189608574\nEpoch 169, Loss 0.0003487944195512682 \n Val Loss, 0.003613188862800598\nEpoch 170, Loss 0.0003479481383692473 \n Val Loss, 0.0036136629059910774\nEpoch 171, Loss 0.0003482568426989019 \n Val Loss, 0.0036135083064436913\nEpoch 172, Loss 0.0003488742222543806 \n Val Loss, 0.003613043576478958\nEpoch 173, Loss 0.00034820340806618333 \n Val Loss, 0.0036131818778812885\nEpoch 174, Loss 0.0003483134205453098 \n Val Loss, 0.0036137818824499846\nEpoch 175, Loss 0.0003488347865641117 \n Val Loss, 0.0036139297299087048\nEpoch 176, Loss 0.0003481971798464656 \n Val Loss, 0.0036135599948465824\nEpoch 177, Loss 0.00034788151970133185 \n Val Loss, 0.003614057321101427\nEpoch 178, Loss 0.0003481147577986121 \n Val Loss, 0.0036141094751656055\nEpoch 179, Loss 0.0003508121590130031 \n Val Loss, 0.003613836597651243\nEpoch 180, Loss 0.00034808210330083966 \n Val Loss, 0.0036125746555626392\nEpoch 181, Loss 0.00034843760658986866 \n Val Loss, 0.0036120512522757053\nEpoch 182, Loss 0.0003507736837491393 \n Val Loss, 0.003612145781517029\nEpoch 183, Loss 0.0003537417796906084 \n Val Loss, 0.003612593514844775\nEpoch 184, Loss 0.00034843673347495496 \n Val Loss, 0.003613544162362814\nEpoch 185, Loss 0.0003480602754279971 \n Val Loss, 0.003613503649830818\nEpoch 186, Loss 0.0003477240097709 \n Val Loss, 0.00361350504681468\nEpoch 187, Loss 0.0003480404557194561 \n Val Loss, 0.003613689448684454\nEpoch 188, Loss 0.00034820602741092443 \n Val Loss, 0.003613481530919671\nEpoch 189, Loss 0.00035253106034360826 \n Val Loss, 0.0036129839718341827\nEpoch 190, Loss 0.0003492085088510066 \n Val Loss, 0.0036128642968833447\nEpoch 191, Loss 0.00034820105065591633 \n Val Loss, 0.0036130016669631004\nEpoch 192, Loss 0.000347725028404966 \n Val Loss, 0.0036128596402704716\nEpoch 193, Loss 0.00035050237784162164 \n Val Loss, 0.0036131921224296093\nEpoch 194, Loss 0.0003482514584902674 \n Val Loss, 0.0036132927052676678\nEpoch 195, Loss 0.0003478995931800455 \n Val Loss, 0.003613943699747324\nEpoch 196, Loss 0.0003485179040580988 \n Val Loss, 0.0036135059781372547\nEpoch 197, Loss 0.0003498596197459847 \n Val Loss, 0.0036136761773377657\nEpoch 198, Loss 0.0003479688020888716 \n Val Loss, 0.003613363951444626\nEpoch 199, Loss 0.0003513756091706455 \n Val Loss, 0.0036134575493633747\nEpoch 200, Loss 0.00034858225262723863 \n Val Loss, 0.003614578163251281\nEpoch 201, Loss 0.0003500721068121493 \n Val Loss, 0.0036144545301795006\nEpoch 202, Loss 0.00034973485162481666 \n Val Loss, 0.0036130836233496666\nEpoch 203, Loss 0.00034815838444046676 \n Val Loss, 0.0036131106317043304\nEpoch 204, Loss 0.00035036407643929124 \n Val Loss, 0.0036129408981651068\nEpoch 205, Loss 0.00034877596772275865 \n Val Loss, 0.0036132903769612312\nEpoch 206, Loss 0.000348131317878142 \n Val Loss, 0.0036134226247668266\nEpoch 207, Loss 0.00034752197097986937 \n Val Loss, 0.0036128582432866096\nEpoch 208, Loss 0.00034891796531155705 \n Val Loss, 0.0036136035341769457\nEpoch 209, Loss 0.00034950717235915363 \n Val Loss, 0.00361241796053946\nEpoch 210, Loss 0.0003491533570922911 \n Val Loss, 0.0036139683797955513\nEpoch 211, Loss 0.00034825067268684506 \n Val Loss, 0.0036150519736111164\nEpoch 212, Loss 0.0003500921302475035 \n Val Loss, 0.0036132950335741043\nEpoch 213, Loss 0.00034951677662320435 \n Val Loss, 0.003613241482526064\nEpoch 214, Loss 0.00034802916343323886 \n Val Loss, 0.003613380715250969\nEpoch 215, Loss 0.0003495778073556721 \n Val Loss, 0.003613365814089775\nEpoch 216, Loss 0.00034766431781463325 \n Val Loss, 0.0036130640655755997\nEpoch 217, Loss 0.000348316301824525 \n Val Loss, 0.0036138310097157955\nEpoch 218, Loss 0.00034941453486680984 \n Val Loss, 0.003613916225731373\nEpoch 219, Loss 0.00034768605837598443 \n Val Loss, 0.0036135083064436913\nEpoch 220, Loss 0.00034778594272211194 \n Val Loss, 0.003612454514950514\nEpoch 221, Loss 0.0003475382400210947 \n Val Loss, 0.003613021457567811\nEpoch 222, Loss 0.00034831775701604784 \n Val Loss, 0.003612825646996498\nEpoch 223, Loss 0.00034741911804303527 \n Val Loss, 0.0036129688378423452\nEpoch 224, Loss 0.00034809517092071474 \n Val Loss, 0.003612873610109091\nEpoch 225, Loss 0.00034985123784281313 \n Val Loss, 0.003613363951444626\nEpoch 226, Loss 0.00035371299600228667 \n Val Loss, 0.0036126496270298958\nEpoch 227, Loss 0.0003543393686413765 \n Val Loss, 0.003613268956542015\nEpoch 228, Loss 0.00035036070039495826 \n Val Loss, 0.0036136561539024115\nEpoch 229, Loss 0.00035008316626772285 \n Val Loss, 0.003613661043345928\nEpoch 230, Loss 0.0003476432175375521 \n Val Loss, 0.003613947657868266\nEpoch 231, Loss 0.00034916942240670323 \n Val Loss, 0.003613661043345928\nEpoch 232, Loss 0.00034759362461045384 \n Val Loss, 0.003613079432398081\nEpoch 233, Loss 0.00034761085407808423 \n Val Loss, 0.0036143683828413486\nEpoch 234, Loss 0.00034825431066565216 \n Val Loss, 0.0036132836248725653\nEpoch 235, Loss 0.00034768402110785246 \n Val Loss, 0.0036139024887233973\nEpoch 236, Loss 0.0003498209116514772 \n Val Loss, 0.0036130191292613745\nEpoch 237, Loss 0.0003488452930469066 \n Val Loss, 0.003613637061789632\nEpoch 238, Loss 0.00034751318162307143 \n Val Loss, 0.0036139367148280144\nEpoch 239, Loss 0.0003482874308247119 \n Val Loss, 0.003613499691709876\nEpoch 240, Loss 0.00034892704570665956 \n Val Loss, 0.003613512497395277\nEpoch 241, Loss 0.000348146801115945 \n Val Loss, 0.003614348592236638\nEpoch 242, Loss 0.0003483756154309958 \n Val Loss, 0.0036142291501164436\nEpoch 243, Loss 0.00034915306605398655 \n Val Loss, 0.0036134705878794193\nEpoch 244, Loss 0.00035003290395252407 \n Val Loss, 0.003614010289311409\nEpoch 245, Loss 0.00034946526284329593 \n Val Loss, 0.003613624721765518\nEpoch 246, Loss 0.00035106693394482136 \n Val Loss, 0.003613954409956932\nEpoch 247, Loss 0.0003510995302349329 \n Val Loss, 0.0036135967820882797\nEpoch 248, Loss 0.000347410881659016 \n Val Loss, 0.0036142601165920496\nEpoch 249, Loss 0.00034872349351644516 \n Val Loss, 0.003614349290728569\nEpoch 250, Loss 0.0003492482064757496 \n Val Loss, 0.003614140208810568\nEpoch 251, Loss 0.0003484611806925386 \n Val Loss, 0.0036134771071374416\nEpoch 252, Loss 0.0003486443019937724 \n Val Loss, 0.0036126349586993456\nEpoch 253, Loss 0.00034744967706501484 \n Val Loss, 0.0036128999199718237\nEpoch 254, Loss 0.00034825506736524403 \n Val Loss, 0.003613292472437024\nEpoch 255, Loss 0.0003489240480121225 \n Val Loss, 0.003612926695495844\nEpoch 256, Loss 0.00035134819336235523 \n Val Loss, 0.00361341773532331\nEpoch 257, Loss 0.00034908708767034113 \n Val Loss, 0.003613499691709876\nEpoch 258, Loss 0.00034770360798574984 \n Val Loss, 0.003614726010710001\nEpoch 259, Loss 0.00034934247378259897 \n Val Loss, 0.0036144210025668144\nEpoch 260, Loss 0.0003485666529741138 \n Val Loss, 0.003614300861954689\nEpoch 261, Loss 0.00035292035317979753 \n Val Loss, 0.0036146400962024927\nEpoch 262, Loss 0.0003503767657093704 \n Val Loss, 0.003614327870309353\nEpoch 263, Loss 0.0003485738707240671 \n Val Loss, 0.0036149732768535614\nEpoch 264, Loss 0.0003495021373964846 \n Val Loss, 0.003614694345742464\nEpoch 265, Loss 0.00034810055512934923 \n Val Loss, 0.003613510634750128\nEpoch 266, Loss 0.000347582099493593 \n Val Loss, 0.00361342029646039\nEpoch 267, Loss 0.0003474220575299114 \n Val Loss, 0.0036143509205430746\nEpoch 268, Loss 0.00034850314841605723 \n Val Loss, 0.0036138221621513367\nEpoch 269, Loss 0.0003486860659904778 \n Val Loss, 0.003613789100199938\nEpoch 270, Loss 0.0003527842927724123 \n Val Loss, 0.0036139353178441525\nEpoch 271, Loss 0.0003497486759442836 \n Val Loss, 0.003614329034462571\nEpoch 272, Loss 0.00034793789382092655 \n Val Loss, 0.003614511340856552\nEpoch 273, Loss 0.0003482681349851191 \n Val Loss, 0.0036143576726317406\nEpoch 274, Loss 0.00034972798312082887 \n Val Loss, 0.003613000037148595\nEpoch 275, Loss 0.00035010030842386186 \n Val Loss, 0.0036132754758000374\nEpoch 276, Loss 0.0003489595546852797 \n Val Loss, 0.0036132303066551685\nEpoch 277, Loss 0.00034882285399362445 \n Val Loss, 0.0036141315940767527\nEpoch 278, Loss 0.00034883711487054825 \n Val Loss, 0.0036143267061561346\nEpoch 279, Loss 0.00034942865022458136 \n Val Loss, 0.0036155646666884422\nEpoch 280, Loss 0.0003481967723928392 \n Val Loss, 0.0036140650045126677\nEpoch 281, Loss 0.00035026410478167236 \n Val Loss, 0.0036143730394542217\nEpoch 282, Loss 0.0003555337025318295 \n Val Loss, 0.003613307373598218\nEpoch 283, Loss 0.0003496711142361164 \n Val Loss, 0.0036141392774879932\nEpoch 284, Loss 0.00034851173404604197 \n Val Loss, 0.0036137730348855257\nEpoch 285, Loss 0.0003480374871287495 \n Val Loss, 0.0036144545301795006\nEpoch 286, Loss 0.0003492251562420279 \n Val Loss, 0.0036145688500255346\nEpoch 287, Loss 0.0003498334262985736 \n Val Loss, 0.0036144517362117767\nEpoch 288, Loss 0.00034850294468924403 \n Val Loss, 0.0036150922533124685\nEpoch 289, Loss 0.00034776784013956785 \n Val Loss, 0.0036147080827504396\nEpoch 290, Loss 0.0003488500660751015 \n Val Loss, 0.0036141546443104744\nEpoch 291, Loss 0.00034855492413043976 \n Val Loss, 0.003614701796323061\nEpoch 292, Loss 0.00034890303504653275 \n Val Loss, 0.0036140724550932646\nEpoch 293, Loss 0.00034949896507896483 \n Val Loss, 0.003612663596868515\nEpoch 294, Loss 0.000347606313880533 \n Val Loss, 0.003614777699112892\nEpoch 295, Loss 0.00034809092176146805 \n Val Loss, 0.00361527013592422\nEpoch 296, Loss 0.0003478344005998224 \n Val Loss, 0.0036146044731140137\nEpoch 297, Loss 0.00034896243596449494 \n Val Loss, 0.0036149099469184875\nEpoch 298, Loss 0.000349776673829183 \n Val Loss, 0.0036147809587419033\nEpoch 299, Loss 0.000348352623404935 \n Val Loss, 0.003614344634115696\nEpoch 300, Loss 0.0003477309364825487 \n Val Loss, 0.003614161629229784\nEpoch 301, Loss 0.0003495960554573685 \n Val Loss, 0.0036139029543846846\nEpoch 302, Loss 0.0003478007856756449 \n Val Loss, 0.003613594686612487\nEpoch 303, Loss 0.0003503737971186638 \n Val Loss, 0.0036140221636742353\nEpoch 304, Loss 0.0003484399348963052 \n Val Loss, 0.0036139858420938253\nEpoch 305, Loss 0.00035360740730538964 \n Val Loss, 0.0036145690828561783\nEpoch 306, Loss 0.00035028666025027633 \n Val Loss, 0.0036137988790869713\nEpoch 307, Loss 0.0003476045385468751 \n Val Loss, 0.003612809581682086\nEpoch 308, Loss 0.00035035167820751667 \n Val Loss, 0.0036133150570094585\nEpoch 309, Loss 0.000349553331034258 \n Val Loss, 0.0036128368228673935\nEpoch 310, Loss 0.00034770279307849705 \n Val Loss, 0.0036130398511886597\nEpoch 311, Loss 0.0003483742184471339 \n Val Loss, 0.003613104112446308\nEpoch 312, Loss 0.00034770849742926657 \n Val Loss, 0.0036120477598160505\nEpoch 313, Loss 0.00034848653012886643 \n Val Loss, 0.0036131388042122126\nEpoch 314, Loss 0.0003491388924885541 \n Val Loss, 0.003612493397668004\nEpoch 315, Loss 0.00035315987770445645 \n Val Loss, 0.0036140887532383204\nEpoch 316, Loss 0.000350594287738204 \n Val Loss, 0.003613674081861973\nEpoch 317, Loss 0.0003475236298982054 \n Val Loss, 0.0036136070266366005\nEpoch 318, Loss 0.00034764796146191657 \n Val Loss, 0.0036138840951025486\nEpoch 319, Loss 0.00034811560180969536 \n Val Loss, 0.0036136037670075893\nEpoch 320, Loss 0.0003542425110936165 \n Val Loss, 0.0036140941083431244\nEpoch 321, Loss 0.0003478914441075176 \n Val Loss, 0.0036142845638096333\nEpoch 322, Loss 0.00034757363027893007 \n Val Loss, 0.0036134091205894947\nEpoch 323, Loss 0.00034796734689734876 \n Val Loss, 0.0036130384542047977\nEpoch 324, Loss 0.000347688706824556 \n Val Loss, 0.0036131993401795626\nEpoch 325, Loss 0.00034743192372843623 \n Val Loss, 0.0036131308879703283\nEpoch 326, Loss 0.0003487860376480967 \n Val Loss, 0.003613094799220562\nEpoch 327, Loss 0.0003481105377431959 \n Val Loss, 0.003613613545894623\nEpoch 328, Loss 0.0003480536106508225 \n Val Loss, 0.003613443113863468\nEpoch 329, Loss 0.00035143434070050716 \n Val Loss, 0.0036139446310698986\nEpoch 330, Loss 0.0003480984305497259 \n Val Loss, 0.0036137187853455544\nEpoch 331, Loss 0.0003506099455989897 \n Val Loss, 0.0036135681439191103\nEpoch 332, Loss 0.00034809348289854825 \n Val Loss, 0.003613088745623827\nEpoch 333, Loss 0.0003474556724540889 \n Val Loss, 0.0036135329864919186\nEpoch 334, Loss 0.00034984564990736544 \n Val Loss, 0.003613589331507683\nEpoch 335, Loss 0.0003494452976156026 \n Val Loss, 0.0036141809541732073\nEpoch 336, Loss 0.00034987213439308107 \n Val Loss, 0.0036139599978923798\nEpoch 337, Loss 0.0003477697027847171 \n Val Loss, 0.0036140629090368748\nEpoch 338, Loss 0.0003487889771349728 \n Val Loss, 0.00361350504681468\nEpoch 339, Loss 0.000352442089933902 \n Val Loss, 0.003613738575950265\nEpoch 340, Loss 0.0003490797244012356 \n Val Loss, 0.0036140591837465763\nEpoch 341, Loss 0.00034983165096491575 \n Val Loss, 0.003613838227465749\nEpoch 342, Loss 0.00035052065504714847 \n Val Loss, 0.00361373252235353\nEpoch 343, Loss 0.0003483126638457179 \n Val Loss, 0.0036138067953288555\nEpoch 344, Loss 0.0003481377207208425 \n Val Loss, 0.003613858949393034\nEpoch 345, Loss 0.00034888414666056633 \n Val Loss, 0.0036134012043476105\nEpoch 346, Loss 0.0003485286724753678 \n Val Loss, 0.003613556269556284\nEpoch 347, Loss 0.0003486327186692506 \n Val Loss, 0.003614051267504692\nEpoch 348, Loss 0.00034818710992112756 \n Val Loss, 0.003613110398873687\nEpoch 349, Loss 0.0003498847072478384 \n Val Loss, 0.003613010747358203\nEpoch 350, Loss 0.00034876997233368456 \n Val Loss, 0.003612708766013384\nEpoch 351, Loss 0.0003474615514278412 \n Val Loss, 0.0036125839687883854\nEpoch 352, Loss 0.00034840102307498455 \n Val Loss, 0.003613193053752184\nEpoch 353, Loss 0.0003482213360257447 \n Val Loss, 0.003612791420891881\nEpoch 354, Loss 0.0003605447127483785 \n Val Loss, 0.0036129909567534924\nEpoch 355, Loss 0.00034936406882479787 \n Val Loss, 0.0036126216873526573\nEpoch 356, Loss 0.00034780119312927127 \n Val Loss, 0.003613256849348545\nEpoch 357, Loss 0.00034927649539895356 \n Val Loss, 0.003612872678786516\nEpoch 358, Loss 0.0003481826570350677 \n Val Loss, 0.003614170243963599\nEpoch 359, Loss 0.00034806743497028947 \n Val Loss, 0.003614739514887333\nEpoch 360, Loss 0.0003475177800282836 \n Val Loss, 0.0036142109893262386\nEpoch 361, Loss 0.00034860073355957866 \n Val Loss, 0.00361375673674047\nEpoch 362, Loss 0.00034972975845448673 \n Val Loss, 0.0036131045781075954\nEpoch 363, Loss 0.00034781036083586514 \n Val Loss, 0.003613200504332781\nEpoch 364, Loss 0.0003485470952000469 \n Val Loss, 0.003613836597651243\nEpoch 365, Loss 0.00034934855648316443 \n Val Loss, 0.003612604457885027\nEpoch 366, Loss 0.00034868967486545444 \n Val Loss, 0.003612541826441884\nEpoch 367, Loss 0.00034949477412737906 \n Val Loss, 0.0036124936304986477\nEpoch 368, Loss 0.00035060523077845573 \n Val Loss, 0.0036124149337410927\nEpoch 369, Loss 0.0003545079380273819 \n Val Loss, 0.003611573949456215\nEpoch 370, Loss 0.0003542117483448237 \n Val Loss, 0.0036127713974565268\nEpoch 371, Loss 0.00034790491918101907 \n Val Loss, 0.0036125718615949154\nEpoch 372, Loss 0.0003509038360789418 \n Val Loss, 0.003612315747886896\nEpoch 373, Loss 0.00035264017060399055 \n Val Loss, 0.003612622618675232\nEpoch 374, Loss 0.00034797764965333045 \n Val Loss, 0.0036116803530603647\nEpoch 375, Loss 0.0003488684887997806 \n Val Loss, 0.0036128265783190727\nEpoch 376, Loss 0.0003483571927063167 \n Val Loss, 0.0036133492831140757\nEpoch 377, Loss 0.00034747167956084013 \n Val Loss, 0.0036132177338004112\nEpoch 378, Loss 0.0003482524480205029 \n Val Loss, 0.0036138277500867844\nEpoch 379, Loss 0.00035122540430165827 \n Val Loss, 0.0036138263531029224\nEpoch 380, Loss 0.0003509109665174037 \n Val Loss, 0.003613443346694112\nEpoch 381, Loss 0.0003490319650154561 \n Val Loss, 0.003613780252635479\nEpoch 382, Loss 0.00034789874916896224 \n Val Loss, 0.0036138352006673813\nEpoch 383, Loss 0.00034876540303230286 \n Val Loss, 0.0036140233278274536\nEpoch 384, Loss 0.0003524771600496024 \n Val Loss, 0.0036145150661468506\nEpoch 385, Loss 0.00034786222386173904 \n Val Loss, 0.0036133399698883295\nEpoch 386, Loss 0.00035005176323466003 \n Val Loss, 0.0036136696580797434\nEpoch 387, Loss 0.0003481277672108263 \n Val Loss, 0.0036128864157944918\nEpoch 388, Loss 0.00034851065720431507 \n Val Loss, 0.0036132331006228924\nEpoch 389, Loss 0.00035143111017532647 \n Val Loss, 0.0036129625514149666\nEpoch 390, Loss 0.00034827113267965615 \n Val Loss, 0.003612582804635167\nEpoch 391, Loss 0.0003480963932815939 \n Val Loss, 0.003612999804317951\nEpoch 392, Loss 0.0003476999409031123 \n Val Loss, 0.003612259402871132\nEpoch 393, Loss 0.00034838932333514094 \n Val Loss, 0.003611121792346239\nEpoch 394, Loss 0.00034728238824754953 \n Val Loss, 0.003612327389419079\nEpoch 395, Loss 0.00034959157346747816 \n Val Loss, 0.0036126533523201942\nEpoch 396, Loss 0.0003478167927823961 \n Val Loss, 0.0036130929365754128\nEpoch 397, Loss 0.00034851927193813026 \n Val Loss, 0.003612432861700654\nEpoch 398, Loss 0.00034873050753958523 \n Val Loss, 0.0036120053846389055\nEpoch 399, Loss 0.0003493416297715157 \n Val Loss, 0.00361194577999413\nEpoch 400, Loss 0.00034851476084440947 \n Val Loss, 0.0036128824576735497\nEpoch 401, Loss 0.0003524574276525527 \n Val Loss, 0.0036135779228061438\nEpoch 402, Loss 0.0003482704923953861 \n Val Loss, 0.003610882442444563\nEpoch 403, Loss 0.0003478722064755857 \n Val Loss, 0.0036118575371801853\nEpoch 404, Loss 0.00034892806434072554 \n Val Loss, 0.003611578606069088\nEpoch 405, Loss 0.0003480308223515749 \n Val Loss, 0.0036113448441028595\nEpoch 406, Loss 0.00034797179978340864 \n Val Loss, 0.0036112756934016943\nEpoch 407, Loss 0.00034865294583141804 \n Val Loss, 0.003611169056966901\nEpoch 408, Loss 0.00034789551864378154 \n Val Loss, 0.003612194210290909\nEpoch 409, Loss 0.0003526253567542881 \n Val Loss, 0.003612156957387924\nEpoch 410, Loss 0.0003536660224199295 \n Val Loss, 0.0036130195949226618\nEpoch 411, Loss 0.00034752118517644703 \n Val Loss, 0.0036132605746388435\nEpoch 412, Loss 0.0003476406855043024 \n Val Loss, 0.0036122458986938\nEpoch 413, Loss 0.00034931430127471685 \n Val Loss, 0.003612217493355274\nEpoch 414, Loss 0.0003476814308669418 \n Val Loss, 0.0036122954916208982\nEpoch 415, Loss 0.00034760977723635733 \n Val Loss, 0.003611456137150526\nEpoch 416, Loss 0.00034806624171324074 \n Val Loss, 0.0036118763964623213\nEpoch 417, Loss 0.0003472910902928561 \n Val Loss, 0.003611411899328232\nEpoch 418, Loss 0.0003478876024018973 \n Val Loss, 0.0036122435703873634\nEpoch 419, Loss 0.0003489909286145121 \n Val Loss, 0.0036122966557741165\nEpoch 420, Loss 0.0003494555130600929 \n Val Loss, 0.003612284082919359\nEpoch 421, Loss 0.0003493662807159126 \n Val Loss, 0.0036122589372098446\nEpoch 422, Loss 0.00034860908635891974 \n Val Loss, 0.003612060099840164\nEpoch 423, Loss 0.00034876694553531706 \n Val Loss, 0.0036122677847743034\nEpoch 424, Loss 0.00035034044412896037 \n Val Loss, 0.0036120833829045296\nEpoch 425, Loss 0.000349446723703295 \n Val Loss, 0.0036137495189905167\nEpoch 426, Loss 0.0003499503945931792 \n Val Loss, 0.0036129169166088104\nEpoch 427, Loss 0.000349575188010931 \n Val Loss, 0.003612989792600274\nEpoch 428, Loss 0.00034926924854516983 \n Val Loss, 0.0036121217999607325\nEpoch 429, Loss 0.00034729496110230684 \n Val Loss, 0.0036126412451267242\nEpoch 430, Loss 0.0003478052094578743 \n Val Loss, 0.003612297121435404\nEpoch 431, Loss 0.00034755447995848954 \n Val Loss, 0.0036126445047557354\nEpoch 432, Loss 0.0003480614395812154 \n Val Loss, 0.003612568834796548\nEpoch 433, Loss 0.00034857221180573106 \n Val Loss, 0.0036129208747297525\nEpoch 434, Loss 0.0003480355953797698 \n Val Loss, 0.003611949272453785\nEpoch 435, Loss 0.0003477064601611346 \n Val Loss, 0.0036132545210421085\nEpoch 436, Loss 0.0003493033582344651 \n Val Loss, 0.0036128873471170664\nEpoch 437, Loss 0.0003496076387818903 \n Val Loss, 0.0036134524270892143\nEpoch 438, Loss 0.00034808501368388534 \n Val Loss, 0.0036122514866292477\nEpoch 439, Loss 0.00034787834738381207 \n Val Loss, 0.0036125043407082558\nEpoch 440, Loss 0.00035156929516233504 \n Val Loss, 0.003611937165260315\nEpoch 441, Loss 0.0003475522680673748 \n Val Loss, 0.0036128477659076452\nEpoch 442, Loss 0.00035372914862819016 \n Val Loss, 0.003612138796597719\nEpoch 443, Loss 0.0003475502599030733 \n Val Loss, 0.0036115581169724464\nEpoch 444, Loss 0.0003522546321619302 \n Val Loss, 0.003611175809055567\nEpoch 445, Loss 0.00034873359254561365 \n Val Loss, 0.0036117727868258953\nEpoch 446, Loss 0.0003478283470030874 \n Val Loss, 0.0036118337884545326\nEpoch 447, Loss 0.00034878365113399923 \n Val Loss, 0.003611380700021982\nEpoch 448, Loss 0.0003480801242403686 \n Val Loss, 0.003611471736803651\nEpoch 449, Loss 0.0003505242057144642 \n Val Loss, 0.0036120698787271976\nEpoch 450, Loss 0.0003483413311187178 \n Val Loss, 0.00361235486343503\nEpoch 451, Loss 0.0003476355050224811 \n Val Loss, 0.003612094558775425\nEpoch 452, Loss 0.000348597124684602 \n Val Loss, 0.00361200631596148\nEpoch 453, Loss 0.000347731402143836 \n Val Loss, 0.003611680818721652\nEpoch 454, Loss 0.00034740648698061705 \n Val Loss, 0.0036117166746407747\nEpoch 455, Loss 0.00034909663372673094 \n Val Loss, 0.003612300381064415\nEpoch 456, Loss 0.00034759295522235334 \n Val Loss, 0.003612734144553542\nEpoch 457, Loss 0.00034750605118460953 \n Val Loss, 0.003611342515796423\nEpoch 458, Loss 0.0003532202390488237 \n Val Loss, 0.0036120018921792507\nEpoch 459, Loss 0.0003480941231828183 \n Val Loss, 0.0036118587013334036\nEpoch 460, Loss 0.0003481657477095723 \n Val Loss, 0.0036139157600700855\nEpoch 461, Loss 0.00034745779703371227 \n Val Loss, 0.0036127972416579723\nEpoch 462, Loss 0.0003479740407783538 \n Val Loss, 0.0036129087675362825\nEpoch 463, Loss 0.0003526627260725945 \n Val Loss, 0.003612805623561144\nEpoch 464, Loss 0.0003486975037958473 \n Val Loss, 0.00361286336556077\nEpoch 465, Loss 0.0003474259574431926 \n Val Loss, 0.0036130324006080627\nEpoch 466, Loss 0.00034771961509250104 \n Val Loss, 0.003612849861383438\nEpoch 467, Loss 0.00034808143391273916 \n Val Loss, 0.003612487344071269\nEpoch 468, Loss 0.0003491649986244738 \n Val Loss, 0.0036128221545368433\nEpoch 469, Loss 0.00034806382609531283 \n Val Loss, 0.0036116698756814003\nEpoch 470, Loss 0.0003483775071799755 \n Val Loss, 0.0036122826859354973\nEpoch 471, Loss 0.00034790823701769114 \n Val Loss, 0.0036122428718954325\nEpoch 472, Loss 0.00034850090742111206 \n Val Loss, 0.003612689906731248\nEpoch 473, Loss 0.0003488430520519614 \n Val Loss, 0.0036125201731920242\nEpoch 474, Loss 0.0003477660065982491 \n Val Loss, 0.003611935768276453\nEpoch 475, Loss 0.00034832791425287724 \n Val Loss, 0.003611466148868203\nEpoch 476, Loss 0.00034832957317121327 \n Val Loss, 0.0036114640533924103\nEpoch 477, Loss 0.0003505026106722653 \n Val Loss, 0.003612060332670808\nEpoch 478, Loss 0.00034783888258971274 \n Val Loss, 0.003611820749938488\nEpoch 479, Loss 0.000348281319020316 \n Val Loss, 0.0036127734929323196\nEpoch 480, Loss 0.0003480359446257353 \n Val Loss, 0.0036123106256127357\nEpoch 481, Loss 0.00034790547215379775 \n Val Loss, 0.0036116624251008034\nEpoch 482, Loss 0.00035098069929517806 \n Val Loss, 0.003612035419791937\nEpoch 483, Loss 0.0003483859181869775 \n Val Loss, 0.0036114417016506195\nEpoch 484, Loss 0.00034859078004956245 \n Val Loss, 0.003611912950873375\nEpoch 485, Loss 0.00034796519321389496 \n Val Loss, 0.0036123369354754686\nEpoch 486, Loss 0.00034745779703371227 \n Val Loss, 0.0036127797793596983\nEpoch 487, Loss 0.0003493788535706699 \n Val Loss, 0.0036127683706581593\nEpoch 488, Loss 0.0003482485772110522 \n Val Loss, 0.0036133956164121628\nEpoch 489, Loss 0.0003487106296233833 \n Val Loss, 0.0036120442673563957\nEpoch 490, Loss 0.0003481414169073105 \n Val Loss, 0.00361143727786839\nEpoch 491, Loss 0.00034827555646188557 \n Val Loss, 0.003611242398619652\nEpoch 492, Loss 0.0003488976217340678 \n Val Loss, 0.003612619824707508\nEpoch 493, Loss 0.00034831694210879505 \n Val Loss, 0.003613331587985158\nEpoch 494, Loss 0.00034748506732285023 \n Val Loss, 0.0036126684863120317\nEpoch 495, Loss 0.00034777354449033737 \n Val Loss, 0.0036125066690146923\nEpoch 496, Loss 0.00034743049764074385 \n Val Loss, 0.0036122980527579784\nEpoch 497, Loss 0.0003479589940980077 \n Val Loss, 0.0036122773308306932\nEpoch 498, Loss 0.0003487193025648594 \n Val Loss, 0.003611900843679905\nEpoch 499, Loss 0.00034811621299013495 \n Val Loss, 0.003612154396250844\nEpoch 500, Loss 0.00034747281461022794 \n Val Loss, 0.0036112875677645206\nEpoch 501, Loss 0.00034825067268684506 \n Val Loss, 0.0036111632362008095\nEpoch 502, Loss 0.00034929183311760426 \n Val Loss, 0.0036108968779444695\nEpoch 503, Loss 0.0003479802398942411 \n Val Loss, 0.003611095016822219\nEpoch 504, Loss 0.0003476511628832668 \n Val Loss, 0.0036118368152529\nEpoch 505, Loss 0.00035294488770887256 \n Val Loss, 0.003610737156122923\nEpoch 506, Loss 0.0003489194205030799 \n Val Loss, 0.0036105040926486254\nEpoch 507, Loss 0.0003478907165117562 \n Val Loss, 0.0036105834878981113\nEpoch 508, Loss 0.0003476762503851205 \n Val Loss, 0.0036111255176365376\nEpoch 509, Loss 0.0003473467077128589 \n Val Loss, 0.003611384192481637\nEpoch 510, Loss 0.0003477444697637111 \n Val Loss, 0.0036104810424149036\nEpoch 511, Loss 0.0003484787594061345 \n Val Loss, 0.0036111674271523952\nEpoch 512, Loss 0.00035517034120857716 \n Val Loss, 0.0036118305288255215\nEpoch 513, Loss 0.0003476961865089834 \n Val Loss, 0.0036123041063547134\nEpoch 514, Loss 0.00034776245593093336 \n Val Loss, 0.003611084073781967\nEpoch 515, Loss 0.0003492332762107253 \n Val Loss, 0.003611119696870446\nEpoch 516, Loss 0.000347277702530846 \n Val Loss, 0.0036110850051045418\nEpoch 517, Loss 0.00034942355705425143 \n Val Loss, 0.0036112333182245493\nEpoch 518, Loss 0.0003475593402981758 \n Val Loss, 0.0036121378652751446\nEpoch 519, Loss 0.0003473693213891238 \n Val Loss, 0.003611798631027341\nEpoch 520, Loss 0.00034780235728248954 \n Val Loss, 0.003611265914514661\nEpoch 521, Loss 0.00034983723890036345 \n Val Loss, 0.003611181629821658\nEpoch 522, Loss 0.00034842060995288193 \n Val Loss, 0.003612061496824026\nEpoch 523, Loss 0.00035318254958838224 \n Val Loss, 0.0036119529977440834\nEpoch 524, Loss 0.0003485363267827779 \n Val Loss, 0.003611599560827017\nEpoch 525, Loss 0.0003485381603240967 \n Val Loss, 0.0036122475285083055\nEpoch 526, Loss 0.0003475241828709841 \n Val Loss, 0.0036117814015597105\nEpoch 527, Loss 0.0003472329117357731 \n Val Loss, 0.0036110745277255774\nEpoch 528, Loss 0.00034772499930113554 \n Val Loss, 0.0036117960698902607\nEpoch 529, Loss 0.0003476075944490731 \n Val Loss, 0.0036112454254180193\nEpoch 530, Loss 0.00034901656908914447 \n Val Loss, 0.0036116389092057943\nEpoch 531, Loss 0.0003490278322715312 \n Val Loss, 0.0036111045628786087\nEpoch 532, Loss 0.00034923668135888875 \n Val Loss, 0.0036120435688644648\nEpoch 533, Loss 0.0003482554166112095 \n Val Loss, 0.0036112889647483826\nEpoch 534, Loss 0.0003483072796370834 \n Val Loss, 0.003611472435295582\nEpoch 535, Loss 0.0003492655814625323 \n Val Loss, 0.0036118419375270605\nEpoch 536, Loss 0.0003478102444205433 \n Val Loss, 0.003611983498558402\nEpoch 537, Loss 0.00035221048165112734 \n Val Loss, 0.003611612133681774\nEpoch 538, Loss 0.00034959797631017864 \n Val Loss, 0.0036118170246481895\nEpoch 539, Loss 0.00034733180655166507 \n Val Loss, 0.003611775115132332\nEpoch 540, Loss 0.00034819607390090823 \n Val Loss, 0.003612769301980734\nEpoch 541, Loss 0.0003477701102383435 \n Val Loss, 0.003611604683101177\nEpoch 542, Loss 0.00034858722938224673 \n Val Loss, 0.0036122160963714123\nEpoch 543, Loss 0.0003482096653897315 \n Val Loss, 0.0036126202903687954\nEpoch 544, Loss 0.0003473452234175056 \n Val Loss, 0.0036119609139859676\nEpoch 545, Loss 0.0003493003605399281 \n Val Loss, 0.0036114035174250603\nEpoch 546, Loss 0.000347800349118188 \n Val Loss, 0.0036124533507972956\nEpoch 547, Loss 0.0003482905449345708 \n Val Loss, 0.0036119050346314907\nEpoch 548, Loss 0.00035005263634957373 \n Val Loss, 0.0036113013047724962\nEpoch 549, Loss 0.000347368186339736 \n Val Loss, 0.00361093832179904\nEpoch 550, Loss 0.00034998348564840853 \n Val Loss, 0.0036102659069001675\nEpoch 551, Loss 0.0003484079206828028 \n Val Loss, 0.0036115297116339207\nEpoch 552, Loss 0.00034773058723658323 \n Val Loss, 0.0036096652038395405\nEpoch 553, Loss 0.00034848181530833244 \n Val Loss, 0.0036098265554755926\nEpoch 554, Loss 0.0003558906610123813 \n Val Loss, 0.003610624698922038\nEpoch 555, Loss 0.0003488973597995937 \n Val Loss, 0.0036105443723499775\nEpoch 556, Loss 0.00034787834738381207 \n Val Loss, 0.003610231215134263\nEpoch 557, Loss 0.0003513292467687279 \n Val Loss, 0.003610368352383375\nEpoch 558, Loss 0.00034753064392134547 \n Val Loss, 0.00360981747508049\nEpoch 559, Loss 0.00034765974851325154 \n Val Loss, 0.0036103527527302504\nEpoch 560, Loss 0.00034857497666962445 \n Val Loss, 0.0036105613689869642\nEpoch 561, Loss 0.0003479663282632828 \n Val Loss, 0.0036098724231123924\nEpoch 562, Loss 0.00034758885158225894 \n Val Loss, 0.0036099720746278763\nEpoch 563, Loss 0.00034940196201205254 \n Val Loss, 0.0036100675351917744\nEpoch 564, Loss 0.00034799345303326845 \n Val Loss, 0.0036109115462750196\nEpoch 565, Loss 0.00034724551369436085 \n Val Loss, 0.003610451240092516\nEpoch 566, Loss 0.00034826667979359627 \n Val Loss, 0.0036104810424149036\nEpoch 567, Loss 0.00034973773290403187 \n Val Loss, 0.0036103534512221813\nEpoch 568, Loss 0.000348179426509887 \n Val Loss, 0.0036103962920606136\nEpoch 569, Loss 0.0003491579263936728 \n Val Loss, 0.003610998624935746\nEpoch 570, Loss 0.0003476194688118994 \n Val Loss, 0.003610502928495407\nEpoch 571, Loss 0.0003484254120849073 \n Val Loss, 0.003611094318330288\nEpoch 572, Loss 0.00034740695264190435 \n Val Loss, 0.0036105141043663025\nEpoch 573, Loss 0.0003472963871899992 \n Val Loss, 0.0036095662508159876\nEpoch 574, Loss 0.000347809458617121 \n Val Loss, 0.003610131097957492\nEpoch 575, Loss 0.0003481855965219438 \n Val Loss, 0.0036098589189350605\nEpoch 576, Loss 0.00035049652797169983 \n Val Loss, 0.0036103366874158382\nEpoch 577, Loss 0.0003489243099465966 \n Val Loss, 0.003610371146351099\nEpoch 578, Loss 0.00034773125662468374 \n Val Loss, 0.0036106170155107975\nEpoch 579, Loss 0.00034848900395445526 \n Val Loss, 0.003609795356169343\nEpoch 580, Loss 0.00035033334279432893 \n Val Loss, 0.0036106202751398087\nEpoch 581, Loss 0.0003494156408123672 \n Val Loss, 0.0036104870960116386\nEpoch 582, Loss 0.0003484897315502167 \n Val Loss, 0.003610242623835802\nEpoch 583, Loss 0.00034763809526339173 \n Val Loss, 0.0036101271398365498\nEpoch 584, Loss 0.00034901677281595767 \n Val Loss, 0.003610505722463131\nEpoch 585, Loss 0.00035028698039241135 \n Val Loss, 0.0036108174826949835\nEpoch 586, Loss 0.00034793344093486667 \n Val Loss, 0.0036115902476012707\nEpoch 587, Loss 0.0003494983830023557 \n Val Loss, 0.0036108861677348614\nEpoch 588, Loss 0.00034749446786008775 \n Val Loss, 0.0036105546168982983\nEpoch 589, Loss 0.00035093503538519144 \n Val Loss, 0.0036103972233831882\nEpoch 590, Loss 0.00034886333742178977 \n Val Loss, 0.0036100924480706453\nEpoch 591, Loss 0.00034797698026522994 \n Val Loss, 0.0036103150341659784\nEpoch 592, Loss 0.00034856199636124074 \n Val Loss, 0.003610006533563137\nEpoch 593, Loss 0.0003480013692751527 \n Val Loss, 0.003609510138630867\nEpoch 594, Loss 0.0003483682812657207 \n Val Loss, 0.0036103706806898117\nEpoch 595, Loss 0.0003475825651548803 \n Val Loss, 0.0036100028082728386\nEpoch 596, Loss 0.00034732642234303057 \n Val Loss, 0.003609814215451479\nEpoch 597, Loss 0.00034727310412563384 \n Val Loss, 0.003609008388593793\nEpoch 598, Loss 0.0003498926234897226 \n Val Loss, 0.0036096503026783466\nEpoch 599, Loss 0.0003501142782624811 \n Val Loss, 0.0036102295853197575\nEpoch 600, Loss 0.0003478634462226182 \n Val Loss, 0.0036100908182561398\nEpoch 601, Loss 0.0003480073355603963 \n Val Loss, 0.0036101157311350107\nEpoch 602, Loss 0.0003495459968689829 \n Val Loss, 0.00361015391536057\nEpoch 603, Loss 0.0003474044206086546 \n Val Loss, 0.003609660780057311\nEpoch 604, Loss 0.0003487349022179842 \n Val Loss, 0.0036096470430493355\nEpoch 605, Loss 0.0003474165569059551 \n Val Loss, 0.003610009793192148\nEpoch 606, Loss 0.0003479905135463923 \n Val Loss, 0.0036102510057389736\nEpoch 607, Loss 0.0003485650522634387 \n Val Loss, 0.0036096787080168724\nEpoch 608, Loss 0.00034801592119038105 \n Val Loss, 0.0036098305135965347\nEpoch 609, Loss 0.0003486554487608373 \n Val Loss, 0.003608793020248413\nEpoch 610, Loss 0.0003473233664408326 \n Val Loss, 0.003608881728723645\nEpoch 611, Loss 0.0003479913284536451 \n Val Loss, 0.003609546460211277\nEpoch 612, Loss 0.00034786202013492584 \n Val Loss, 0.003610089421272278\nEpoch 613, Loss 0.00034951986162923276 \n Val Loss, 0.0036097730044275522\nEpoch 614, Loss 0.0003485717752482742 \n Val Loss, 0.0036103571765124798\nEpoch 615, Loss 0.00034731809864751995 \n Val Loss, 0.0036106458865106106\nEpoch 616, Loss 0.0003506684151943773 \n Val Loss, 0.003609428647905588\nEpoch 617, Loss 0.0003476772690191865 \n Val Loss, 0.0036092731170356274\nEpoch 618, Loss 0.00034745424636639655 \n Val Loss, 0.003608745988458395\nEpoch 619, Loss 0.00034779272391460836 \n Val Loss, 0.0036089387722313404\nEpoch 620, Loss 0.0003474684781394899 \n Val Loss, 0.0036083953455090523\nEpoch 621, Loss 0.0003496518183965236 \n Val Loss, 0.0036092475056648254\nEpoch 622, Loss 0.00034721632255241275 \n Val Loss, 0.0036088316701352596\nEpoch 623, Loss 0.0003476905112620443 \n Val Loss, 0.0036088069900870323\nEpoch 624, Loss 0.0003474488912615925 \n Val Loss, 0.003608949016779661\nEpoch 625, Loss 0.0003483858599793166 \n Val Loss, 0.003608441911637783\nEpoch 626, Loss 0.0003477719146758318 \n Val Loss, 0.0036089704371988773\nEpoch 627, Loss 0.0003486504137981683 \n Val Loss, 0.003608794417232275\nEpoch 628, Loss 0.0003481493622530252 \n Val Loss, 0.0036097285337746143\nEpoch 629, Loss 0.0003478125436231494 \n Val Loss, 0.003608050523325801\nEpoch 630, Loss 0.00035059903166256845 \n Val Loss, 0.0036084239836782217\nEpoch 631, Loss 0.00034848530776798725 \n Val Loss, 0.00360825564712286\nEpoch 632, Loss 0.0003476007259450853 \n Val Loss, 0.0036087436601519585\nEpoch 633, Loss 0.00034742598654702306 \n Val Loss, 0.003608421189710498\nEpoch 634, Loss 0.00034777092514559627 \n Val Loss, 0.003608777653425932\nEpoch 635, Loss 0.00034942873753607273 \n Val Loss, 0.003609581384807825\nEpoch 636, Loss 0.00034832232631742954 \n Val Loss, 0.0036082544829696417\nEpoch 637, Loss 0.0003476698184385896 \n Val Loss, 0.003608102211728692\nEpoch 638, Loss 0.0003496558638289571 \n Val Loss, 0.003607960185036063\nEpoch 639, Loss 0.0003505762724671513 \n Val Loss, 0.003609169041737914\nEpoch 640, Loss 0.00035271310480311513 \n Val Loss, 0.0036088628694415092\nEpoch 641, Loss 0.0003487871726974845 \n Val Loss, 0.0036084591411054134\nEpoch 642, Loss 0.00035078777000308037 \n Val Loss, 0.0036098926793783903\nEpoch 643, Loss 0.0003488542861305177 \n Val Loss, 0.0036088409833610058\nEpoch 644, Loss 0.00034941837657243013 \n Val Loss, 0.0036093811504542828\nEpoch 645, Loss 0.00034727700403891504 \n Val Loss, 0.0036079920828342438\nEpoch 646, Loss 0.0003524328349158168 \n Val Loss, 0.003608481492847204\nEpoch 647, Loss 0.0003476400161162019 \n Val Loss, 0.0036089166533201933\nEpoch 648, Loss 0.00035307180951349437 \n Val Loss, 0.0036076775286346674\nEpoch 649, Loss 0.0003487661306280643 \n Val Loss, 0.0036070290952920914\nEpoch 650, Loss 0.00034885856439359486 \n Val Loss, 0.0036073168739676476\nEpoch 651, Loss 0.00034737426904030144 \n Val Loss, 0.0036072421353310347\nEpoch 652, Loss 0.0003516752040013671 \n Val Loss, 0.003607543185353279\nEpoch 653, Loss 0.00034946162486448884 \n Val Loss, 0.0036078672856092453\nEpoch 654, Loss 0.0003491816751193255 \n Val Loss, 0.00360881257802248\nEpoch 655, Loss 0.00034728762693703175 \n Val Loss, 0.0036089220084249973\nEpoch 656, Loss 0.0003517626610118896 \n Val Loss, 0.0036086533218622208\nEpoch 657, Loss 0.00034836758277378976 \n Val Loss, 0.0036083697341382504\nEpoch 658, Loss 0.0003490881936158985 \n Val Loss, 0.0036089990753680468\nEpoch 659, Loss 0.00035054358886554837 \n Val Loss, 0.0036093478556722403\nEpoch 660, Loss 0.0003475971461739391 \n Val Loss, 0.003609288949519396\nEpoch 661, Loss 0.00034936206066049635 \n Val Loss, 0.003608816070482135\nEpoch 662, Loss 0.0003473921096883714 \n Val Loss, 0.0036094929091632366\nEpoch 663, Loss 0.00035084763658232987 \n Val Loss, 0.0036092484369874\nEpoch 664, Loss 0.00034902378683909774 \n Val Loss, 0.003609550418332219\nEpoch 665, Loss 0.00034818577114492655 \n Val Loss, 0.0036090100184082985\nEpoch 666, Loss 0.0003489889786578715 \n Val Loss, 0.0036085895262658596\nEpoch 667, Loss 0.0003496524295769632 \n Val Loss, 0.0036087422631680965\nEpoch 668, Loss 0.0003477482823655009 \n Val Loss, 0.0036091329529881477\nEpoch 669, Loss 0.00035178809775970876 \n Val Loss, 0.0036085708998143673\nEpoch 670, Loss 0.0003481315798126161 \n Val Loss, 0.0036082733422517776\nEpoch 671, Loss 0.00034781653084792197 \n Val Loss, 0.0036082712467759848\nEpoch 672, Loss 0.0003487797803245485 \n Val Loss, 0.0036077953409403563\nEpoch 673, Loss 0.00034799822606146336 \n Val Loss, 0.003607948776334524\nEpoch 674, Loss 0.0003499195445328951 \n Val Loss, 0.0036073587834835052\nEpoch 675, Loss 0.0003479456645436585 \n Val Loss, 0.0036067748442292213\nEpoch 676, Loss 0.00034809260978363454 \n Val Loss, 0.003608037019148469\nEpoch 677, Loss 0.0003552776761353016 \n Val Loss, 0.003608410945162177\nEpoch 678, Loss 0.0003483797190710902 \n Val Loss, 0.003607870312407613\nEpoch 679, Loss 0.00034998953924514353 \n Val Loss, 0.003608172060921788\nEpoch 680, Loss 0.00034729743492789567 \n Val Loss, 0.0036080542486160994\nEpoch 681, Loss 0.0003490084782242775 \n Val Loss, 0.003608015365898609\nEpoch 682, Loss 0.00034924771171063185 \n Val Loss, 0.003607506398111582\nEpoch 683, Loss 0.00034796734689734876 \n Val Loss, 0.0036076148971915245\nEpoch 684, Loss 0.0003492319956421852 \n Val Loss, 0.003607453079894185\nEpoch 685, Loss 0.00034721565316431224 \n Val Loss, 0.0036083499435335398\nEpoch 686, Loss 0.0003492589166853577 \n Val Loss, 0.0036081625148653984\nEpoch 687, Loss 0.0003484562912490219 \n Val Loss, 0.003608293365687132\nEpoch 688, Loss 0.0003475795383565128 \n Val Loss, 0.0036079008132219315\nEpoch 689, Loss 0.00034863463952206075 \n Val Loss, 0.003609225619584322\nEpoch 690, Loss 0.0003498357255011797 \n Val Loss, 0.003608507104218006\nEpoch 691, Loss 0.0003604407829698175 \n Val Loss, 0.0036088419146835804\nEpoch 692, Loss 0.0003487691283226013 \n Val Loss, 0.0036092540249228477\nEpoch 693, Loss 0.0003485140041448176 \n Val Loss, 0.0036083015147596598\nEpoch 694, Loss 0.00034856164711527526 \n Val Loss, 0.003608448663726449\nEpoch 695, Loss 0.0003473300312180072 \n Val Loss, 0.0036085695028305054\nEpoch 696, Loss 0.0003492316172923893 \n Val Loss, 0.0036082526203244925\nEpoch 697, Loss 0.0003488308866508305 \n Val Loss, 0.003608145983889699\nEpoch 698, Loss 0.0003476872807368636 \n Val Loss, 0.003608505707234144\nEpoch 699, Loss 0.0003471678064670414 \n Val Loss, 0.00360860675573349\nEpoch 700, Loss 0.000348276284057647 \n Val Loss, 0.003609074279665947\nEpoch 701, Loss 0.000347368506481871 \n Val Loss, 0.0036090118810534477\nEpoch 702, Loss 0.00034725142177194357 \n Val Loss, 0.003609083127230406\nEpoch 703, Loss 0.0003476784913800657 \n Val Loss, 0.003608832135796547\nEpoch 704, Loss 0.00034753483487293124 \n Val Loss, 0.003609237726777792\nEpoch 705, Loss 0.00035110305179841816 \n Val Loss, 0.003608179511502385\nEpoch 706, Loss 0.00034852290991693735 \n Val Loss, 0.0036076803226023912\nEpoch 707, Loss 0.0003472698444966227 \n Val Loss, 0.0036079110577702522\nEpoch 708, Loss 0.00034766641329042614 \n Val Loss, 0.003607797669246793\nEpoch 709, Loss 0.0003486812056507915 \n Val Loss, 0.003608288476243615\nEpoch 710, Loss 0.00034734682412818074 \n Val Loss, 0.0036083022132515907\nEpoch 711, Loss 0.00034787639742717147 \n Val Loss, 0.0036087993066757917\nEpoch 712, Loss 0.0003498257137835026 \n Val Loss, 0.0036091122310608625\nEpoch 713, Loss 0.0003482443280518055 \n Val Loss, 0.003609167877584696\nEpoch 714, Loss 0.0003580178308766335 \n Val Loss, 0.0036083657760173082\nEpoch 715, Loss 0.00034814339596778154 \n Val Loss, 0.0036082598380744457\nEpoch 716, Loss 0.0003501943137962371 \n Val Loss, 0.0036090442445129156\nEpoch 717, Loss 0.00034714193316176534 \n Val Loss, 0.0036081350408494473\nEpoch 718, Loss 0.00034745424636639655 \n Val Loss, 0.003608513390645385\nEpoch 719, Loss 0.0003482622269075364 \n Val Loss, 0.003608115017414093\nEpoch 720, Loss 0.00035335065331310034 \n Val Loss, 0.003609579289332032\nEpoch 721, Loss 0.0003479122242424637 \n Val Loss, 0.003608855651691556\nEpoch 722, Loss 0.0003471743839327246 \n Val Loss, 0.0036078174598515034\nEpoch 723, Loss 0.000347045308444649 \n Val Loss, 0.003608529455959797\nEpoch 724, Loss 0.0003478470316622406 \n Val Loss, 0.003608454018831253\nEpoch 725, Loss 0.0003491962852422148 \n Val Loss, 0.0036084908060729504\nEpoch 726, Loss 0.00034733262145891786 \n Val Loss, 0.003608421655371785\nEpoch 727, Loss 0.00034812462399713695 \n Val Loss, 0.003608359955251217\nEpoch 728, Loss 0.00034784909803420305 \n Val Loss, 0.003609156236052513\nEpoch 729, Loss 0.0003481301246210933 \n Val Loss, 0.0036085229367017746\nEpoch 730, Loss 0.0003472900716587901 \n Val Loss, 0.0036082803271710873\nEpoch 731, Loss 0.0003510897804517299 \n Val Loss, 0.003608755301684141\nEpoch 732, Loss 0.0003472987446002662 \n Val Loss, 0.0036078598350286484\nEpoch 733, Loss 0.0003486375499051064 \n Val Loss, 0.0036077615804970264\nEpoch 734, Loss 0.00034874497214332223 \n Val Loss, 0.0036080656573176384\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-df952353f5df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 974\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    975\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    939\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 941\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    942\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# unserialize the data after having released the lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mqsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/multiprocessing/reductions.py\u001b[0m in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrebuild_storage_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m     \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage_from_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mdetach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;34m'''Get the fd.  This should only be called once.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0m_resource_sharer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mget_connection\u001b[0;34m(ident)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mident\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mClient\u001b[0;34m(address, family, authkey)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mauthkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m         \u001b[0manswer_challenge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0mdeliver_challenge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36manswer_challenge\u001b[0;34m(connection, authkey)\u001b[0m\n\u001b[1;32m    730\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mhmac\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauthkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m     \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# reject large message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHALLENGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mCHALLENGE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'message = %r'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m     \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHALLENGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv_bytes\u001b[0;34m(self, maxlength)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxlength\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmaxlength\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"negative maxlength\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bad_message_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxsize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lowest_val_score = float('inf')\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, data in enumerate(train_data_loader):\n",
    "        features = data['features']\n",
    "        targets = data['target']\n",
    "\n",
    "        features = features.to(DEVICE).float()\n",
    "        targets = targets.to(DEVICE).float()\n",
    "\n",
    "        model.zero_grad()\n",
    "        out = model(features)\n",
    "        loss = quantile_loss(out, targets, QUANTILES)\n",
    "        total_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # scheduler.step()\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(valid_data_loader):\n",
    "            features = data['features']\n",
    "            targets = data['target']\n",
    "\n",
    "            features = features.to(DEVICE).float()\n",
    "            targets = targets.to(DEVICE).float()\n",
    "\n",
    "            out = model(features)\n",
    "            loss = quantile_loss(out, targets, QUANTILES)\n",
    "            val_loss += loss\n",
    "    \n",
    "    if val_loss < lowest_val_score:\n",
    "        lowest_val_score = val_loss\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, os.path.join(CONFIG.CFG.DATA.MODELS_OUT, \"best_model.pt\"))\n",
    "\n",
    "    if lr_scheduler is not None:\n",
    "        lr_scheduler.step(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch}, Loss {total_loss/len(train_data_loader)} \\n Val Loss, {val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}, os.path.join(CONFIG.CFG.DATA.MODELS_OUT, \"model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG.upload_to_kaggle(\"osicqrmodel\", \"OSIC QR Model\", new=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for j, test_data in enumerate(test_data_loader):\n",
    "        features = test_data['features']\n",
    "        targets = test_data['target']\n",
    "\n",
    "        features = features.to(DEVICE).float()\n",
    "        targets = targets.to(DEVICE).float()\n",
    "\n",
    "        out = model(features)\n",
    "        for ou in out.cpu().numpy().tolist():\n",
    "            preds.append(ou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse the scaling operation for FVC\n",
    "preds -= MIN_MAX_SCALER.min_[SCALE_COLUMNS.index('FVC')]\n",
    "preds /= MIN_MAX_SCALER.scale_[SCALE_COLUMNS.index('FVC')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[3013.62531543, 3013.3719101 , 3014.14839983],\n       [3014.33255875, 3014.23491633, 3014.85115957],\n       [3015.03996813, 3015.09792256, 3015.55408537],\n       ...,\n       [2925.18187261, 2922.9485513 , 2946.21169221],\n       [2924.96333957, 2922.73516607, 2946.29405725],\n       [2924.74447441, 2922.52178085, 2946.37625623]])"
     },
     "metadata": {},
     "execution_count": 106
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}