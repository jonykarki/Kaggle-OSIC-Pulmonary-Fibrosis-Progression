{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "if os.path.abspath(os.pardir) not in sys.path:\n",
    "    sys.path.insert(0, os.path.abspath(os.pardir))\n",
    "import CONFIG\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pydicom\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = CONFIG.CFG.DATA.BASE\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "QUANTILES = [0.2, 0.5, 0.8]\n",
    "SCALE_COLUMNS = ['Weeks', 'FVC', 'Percent', 'Age'] #'Percent'\n",
    "SCALE_COLUMNS = ['Weeks_Passed', 'Base_FVC', 'Base_Percent', 'Base_Age']\n",
    "SEX_COLUMNS = ['Male', 'Female']\n",
    "SMOKING_STATUS_COLUMNS = ['Currently smokes', 'Ex-smoker', 'Never smoked']\n",
    "FV = SEX_COLUMNS + SMOKING_STATUS_COLUMNS + SCALE_COLUMNS\n",
    "\n",
    "# number of images used to create a single 3D array of the scan\n",
    "NUM_IMAGES = 8\n",
    "IMG_SIZE = 256\n",
    "K_FOLDS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = model_selection.KFold(K_FOLDS)\n",
    "MIN_MAX_SCALER = preprocessing.MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(DATA_DIR, \"test.csv\"))\n",
    "sub_df = pd.read_csv(os.path.join(DATA_DIR, \"sample_submission.csv\"))\n",
    "# remove the duplicates from the train_df\n",
    "train_df.drop_duplicates(keep=False, inplace=True, subset=['Patient', 'Weeks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the Patient and weeks from the Patient_Week column\n",
    "sub_df['Patient'] = sub_df['Patient_Week'].apply(lambda x: x.split('_')[0])\n",
    "sub_df['Weeks'] = sub_df['Patient_Week'].apply(lambda x: int(x.split('_')[-1]))\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the sub_df with the test_df\n",
    "sub_df = sub_df.drop('FVC', axis=1).merge(test_df.drop('Weeks', axis=1), on='Patient')\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['FROM'] = 'train'\n",
    "test_df['FROM'] = 'val'\n",
    "sub_df['FROM'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = train_df.append([test_df, sub_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize base_week column\n",
    "combined_df['Base_Week'] = combined_df['Weeks']\n",
    "# make the weeks from sub_df to be np.nan so that when we calculate the base_week it comes from the test_df\n",
    "combined_df.loc[combined_df['FROM'] == 'test', 'Base_Week'] = np.nan\n",
    "# now calculate the min for each patient group and set it to the Base_Week column\n",
    "combined_df['Base_Week'] = combined_df.groupby('Patient')['Base_Week'].transform('min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the base_df (where the Base_Week == the min_week we calculated) so that we can get the base_fvc, base_age and base_percentage\n",
    "base_df = combined_df[combined_df['Weeks'] == combined_df['Base_Week']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_df.rename(columns={\n",
    "    'FVC': 'Base_FVC',\n",
    "    'Percent': 'Base_Percent',\n",
    "    'Age': 'Base_Age'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.merge(base_df[['Patient', 'Base_FVC', 'Base_Percent', 'Base_Age']], on='Patient', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['Weeks_Passed'] = combined_df['Weeks'] - combined_df['Base_Week']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_MAX_SCALER.fit(combined_df[combined_df['FROM'] == 'train'][['Weeks_Passed', 'FVC', 'Percent', 'Age']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df[['Weeks_Passed', 'Base_FVC', 'Base_Percent', 'Base_Age']] = MIN_MAX_SCALER.transform(combined_df[['Weeks_Passed', 'Base_FVC', 'Base_Percent', 'Base_Age']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert categoricals into dummies\n",
    "combined_df['Sex'] = pd.Categorical(combined_df['Sex'], categories=SEX_COLUMNS)\n",
    "combined_df['SmokingStatus'] = pd.Categorical(combined_df['SmokingStatus'], categories=SMOKING_STATUS_COLUMNS)\n",
    "combined_df = combined_df.join(pd.get_dummies(combined_df['Sex']))\n",
    "combined_df = combined_df.join(pd.get_dummies(combined_df['SmokingStatus']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATIENTS = train_df['Patient'].unique().tolist()\n",
    "# gave the gdcm error\n",
    "BAD_PATIENT_IDS = ['ID00011637202177653955184', 'ID00052637202186188008618']\n",
    "ALL_TRAIN_PATIENTS = [pat for pat in TRAIN_PATIENTS if pat not in BAD_PATIENT_IDS]\n",
    "ALL_TEST_PATIENTS = test_df['Patient'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_averaged_slices(patient_id, folder_path, num_images):\n",
    "    # the preprocessed array with NUM_SLICES elements\n",
    "    # TODO: Handle the case when the NUM_SLICES > the actual total slices\n",
    "    # TODO: resize the image to 256 X 256?\n",
    "\n",
    "    full_path = os.path.join(folder_path, patient_id)\n",
    "    # list of all files in that path and sort them\n",
    "    all_files = os.listdir(full_path)\n",
    "    # sorted using the first number part of the file name\n",
    "    all_files.sort(key = lambda x: int(x.split('.')[0]))\n",
    "\n",
    "    # read all the dicom files for the patient into the slices list\n",
    "    slices = [pydicom.read_file(os.path.join(full_path, s)) for s in all_files]\n",
    "    # sort the slices using their order (file number works too)\n",
    "    # slices.sort(key = lambda x: int(x.ImagePositionPatient[2]))\n",
    "\n",
    "    # final array containing averaged num_images images\n",
    "    out_array = []\n",
    "\n",
    "    # how many extra files while averaging all images into (num_images) images\n",
    "    remainder_array_size = len(slices)%num_images\n",
    "\n",
    "    # how many to average to get a single averaged image\n",
    "    avging_array_size = len(slices)//num_images\n",
    "\n",
    "    # get the first one with the remainder images\n",
    "    first_array = []\n",
    "    # select the first remainder + avg_arrray_size imgaes and average into one\n",
    "    for slice in slices[:remainder_array_size+avging_array_size]:\n",
    "        first_array.append(slice.pixel_array)\n",
    "    first_avged_array = np.average(first_array, axis=0)\n",
    "    first_resized = cv2.resize(first_avged_array, (IMG_SIZE, IMG_SIZE))\n",
    "    out_array.append(first_resized)\n",
    "\n",
    "    # after the first one get the remaining ones into out_array rolling averaging (avging_array_size) at a time.\n",
    "    for i in range(remainder_array_size + avging_array_size, len(slices), avging_array_size):\n",
    "        temp_array = []\n",
    "        for slice in slices[i:i+avging_array_size]:\n",
    "            temp_array.append(slice.pixel_array)\n",
    "        avged_temp_array = np.average(temp_array, axis=0)\n",
    "        avged_resized = cv2.resize(avged_temp_array, (IMG_SIZE, IMG_SIZE))\n",
    "        out_array.append(avged_resized)\n",
    "    \n",
    "    return np.array(out_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_from_id = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the train and test images in array_from_id\n",
    "for id in ALL_TRAIN_PATIENTS:\n",
    "    array_from_id[id] = get_averaged_slices(id, os.path.join(DATA_DIR, \"train\"), NUM_IMAGES)\n",
    "\n",
    "for id in ALL_TEST_PATIENTS:\n",
    "    array_from_id[id] = get_averaged_slices(id, os.path.join(DATA_DIR, \"test\"), NUM_IMAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PulmonaryDataset(Dataset):\n",
    "    def __init__(self, df, FV, test=False):\n",
    "        self.df = df\n",
    "        self.test = test\n",
    "        self.FV = FV\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'imgarray': torch.from_numpy(array_from_id[self.df.iloc[idx]['Patient']]).unsqueeze(0),\n",
    "            # 'tabfeatures': torch.tensor(self.df[self.FV].iloc[idx].values),\n",
    "            'target': torch.tensor(self.df['FVC'].iloc[idx])\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PulmonaryModel(nn.Module):\n",
    "    def __init__(self, cnn_output_size=10, in_features=9, out_quantiles=3):\n",
    "        super(PulmonaryModel, self).__init__()\n",
    "\n",
    "        self.conv_layer1 = self._make_conv_layer(1, 8)\n",
    "        self.conv_layer2 = self._make_conv_layer(8, 32)\n",
    "        self.conv_layer3 = self._make_conv_layer(32, 64)\n",
    "        self.conv_layer4 = nn.Conv3d(64, 128, kernel_size=(1, 3, 3))\n",
    "        self.conv_layer5 = nn.Conv3d(128, 128, kernel_size=(1,3,3), padding=0)\n",
    "\n",
    "        self.fc1 = nn.Linear(86528, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, cnn_output_size)\n",
    "\n",
    "    def _make_conv_layer(self, in_c, out_c):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv3d(in_c, out_c, kernel_size=(2,3,3), padding=0),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv3d(out_c, out_c, kernel_size=(2, 3, 3), padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool3d((2,2,2)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer1(x)\n",
    "        x = self.conv_layer2(x)\n",
    "        x = self.conv_layer3(x)\n",
    "        x = self.conv_layer4(x)\n",
    "        x = self.conv_layer5(x)\n",
    "\n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PulmonaryModel()\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PulmonaryDataset(train_df, FV, test=False)\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, data in enumerate(train_data_loader):\n",
    "    imgarray = data['imgarray'].to(DEVICE).float()\n",
    "    out = model(imgarray)\n",
    "    print(out)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for key in array_from_id:\n",
    "    print(array_from_id[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}