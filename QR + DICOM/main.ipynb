{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "if os.path.abspath(os.pardir) not in sys.path:\n",
    "    sys.path.insert(0, os.path.abspath(os.pardir))\n",
    "import CONFIG\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pydicom\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = CONFIG.CFG.DATA.BASE\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "QUANTILES = [0.2, 0.5, 0.8]\n",
    "SCALE_COLUMNS = ['Weeks', 'FVC', 'Percent', 'Age'] #'Percent'\n",
    "SCALE_COLUMNS = ['Weeks_Passed', 'Base_FVC', 'Base_Percent', 'Base_Age']\n",
    "SEX_COLUMNS = ['Male', 'Female']\n",
    "SMOKING_STATUS_COLUMNS = ['Currently smokes', 'Ex-smoker', 'Never smoked']\n",
    "FV = SEX_COLUMNS + SMOKING_STATUS_COLUMNS + SCALE_COLUMNS\n",
    "\n",
    "# number of images used to create a single 3D array of the scan\n",
    "NUM_IMAGES = 8\n",
    "IMG_SIZE = 256\n",
    "K_FOLDS = 5\n",
    "LEARNING_RATE = 4e-5\n",
    "NUM_EPOCHS = 1000\n",
    "PRINT_EVERY = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = model_selection.KFold(K_FOLDS)\n",
    "MIN_MAX_SCALER = preprocessing.MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(DATA_DIR, \"test.csv\"))\n",
    "sub_df = pd.read_csv(os.path.join(DATA_DIR, \"sample_submission.csv\"))\n",
    "# remove the duplicates from the train_df\n",
    "train_df.drop_duplicates(keep=False, inplace=True, subset=['Patient', 'Weeks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Patient_Week</th>\n      <th>FVC</th>\n      <th>Confidence</th>\n      <th>Patient</th>\n      <th>Weeks</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ID00419637202311204720264_-12</td>\n      <td>2000</td>\n      <td>100</td>\n      <td>ID00419637202311204720264</td>\n      <td>-12</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ID00421637202311550012437_-12</td>\n      <td>2000</td>\n      <td>100</td>\n      <td>ID00421637202311550012437</td>\n      <td>-12</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ID00422637202311677017371_-12</td>\n      <td>2000</td>\n      <td>100</td>\n      <td>ID00422637202311677017371</td>\n      <td>-12</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ID00423637202312137826377_-12</td>\n      <td>2000</td>\n      <td>100</td>\n      <td>ID00423637202312137826377</td>\n      <td>-12</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ID00426637202313170790466_-12</td>\n      <td>2000</td>\n      <td>100</td>\n      <td>ID00426637202313170790466</td>\n      <td>-12</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                    Patient_Week   FVC  ...                    Patient Weeks\n0  ID00419637202311204720264_-12  2000  ...  ID00419637202311204720264   -12\n1  ID00421637202311550012437_-12  2000  ...  ID00421637202311550012437   -12\n2  ID00422637202311677017371_-12  2000  ...  ID00422637202311677017371   -12\n3  ID00423637202312137826377_-12  2000  ...  ID00423637202312137826377   -12\n4  ID00426637202313170790466_-12  2000  ...  ID00426637202313170790466   -12\n\n[5 rows x 5 columns]"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# extract the Patient and weeks from the Patient_Week column\n",
    "sub_df['Patient'] = sub_df['Patient_Week'].apply(lambda x: x.split('_')[0])\n",
    "sub_df['Weeks'] = sub_df['Patient_Week'].apply(lambda x: int(x.split('_')[-1]))\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Patient_Week</th>\n      <th>Confidence</th>\n      <th>Patient</th>\n      <th>Weeks</th>\n      <th>FVC</th>\n      <th>Percent</th>\n      <th>Age</th>\n      <th>Sex</th>\n      <th>SmokingStatus</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ID00419637202311204720264_-12</td>\n      <td>100</td>\n      <td>ID00419637202311204720264</td>\n      <td>-12</td>\n      <td>3020</td>\n      <td>70.186855</td>\n      <td>73</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ID00419637202311204720264_-11</td>\n      <td>100</td>\n      <td>ID00419637202311204720264</td>\n      <td>-11</td>\n      <td>3020</td>\n      <td>70.186855</td>\n      <td>73</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ID00419637202311204720264_-10</td>\n      <td>100</td>\n      <td>ID00419637202311204720264</td>\n      <td>-10</td>\n      <td>3020</td>\n      <td>70.186855</td>\n      <td>73</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ID00419637202311204720264_-9</td>\n      <td>100</td>\n      <td>ID00419637202311204720264</td>\n      <td>-9</td>\n      <td>3020</td>\n      <td>70.186855</td>\n      <td>73</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ID00419637202311204720264_-8</td>\n      <td>100</td>\n      <td>ID00419637202311204720264</td>\n      <td>-8</td>\n      <td>3020</td>\n      <td>70.186855</td>\n      <td>73</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                    Patient_Week  Confidence  ...   Sex  SmokingStatus\n0  ID00419637202311204720264_-12         100  ...  Male      Ex-smoker\n1  ID00419637202311204720264_-11         100  ...  Male      Ex-smoker\n2  ID00419637202311204720264_-10         100  ...  Male      Ex-smoker\n3   ID00419637202311204720264_-9         100  ...  Male      Ex-smoker\n4   ID00419637202311204720264_-8         100  ...  Male      Ex-smoker\n\n[5 rows x 9 columns]"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# merge the sub_df with the test_df\n",
    "sub_df = sub_df.drop('FVC', axis=1).merge(test_df.drop('Weeks', axis=1), on='Patient')\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['FROM'] = 'train'\n",
    "test_df['FROM'] = 'val'\n",
    "sub_df['FROM'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = train_df.append([test_df, sub_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize base_week column\n",
    "combined_df['Base_Week'] = combined_df['Weeks']\n",
    "# make the weeks from sub_df to be np.nan so that when we calculate the base_week it comes from the test_df\n",
    "combined_df.loc[combined_df['FROM'] == 'test', 'Base_Week'] = np.nan\n",
    "# now calculate the min for each patient group and set it to the Base_Week column\n",
    "combined_df['Base_Week'] = combined_df.groupby('Patient')['Base_Week'].transform('min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the base_df (where the Base_Week == the min_week we calculated) so that we can get the base_fvc, base_age and base_percentage\n",
    "base_df = combined_df[combined_df['Weeks'] == combined_df['Base_Week']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:4133: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  errors=errors,\n"
    }
   ],
   "source": [
    "base_df.rename(columns={\n",
    "    'FVC': 'Base_FVC',\n",
    "    'Percent': 'Base_Percent',\n",
    "    'Age': 'Base_Age'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.merge(base_df[['Patient', 'Base_FVC', 'Base_Percent', 'Base_Age']], on='Patient', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['Weeks_Passed'] = combined_df['Weeks'] - combined_df['Base_Week']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "MinMaxScaler(copy=True, feature_range=(0, 1))"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "MIN_MAX_SCALER.fit(combined_df[combined_df['FROM'] == 'train'][['Weeks_Passed', 'FVC', 'Percent', 'Age']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df[['Weeks_Passed', 'Base_FVC', 'Base_Percent', 'Base_Age']] = MIN_MAX_SCALER.transform(combined_df[['Weeks_Passed', 'Base_FVC', 'Base_Percent', 'Base_Age']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert categoricals into dummies\n",
    "combined_df['Sex'] = pd.Categorical(combined_df['Sex'], categories=SEX_COLUMNS)\n",
    "combined_df['SmokingStatus'] = pd.Categorical(combined_df['SmokingStatus'], categories=SMOKING_STATUS_COLUMNS)\n",
    "combined_df = combined_df.join(pd.get_dummies(combined_df['Sex']))\n",
    "combined_df = combined_df.join(pd.get_dummies(combined_df['SmokingStatus']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Patient</th>\n      <th>Weeks</th>\n      <th>FVC</th>\n      <th>Percent</th>\n      <th>Age</th>\n      <th>Sex</th>\n      <th>SmokingStatus</th>\n      <th>FROM</th>\n      <th>Patient_Week</th>\n      <th>Confidence</th>\n      <th>Base_Week</th>\n      <th>Base_FVC</th>\n      <th>Base_Percent</th>\n      <th>Base_Age</th>\n      <th>Weeks_Passed</th>\n      <th>Male</th>\n      <th>Female</th>\n      <th>Currently smokes</th>\n      <th>Ex-smoker</th>\n      <th>Never smoked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ID00007637202177411956430</td>\n      <td>-4</td>\n      <td>2315</td>\n      <td>58.253649</td>\n      <td>79</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n      <td>train</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-4.0</td>\n      <td>0.267050</td>\n      <td>0.236393</td>\n      <td>0.769231</td>\n      <td>0.000000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ID00007637202177411956430</td>\n      <td>5</td>\n      <td>2214</td>\n      <td>55.712129</td>\n      <td>79</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n      <td>train</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-4.0</td>\n      <td>0.267050</td>\n      <td>0.236393</td>\n      <td>0.769231</td>\n      <td>0.142857</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ID00007637202177411956430</td>\n      <td>7</td>\n      <td>2061</td>\n      <td>51.862104</td>\n      <td>79</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n      <td>train</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-4.0</td>\n      <td>0.267050</td>\n      <td>0.236393</td>\n      <td>0.769231</td>\n      <td>0.174603</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ID00007637202177411956430</td>\n      <td>9</td>\n      <td>2144</td>\n      <td>53.950679</td>\n      <td>79</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n      <td>train</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-4.0</td>\n      <td>0.267050</td>\n      <td>0.236393</td>\n      <td>0.769231</td>\n      <td>0.206349</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ID00007637202177411956430</td>\n      <td>11</td>\n      <td>2069</td>\n      <td>52.063412</td>\n      <td>79</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n      <td>train</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-4.0</td>\n      <td>0.267050</td>\n      <td>0.236393</td>\n      <td>0.769231</td>\n      <td>0.238095</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2265</th>\n      <td>ID00426637202313170790466</td>\n      <td>129</td>\n      <td>2925</td>\n      <td>71.824968</td>\n      <td>73</td>\n      <td>Male</td>\n      <td>Never smoked</td>\n      <td>test</td>\n      <td>ID00426637202313170790466_129</td>\n      <td>100.0</td>\n      <td>0.0</td>\n      <td>0.376525</td>\n      <td>0.345604</td>\n      <td>0.615385</td>\n      <td>2.047619</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2266</th>\n      <td>ID00426637202313170790466</td>\n      <td>130</td>\n      <td>2925</td>\n      <td>71.824968</td>\n      <td>73</td>\n      <td>Male</td>\n      <td>Never smoked</td>\n      <td>test</td>\n      <td>ID00426637202313170790466_130</td>\n      <td>100.0</td>\n      <td>0.0</td>\n      <td>0.376525</td>\n      <td>0.345604</td>\n      <td>0.615385</td>\n      <td>2.063492</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2267</th>\n      <td>ID00426637202313170790466</td>\n      <td>131</td>\n      <td>2925</td>\n      <td>71.824968</td>\n      <td>73</td>\n      <td>Male</td>\n      <td>Never smoked</td>\n      <td>test</td>\n      <td>ID00426637202313170790466_131</td>\n      <td>100.0</td>\n      <td>0.0</td>\n      <td>0.376525</td>\n      <td>0.345604</td>\n      <td>0.615385</td>\n      <td>2.079365</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2268</th>\n      <td>ID00426637202313170790466</td>\n      <td>132</td>\n      <td>2925</td>\n      <td>71.824968</td>\n      <td>73</td>\n      <td>Male</td>\n      <td>Never smoked</td>\n      <td>test</td>\n      <td>ID00426637202313170790466_132</td>\n      <td>100.0</td>\n      <td>0.0</td>\n      <td>0.376525</td>\n      <td>0.345604</td>\n      <td>0.615385</td>\n      <td>2.095238</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2269</th>\n      <td>ID00426637202313170790466</td>\n      <td>133</td>\n      <td>2925</td>\n      <td>71.824968</td>\n      <td>73</td>\n      <td>Male</td>\n      <td>Never smoked</td>\n      <td>test</td>\n      <td>ID00426637202313170790466_133</td>\n      <td>100.0</td>\n      <td>0.0</td>\n      <td>0.376525</td>\n      <td>0.345604</td>\n      <td>0.615385</td>\n      <td>2.111111</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>2270 rows Ã— 20 columns</p>\n</div>",
      "text/plain": "                        Patient  Weeks  ...  Ex-smoker  Never smoked\n0     ID00007637202177411956430     -4  ...          1             0\n1     ID00007637202177411956430      5  ...          1             0\n2     ID00007637202177411956430      7  ...          1             0\n3     ID00007637202177411956430      9  ...          1             0\n4     ID00007637202177411956430     11  ...          1             0\n...                         ...    ...  ...        ...           ...\n2265  ID00426637202313170790466    129  ...          0             1\n2266  ID00426637202313170790466    130  ...          0             1\n2267  ID00426637202313170790466    131  ...          0             1\n2268  ID00426637202313170790466    132  ...          0             1\n2269  ID00426637202313170790466    133  ...          0             1\n\n[2270 rows x 20 columns]"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "combined_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATIENTS = train_df['Patient'].unique().tolist()\n",
    "# gave the gdcm error\n",
    "BAD_PATIENT_IDS = ['ID00011637202177653955184', 'ID00052637202186188008618']\n",
    "ALL_TRAIN_PATIENTS = np.array([pat for pat in TRAIN_PATIENTS if pat not in BAD_PATIENT_IDS])\n",
    "ALL_TEST_PATIENTS = test_df['Patient'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_averaged_slices(patient_id, folder_path, num_images):\n",
    "    # the preprocessed array with NUM_SLICES elements\n",
    "    # TODO: Handle the case when the NUM_SLICES > the actual total slices\n",
    "    # TODO: resize the image to 256 X 256?\n",
    "\n",
    "    full_path = os.path.join(folder_path, patient_id)\n",
    "    # list of all files in that path and sort them\n",
    "    all_files = os.listdir(full_path)\n",
    "    # sorted using the first number part of the file name\n",
    "    all_files.sort(key = lambda x: int(x.split('.')[0]))\n",
    "\n",
    "    # read all the dicom files for the patient into the slices list\n",
    "    slices = [pydicom.dcmread(os.path.join(full_path, s)) for s in all_files]\n",
    "    # sort the slices using their order (file number works too)\n",
    "    # slices.sort(key = lambda x: int(x.ImagePositionPatient[2]))\n",
    "\n",
    "    # final array containing averaged num_images images\n",
    "    out_array = []\n",
    "\n",
    "    # how many extra files while averaging all images into (num_images) images\n",
    "    remainder_array_size = len(slices)%num_images\n",
    "\n",
    "    # how many to average to get a single averaged image\n",
    "    avging_array_size = len(slices)//num_images\n",
    "\n",
    "    # get the first one with the remainder images\n",
    "    first_array = []\n",
    "    # select the first remainder + avg_arrray_size imgaes and average into one\n",
    "    for slice in slices[:remainder_array_size+avging_array_size]:\n",
    "        first_array.append(slice.pixel_array)\n",
    "    first_avged_array = np.average(first_array, axis=0)\n",
    "    first_resized = cv2.resize(first_avged_array / 2**11, (IMG_SIZE, IMG_SIZE))\n",
    "    out_array.append(first_resized)\n",
    "\n",
    "    # after the first one get the remaining ones into out_array rolling averaging (avging_array_size) at a time.\n",
    "    for i in range(remainder_array_size + avging_array_size, len(slices), avging_array_size):\n",
    "        temp_array = []\n",
    "        for slice in slices[i:i+avging_array_size]:\n",
    "            temp_array.append(slice.pixel_array)\n",
    "        avged_temp_array = np.average(temp_array, axis=0)\n",
    "        avged_resized = cv2.resize(avged_temp_array / 2**11, (IMG_SIZE, IMG_SIZE))\n",
    "        out_array.append(avged_resized)\n",
    "    \n",
    "    return np.array(out_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_from_id = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the train and test images in array_from_id\n",
    "for id in ALL_TRAIN_PATIENTS:\n",
    "    array_from_id[id] = get_averaged_slices(id, os.path.join(DATA_DIR, \"train\"), NUM_IMAGES)\n",
    "\n",
    "for id in ALL_TEST_PATIENTS:\n",
    "    array_from_id[id] = get_averaged_slices(id, os.path.join(DATA_DIR, \"test\"), NUM_IMAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PulmonaryDataset(Dataset):\n",
    "    def __init__(self, df, FV, test=False):\n",
    "        self.df = df\n",
    "        self.test = test\n",
    "        self.FV = FV\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'imgarray': torch.from_numpy(array_from_id[self.df.iloc[idx]['Patient']]).unsqueeze(0),\n",
    "            'features': torch.tensor(self.df[self.FV].iloc[idx].values),\n",
    "            'target': torch.tensor(self.df['FVC'].iloc[idx])\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PulmonaryModel(nn.Module):\n",
    "    def __init__(self, cnn_output_size=5, in_features=9, out_quantiles=3):\n",
    "        super(PulmonaryModel, self).__init__()\n",
    "\n",
    "        self.conv_layer1 = self._make_conv_layer(1, 8)\n",
    "        self.conv_layer2 = self._make_conv_layer(8, 32)\n",
    "        self.conv_layer3 = self._make_conv_layer(32, 64)\n",
    "        self.conv_layer4 = nn.Conv3d(64, 128, kernel_size=(1, 3, 3))\n",
    "        self.conv_layer5 = nn.Conv3d(128, 128, kernel_size=(1,3,3), padding=0)\n",
    "\n",
    "        self.fc1 = nn.Linear(86528, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, cnn_output_size)\n",
    "        \n",
    "        self.fc4 = nn.Linear(cnn_output_size + in_features, 100)\n",
    "        self.fc5 = nn.Linear(100, out_quantiles)\n",
    "\n",
    "    def _make_conv_layer(self, in_c, out_c):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv3d(in_c, out_c, kernel_size=(2,3,3), padding=0),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv3d(out_c, out_c, kernel_size=(2, 3, 3), padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool3d((2,2,2)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = self.conv_layer1(x)\n",
    "        x = self.conv_layer2(x)\n",
    "        x = self.conv_layer3(x)\n",
    "        x = self.conv_layer4(x)\n",
    "        x = self.conv_layer5(x)\n",
    "\n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # concatenate the in_features\n",
    "        x = torch.cat((x, y), dim=1)\n",
    "        x = self.fc4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_one, clip_two = torch.tensor(70, dtype=torch.float32).to(DEVICE), torch.tensor(1000, dtype=torch.float32).to(DEVICE)\n",
    "def score(y_true, y_pred):\n",
    "    sigma = y_pred[:, 2] - y_pred[:, 0]\n",
    "    fvc_pred = y_pred[:, 1]\n",
    "\n",
    "    sigma_clip = torch.max(sigma, clip_one)\n",
    "    delta = torch.abs(y_true - fvc_pred)\n",
    "    delta = torch.min(delta, clip_two)\n",
    "    sq2 = torch.sqrt(torch.tensor(2, dtype=torch.float32))\n",
    "    metric = (delta / sigma_clip) * sq2 + torch.log(sigma_clip * sq2)\n",
    "    return torch.mean(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_loss(preds, target, quantiles, _lambda):\n",
    "    assert not target.requires_grad\n",
    "    assert preds.size(0) == target.size(0)\n",
    "    losses = []\n",
    "    for i, q in enumerate(quantiles):\n",
    "        errors = target - preds[:, i]\n",
    "        losses.append(torch.max((q - 1) * errors, q * errors).unsqueeze(1))\n",
    "    loss = torch.mean(torch.sum(torch.cat(losses, dim=1), dim=1))\n",
    "    return loss\n",
    "    # return _lambda * loss + (1 - _lambda) * score(target, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_data_loader, optimizer, train_loss):\n",
    "    model.train()\n",
    "    for i, data in enumerate(train_data_loader):\n",
    "        images = data['imgarray']\n",
    "        features = data['features']\n",
    "        targets = data['target']\n",
    "\n",
    "        images = images.to(DEVICE).float()\n",
    "        features = features.to(DEVICE).float()\n",
    "        targets = targets.to(DEVICE).float()\n",
    "\n",
    "        model.zero_grad()\n",
    "        out = model(images, features)\n",
    "        loss = quantile_loss(out, targets, QUANTILES, 0.6)\n",
    "        train_loss.update(loss, features.size(0))\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_one_epoch(model, valid_data_loader, valid_loss, lr_scheduler):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(valid_data_loader):\n",
    "            images = data['imgarray']\n",
    "            features = data['features']\n",
    "            targets = data['target']\n",
    "\n",
    "            images = images.to(DEVICE).float()\n",
    "            features = features.to(DEVICE).float()\n",
    "            targets = targets.to(DEVICE).float()\n",
    "            \n",
    "            out = model(images, features)\n",
    "            loss = quantile_loss(out, targets, QUANTILES, 0.6)\n",
    "            valid_loss.update(loss, features.size(0))\n",
    "    \n",
    "    if lr_scheduler is not None:\n",
    "        lr_scheduler.step(valid_loss.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 0/1000, Loss 2661.447265625\nFold 0, Valid Loss 785.1724853515625 \n\nEpoch 1/1000, Loss 707.4561767578125\nFold 0, Valid Loss 778.5208740234375 \n\nEpoch 2/1000, Loss 689.634033203125\nFold 0, Valid Loss 770.9944458007812 \n\nEpoch 3/1000, Loss 693.519287109375\nFold 0, Valid Loss 766.688232421875 \n\nEpoch 4/1000, Loss 679.235107421875\nFold 0, Valid Loss 826.0916137695312 \n\nEpoch 5/1000, Loss 688.4309692382812\nFold 0, Valid Loss 759.41845703125 \n\nEpoch 6/1000, Loss 674.7553100585938\nFold 0, Valid Loss 786.4207763671875 \n\nEpoch 7/1000, Loss 683.1458740234375\nFold 0, Valid Loss 765.164794921875 \n\nEpoch 8/1000, Loss 673.03076171875\nFold 0, Valid Loss 761.2401123046875 \n\nEpoch 9/1000, Loss 673.6397094726562\nFold 0, Valid Loss 775.02685546875 \n\nEpoch 10/1000, Loss 668.6257934570312\nFold 0, Valid Loss 749.5225830078125 \n\nEpoch 11/1000, Loss 662.87841796875\nFold 0, Valid Loss 761.42041015625 \n\nEpoch 12/1000, Loss 661.4465942382812\nFold 0, Valid Loss 765.225341796875 \n\nEpoch 13/1000, Loss 656.9014892578125\nFold 0, Valid Loss 762.364501953125 \n\nEpoch 14/1000, Loss 652.40869140625\nFold 0, Valid Loss 753.8994140625 \n\nEpoch 15/1000, Loss 649.9332275390625\nFold 0, Valid Loss 753.550048828125 \n\nEpoch 16/1000, Loss 652.1334228515625\nFold 0, Valid Loss 752.8502807617188 \n\nEpoch 17/1000, Loss 653.0704956054688\nFold 0, Valid Loss 759.8392944335938 \n\nEpoch 18/1000, Loss 643.2017822265625\nFold 0, Valid Loss 756.5307006835938 \n\nEpoch 19/1000, Loss 633.8728637695312\nFold 0, Valid Loss 768.9577026367188 \n\nEpoch 20/1000, Loss 628.933837890625\nFold 0, Valid Loss 862.3297729492188 \n\nEpoch 21/1000, Loss 628.314453125\nFold 0, Valid Loss 764.953857421875 \n\nEpoch 22/1000, Loss 623.5042724609375\nFold 0, Valid Loss 819.1245727539062 \n\nEpoch 23/1000, Loss 621.5055541992188\nFold 0, Valid Loss 765.20361328125 \n\nEpoch 24/1000, Loss 618.26171875\nFold 0, Valid Loss 764.4979858398438 \n\nEpoch 25/1000, Loss 611.939697265625\nFold 0, Valid Loss 759.8019409179688 \n\nEpoch 26/1000, Loss 604.048828125\nFold 0, Valid Loss 766.7229614257812 \n\nEpoch 27/1000, Loss 600.8287353515625\nFold 0, Valid Loss 781.3272705078125 \n\nEpoch 28/1000, Loss 599.554443359375\nFold 0, Valid Loss 761.8735961914062 \n\nEpoch 29/1000, Loss 593.4981689453125\nFold 0, Valid Loss 833.7042236328125 \n\nEpoch 30/1000, Loss 594.40087890625\nFold 0, Valid Loss 759.2025146484375 \n\nEpoch 31/1000, Loss 585.4680786132812\nFold 0, Valid Loss 771.7011108398438 \n\nEpoch 32/1000, Loss 585.2243041992188\nFold 0, Valid Loss 774.8087158203125 \n\nEpoch 33/1000, Loss 594.3472900390625\nFold 0, Valid Loss 767.68017578125 \n\nEpoch 34/1000, Loss 578.4560546875\nFold 0, Valid Loss 762.202392578125 \n\nEpoch 35/1000, Loss 575.9373168945312\nFold 0, Valid Loss 754.101806640625 \n\nEpoch 36/1000, Loss 581.7999877929688\nFold 0, Valid Loss 762.4713134765625 \n\nEpoch 37/1000, Loss 569.47412109375\nFold 0, Valid Loss 749.3743896484375 \n\nEpoch 38/1000, Loss 577.7048950195312\nFold 0, Valid Loss 741.9804077148438 \n\nEpoch 39/1000, Loss 561.7005615234375\nFold 0, Valid Loss 740.0918579101562 \n\nEpoch 40/1000, Loss 565.3482666015625\nFold 0, Valid Loss 741.998291015625 \n\nEpoch 41/1000, Loss 563.7559814453125\nFold 0, Valid Loss 742.656982421875 \n\nEpoch 42/1000, Loss 562.7590942382812\nFold 0, Valid Loss 744.3565063476562 \n\nEpoch 43/1000, Loss 559.2705688476562\nFold 0, Valid Loss 750.0936889648438 \n\nEpoch 44/1000, Loss 554.5274658203125\nFold 0, Valid Loss 725.770263671875 \n\nEpoch 45/1000, Loss 551.2036743164062\nFold 0, Valid Loss 757.064453125 \n\nEpoch 46/1000, Loss 559.4091796875\nFold 0, Valid Loss 736.720947265625 \n\nEpoch 47/1000, Loss 539.7421875\nFold 0, Valid Loss 732.9554443359375 \n\nEpoch 48/1000, Loss 541.3738403320312\nFold 0, Valid Loss 724.6473999023438 \n\nEpoch 49/1000, Loss 535.9293823242188\nFold 0, Valid Loss 716.7281494140625 \n\nEpoch 50/1000, Loss 532.5032348632812\nFold 0, Valid Loss 763.0521850585938 \n\nEpoch 51/1000, Loss 538.0728759765625\nFold 0, Valid Loss 737.12109375 \n\nEpoch 52/1000, Loss 525.4825439453125\nFold 0, Valid Loss 735.6951904296875 \n\nEpoch 53/1000, Loss 533.3040771484375\nFold 0, Valid Loss 719.3775024414062 \n\nEpoch 54/1000, Loss 528.1113891601562\nFold 0, Valid Loss 750.5277099609375 \n\nEpoch 55/1000, Loss 518.6776733398438\nFold 0, Valid Loss 723.2872314453125 \n\nEpoch 56/1000, Loss 514.5042724609375\nFold 0, Valid Loss 718.89208984375 \n\nEpoch 57/1000, Loss 513.704833984375\nFold 0, Valid Loss 695.0704345703125 \n\nEpoch 58/1000, Loss 510.8631896972656\nFold 0, Valid Loss 706.5975952148438 \n\nEpoch 59/1000, Loss 498.520751953125\nFold 0, Valid Loss 693.1229858398438 \n\nEpoch 60/1000, Loss 505.9925842285156\nFold 0, Valid Loss 707.6033325195312 \n\nEpoch 61/1000, Loss 496.2528991699219\nFold 0, Valid Loss 699.713623046875 \n\nEpoch 62/1000, Loss 494.5706787109375\nFold 0, Valid Loss 726.7576904296875 \n\nEpoch 63/1000, Loss 503.4233093261719\nFold 0, Valid Loss 696.3391723632812 \n\nEpoch 64/1000, Loss 490.8728942871094\nFold 0, Valid Loss 697.7450561523438 \n\nEpoch 65/1000, Loss 487.3709411621094\nFold 0, Valid Loss 716.2283935546875 \n\nEpoch 66/1000, Loss 491.4690856933594\nFold 0, Valid Loss 709.5482177734375 \n\nEpoch 67/1000, Loss 479.2940368652344\nFold 0, Valid Loss 713.7929077148438 \n\nEpoch 68/1000, Loss 474.4400634765625\nFold 0, Valid Loss 720.3993530273438 \n\nEpoch 69/1000, Loss 474.1799011230469\nFold 0, Valid Loss 705.3229370117188 \n\nEpoch 70/1000, Loss 475.3565368652344\nFold 0, Valid Loss 695.17236328125 \n\nEpoch 71/1000, Loss 457.50640869140625\nFold 0, Valid Loss 749.0342407226562 \n\nEpoch 72/1000, Loss 459.61181640625\nFold 0, Valid Loss 695.4178466796875 \n\nEpoch 73/1000, Loss 454.6141052246094\nFold 0, Valid Loss 709.853271484375 \n\nEpoch 74/1000, Loss 452.5690002441406\nFold 0, Valid Loss 729.6478271484375 \n\nEpoch 75/1000, Loss 445.39990234375\nFold 0, Valid Loss 713.3087158203125 \n\nEpoch 76/1000, Loss 452.478271484375\nFold 0, Valid Loss 734.5506591796875 \n\nEpoch 77/1000, Loss 448.0906066894531\nFold 0, Valid Loss 708.297607421875 \n\nEpoch 78/1000, Loss 430.0171203613281\nFold 0, Valid Loss 722.4077758789062 \n\nEpoch 79/1000, Loss 436.68475341796875\nFold 0, Valid Loss 716.3242797851562 \n\nEpoch 80/1000, Loss 426.7862854003906\nFold 0, Valid Loss 724.2109375 \n\nEpoch 81/1000, Loss 413.74102783203125\nFold 0, Valid Loss 727.0568237304688 \n\nEpoch 82/1000, Loss 417.91265869140625\nFold 0, Valid Loss 726.2125854492188 \n\nEpoch 83/1000, Loss 412.67242431640625\nFold 0, Valid Loss 727.3889770507812 \n\nEpoch 84/1000, Loss 421.9592590332031\nFold 0, Valid Loss 723.7673950195312 \n\nEpoch 85/1000, Loss 407.1751403808594\nFold 0, Valid Loss 720.521484375 \n\nEpoch 86/1000, Loss 402.2724914550781\nFold 0, Valid Loss 752.373046875 \n\nEpoch 87/1000, Loss 396.4018249511719\nFold 0, Valid Loss 730.6517944335938 \n\nEpoch 88/1000, Loss 403.75640869140625\nFold 0, Valid Loss 730.3729858398438 \n\nEpoch 89/1000, Loss 405.53302001953125\nFold 0, Valid Loss 728.7333984375 \n\nEpoch 90/1000, Loss 398.8447570800781\nFold 0, Valid Loss 723.6959838867188 \n\nEpoch 91/1000, Loss 384.6820068359375\nFold 0, Valid Loss 737.8491821289062 \n\nEpoch 92/1000, Loss 381.24761962890625\nFold 0, Valid Loss 750.4511108398438 \n\nEpoch 93/1000, Loss 375.835693359375\nFold 0, Valid Loss 724.6873779296875 \n\nEpoch 94/1000, Loss 378.0116271972656\nFold 0, Valid Loss 746.5770874023438 \n\nEpoch 95/1000, Loss 373.8480529785156\nFold 0, Valid Loss 724.4434204101562 \n\nEpoch 96/1000, Loss 373.5914306640625\nFold 0, Valid Loss 743.3580322265625 \n\nEpoch 97/1000, Loss 373.1789245605469\nFold 0, Valid Loss 724.5906982421875 \n\nEpoch 98/1000, Loss 374.17108154296875\nFold 0, Valid Loss 716.9005737304688 \n\nEpoch 99/1000, Loss 365.8587341308594\nFold 0, Valid Loss 713.7752075195312 \n\nEpoch 100/1000, Loss 364.5851745605469\nFold 0, Valid Loss 714.923828125 \n\nEpoch 101/1000, Loss 360.87701416015625\nFold 0, Valid Loss 722.997314453125 \n\nEpoch 102/1000, Loss 365.8507385253906\nFold 0, Valid Loss 721.5770263671875 \n\nEpoch 103/1000, Loss 360.5530090332031\nFold 0, Valid Loss 711.7161254882812 \n\nEpoch 104/1000, Loss 349.7286682128906\nFold 0, Valid Loss 723.59130859375 \n\nEpoch 105/1000, Loss 360.63299560546875\nFold 0, Valid Loss 706.2979736328125 \n\nEpoch 106/1000, Loss 348.97161865234375\nFold 0, Valid Loss 720.500244140625 \n\nEpoch 107/1000, Loss 347.4149475097656\nFold 0, Valid Loss 705.8236083984375 \n\nEpoch 108/1000, Loss 353.49017333984375\nFold 0, Valid Loss 706.8667602539062 \n\nEpoch 109/1000, Loss 345.98101806640625\nFold 0, Valid Loss 699.6546630859375 \n\nEpoch   111: reducing learning rate of group 0 to 2.8000e-05.\nEpoch 110/1000, Loss 341.3191223144531\nFold 0, Valid Loss 710.3826293945312 \n\nEpoch 111/1000, Loss 341.37554931640625\nFold 0, Valid Loss 699.1546020507812 \n\nEpoch 112/1000, Loss 338.24658203125\nFold 0, Valid Loss 703.55419921875 \n\nEpoch 113/1000, Loss 334.3247985839844\nFold 0, Valid Loss 712.8279418945312 \n\nEpoch 114/1000, Loss 337.81536865234375\nFold 0, Valid Loss 693.2536010742188 \n\nEpoch 115/1000, Loss 332.6668701171875\nFold 0, Valid Loss 696.7596435546875 \n\nEpoch 116/1000, Loss 326.8511657714844\nFold 0, Valid Loss 693.7594604492188 \n\nEpoch 117/1000, Loss 331.074462890625\nFold 0, Valid Loss 698.6243286132812 \n\nEpoch 118/1000, Loss 334.9085693359375\nFold 0, Valid Loss 691.7904052734375 \n\nEpoch 119/1000, Loss 330.8298034667969\nFold 0, Valid Loss 692.9913940429688 \n\nEpoch 120/1000, Loss 326.60345458984375\nFold 0, Valid Loss 688.1934814453125 \n\nEpoch 121/1000, Loss 324.89031982421875\nFold 0, Valid Loss 684.8214111328125 \n\nEpoch 122/1000, Loss 323.99481201171875\nFold 0, Valid Loss 693.2759399414062 \n\nEpoch 123/1000, Loss 320.4648742675781\nFold 0, Valid Loss 689.422119140625 \n\nEpoch 124/1000, Loss 321.776611328125\nFold 0, Valid Loss 681.925048828125 \n\nEpoch 125/1000, Loss 320.1070556640625\nFold 0, Valid Loss 688.4204711914062 \n\nEpoch 126/1000, Loss 318.4234619140625\nFold 0, Valid Loss 685.6060791015625 \n\nEpoch 127/1000, Loss 320.0934143066406\nFold 0, Valid Loss 682.9862060546875 \n\nEpoch 128/1000, Loss 314.784423828125\nFold 0, Valid Loss 689.3877563476562 \n\nEpoch 129/1000, Loss 325.64129638671875\nFold 0, Valid Loss 679.1573486328125 \n\nEpoch 130/1000, Loss 315.035400390625\nFold 0, Valid Loss 686.0587158203125 \n\nEpoch 131/1000, Loss 317.39892578125\nFold 0, Valid Loss 680.1739501953125 \n\nEpoch 132/1000, Loss 320.3526306152344\nFold 0, Valid Loss 680.5516967773438 \n\nEpoch 133/1000, Loss 314.871826171875\nFold 0, Valid Loss 688.8392944335938 \n\nEpoch 134/1000, Loss 309.8565368652344\nFold 0, Valid Loss 685.8944091796875 \n\nEpoch 135/1000, Loss 317.1385498046875\nFold 0, Valid Loss 678.2435913085938 \n\nEpoch 136/1000, Loss 316.9778747558594\nFold 0, Valid Loss 687.328369140625 \n\nEpoch 137/1000, Loss 310.8815002441406\nFold 0, Valid Loss 685.5230102539062 \n\nEpoch 138/1000, Loss 304.7195739746094\nFold 0, Valid Loss 689.9061889648438 \n\nEpoch 139/1000, Loss 307.6499938964844\nFold 0, Valid Loss 701.2816162109375 \n\nEpoch 140/1000, Loss 304.986572265625\nFold 0, Valid Loss 681.5778198242188 \n\nEpoch 141/1000, Loss 295.72265625\nFold 0, Valid Loss 686.3359985351562 \n\nEpoch 142/1000, Loss 300.4361877441406\nFold 0, Valid Loss 684.24267578125 \n\nEpoch 143/1000, Loss 308.8956604003906\nFold 0, Valid Loss 698.98291015625 \n\nEpoch 144/1000, Loss 303.624755859375\nFold 0, Valid Loss 683.0902709960938 \n\nEpoch 145/1000, Loss 295.1295471191406\nFold 0, Valid Loss 693.4230346679688 \n\nEpoch 146/1000, Loss 296.7059020996094\nFold 0, Valid Loss 700.5167236328125 \n\nEpoch 147/1000, Loss 292.64715576171875\nFold 0, Valid Loss 690.4048461914062 \n\nEpoch 148/1000, Loss 294.3061218261719\nFold 0, Valid Loss 692.3214721679688 \n\nEpoch 149/1000, Loss 292.75457763671875\nFold 0, Valid Loss 693.383544921875 \n\nEpoch 150/1000, Loss 287.57257080078125\nFold 0, Valid Loss 686.3809204101562 \n\nEpoch 151/1000, Loss 291.96112060546875\nFold 0, Valid Loss 689.77001953125 \n\nEpoch 152/1000, Loss 297.0214538574219\nFold 0, Valid Loss 694.4025268554688 \n\nEpoch 153/1000, Loss 294.5629577636719\nFold 0, Valid Loss 686.1351318359375 \n\nEpoch 154/1000, Loss 292.1637878417969\nFold 0, Valid Loss 689.1373291015625 \n\nEpoch 155/1000, Loss 284.23040771484375\nFold 0, Valid Loss 698.3690795898438 \n\nEpoch 156/1000, Loss 285.16107177734375\nFold 0, Valid Loss 696.7094116210938 \n\nEpoch 157/1000, Loss 285.5689392089844\nFold 0, Valid Loss 709.7875366210938 \n\nEpoch 158/1000, Loss 284.69140625\nFold 0, Valid Loss 705.473388671875 \n\nEpoch 159/1000, Loss 292.09844970703125\nFold 0, Valid Loss 697.1122436523438 \n\nEpoch 160/1000, Loss 279.3752746582031\nFold 0, Valid Loss 691.7884521484375 \n\nEpoch 161/1000, Loss 287.5591735839844\nFold 0, Valid Loss 700.2921752929688 \n\nEpoch 162/1000, Loss 279.141845703125\nFold 0, Valid Loss 711.61181640625 \n\nEpoch 163/1000, Loss 286.81689453125\nFold 0, Valid Loss 693.31298828125 \n\nEpoch 164/1000, Loss 281.2572326660156\nFold 0, Valid Loss 707.9318237304688 \n\nEpoch 165/1000, Loss 278.4712219238281\nFold 0, Valid Loss 694.8311157226562 \n\nEpoch 166/1000, Loss 276.906494140625\nFold 0, Valid Loss 710.3272094726562 \n\nEpoch 167/1000, Loss 276.1639404296875\nFold 0, Valid Loss 707.595458984375 \n\nEpoch 168/1000, Loss 272.7750549316406\nFold 0, Valid Loss 715.6096801757812 \n\nEpoch 169/1000, Loss 274.20751953125\nFold 0, Valid Loss 708.7321166992188 \n\nEpoch 170/1000, Loss 273.7617492675781\nFold 0, Valid Loss 722.0714721679688 \n\nEpoch 171/1000, Loss 271.41009521484375\nFold 0, Valid Loss 726.7379150390625 \n\nEpoch 172/1000, Loss 269.8396911621094\nFold 0, Valid Loss 719.4863891601562 \n\nEpoch 173/1000, Loss 269.3524475097656\nFold 0, Valid Loss 724.9662475585938 \n\nEpoch 174/1000, Loss 265.01080322265625\nFold 0, Valid Loss 735.6603393554688 \n\nEpoch 175/1000, Loss 273.1900634765625\nFold 0, Valid Loss 719.2274169921875 \n\nEpoch 176/1000, Loss 263.0780029296875\nFold 0, Valid Loss 717.8975830078125 \n\nEpoch 177/1000, Loss 262.0728454589844\nFold 0, Valid Loss 731.8572387695312 \n\nEpoch 178/1000, Loss 264.29937744140625\nFold 0, Valid Loss 729.2200927734375 \n\nEpoch 179/1000, Loss 263.6863098144531\nFold 0, Valid Loss 739.306640625 \n\nEpoch 180/1000, Loss 258.32391357421875\nFold 0, Valid Loss 737.3480224609375 \n\nEpoch 181/1000, Loss 252.91506958007812\nFold 0, Valid Loss 747.10693359375 \n\nEpoch 182/1000, Loss 249.69400024414062\nFold 0, Valid Loss 757.3511352539062 \n\nEpoch 183/1000, Loss 253.58189392089844\nFold 0, Valid Loss 754.4317016601562 \n\nEpoch 184/1000, Loss 255.21441650390625\nFold 0, Valid Loss 764.7083740234375 \n\nEpoch 185/1000, Loss 250.5322265625\nFold 0, Valid Loss 775.0617065429688 \n\nEpoch   187: reducing learning rate of group 0 to 1.9600e-05.\nEpoch 186/1000, Loss 263.30657958984375\nFold 0, Valid Loss 764.6494140625 \n\nEpoch 187/1000, Loss 253.0503387451172\nFold 0, Valid Loss 779.2095947265625 \n\nEpoch 188/1000, Loss 242.16127014160156\nFold 0, Valid Loss 777.55517578125 \n\nEpoch 189/1000, Loss 244.23631286621094\nFold 0, Valid Loss 769.3847045898438 \n\nEpoch 190/1000, Loss 240.99122619628906\nFold 0, Valid Loss 785.6049194335938 \n\nEpoch 191/1000, Loss 234.2304229736328\nFold 0, Valid Loss 784.1802368164062 \n\nEpoch 192/1000, Loss 236.5839385986328\nFold 0, Valid Loss 786.2938232421875 \n\nEpoch 193/1000, Loss 233.1645965576172\nFold 0, Valid Loss 793.454345703125 \n\nEpoch 194/1000, Loss 231.9356689453125\nFold 0, Valid Loss 803.2650756835938 \n\nEpoch 195/1000, Loss 231.03457641601562\nFold 0, Valid Loss 797.6708984375 \n\nEpoch 196/1000, Loss 229.966796875\nFold 0, Valid Loss 789.5711669921875 \n\nEpoch 197/1000, Loss 231.45054626464844\nFold 0, Valid Loss 797.6869506835938 \n\nEpoch 198/1000, Loss 229.76715087890625\nFold 0, Valid Loss 801.4627685546875 \n\nEpoch 199/1000, Loss 225.02044677734375\nFold 0, Valid Loss 818.3001708984375 \n\nEpoch 200/1000, Loss 229.38462829589844\nFold 0, Valid Loss 808.1891479492188 \n\nEpoch 201/1000, Loss 224.4607391357422\nFold 0, Valid Loss 813.1319580078125 \n\nEpoch 202/1000, Loss 225.73895263671875\nFold 0, Valid Loss 809.5323486328125 \n\nEpoch 203/1000, Loss 224.03387451171875\nFold 0, Valid Loss 805.40673828125 \n\nEpoch 204/1000, Loss 218.9734649658203\nFold 0, Valid Loss 832.3642578125 \n\nEpoch 205/1000, Loss 217.47047424316406\nFold 0, Valid Loss 818.4698486328125 \n\nEpoch 206/1000, Loss 223.12901306152344\nFold 0, Valid Loss 825.693115234375 \n\nEpoch 207/1000, Loss 214.8111114501953\nFold 0, Valid Loss 822.943359375 \n\nEpoch 208/1000, Loss 222.1405487060547\nFold 0, Valid Loss 826.5732421875 \n\nEpoch 209/1000, Loss 217.26148986816406\nFold 0, Valid Loss 837.7760009765625 \n\nEpoch 210/1000, Loss 211.95916748046875\nFold 0, Valid Loss 846.0181274414062 \n\nEpoch 211/1000, Loss 212.8764190673828\nFold 0, Valid Loss 846.5191650390625 \n\nEpoch 212/1000, Loss 210.07949829101562\nFold 0, Valid Loss 842.3187255859375 \n\nEpoch 213/1000, Loss 213.74136352539062\nFold 0, Valid Loss 838.98095703125 \n\nEpoch 214/1000, Loss 207.45787048339844\nFold 0, Valid Loss 844.395263671875 \n\nEpoch 215/1000, Loss 206.1303253173828\nFold 0, Valid Loss 852.5526733398438 \n\nEpoch 216/1000, Loss 206.71542358398438\nFold 0, Valid Loss 867.4210815429688 \n\nEpoch 217/1000, Loss 201.9027557373047\nFold 0, Valid Loss 853.8292236328125 \n\nEpoch 218/1000, Loss 207.83848571777344\nFold 0, Valid Loss 870.336181640625 \n\nEpoch 219/1000, Loss 199.602783203125\nFold 0, Valid Loss 864.4049072265625 \n\nEpoch 220/1000, Loss 201.8748016357422\nFold 0, Valid Loss 875.1083984375 \n\nEpoch 221/1000, Loss 204.16061401367188\nFold 0, Valid Loss 879.1907958984375 \n\nEpoch 222/1000, Loss 206.9311065673828\nFold 0, Valid Loss 863.7007446289062 \n\nEpoch 223/1000, Loss 196.162353515625\nFold 0, Valid Loss 884.69775390625 \n\nEpoch 224/1000, Loss 193.2028350830078\nFold 0, Valid Loss 886.1715698242188 \n\nEpoch 225/1000, Loss 197.4221954345703\nFold 0, Valid Loss 866.3589477539062 \n\nEpoch 226/1000, Loss 194.60137939453125\nFold 0, Valid Loss 880.6949462890625 \n\nEpoch 227/1000, Loss 190.8397979736328\nFold 0, Valid Loss 875.2890014648438 \n\nEpoch 228/1000, Loss 189.47032165527344\nFold 0, Valid Loss 880.6939086914062 \n\nEpoch 229/1000, Loss 189.27415466308594\nFold 0, Valid Loss 909.0076904296875 \n\nEpoch 230/1000, Loss 191.216552734375\nFold 0, Valid Loss 881.366455078125 \n\nEpoch 231/1000, Loss 195.300537109375\nFold 0, Valid Loss 877.2593383789062 \n\nEpoch 232/1000, Loss 182.66049194335938\nFold 0, Valid Loss 916.4663696289062 \n\nEpoch 233/1000, Loss 192.2276153564453\nFold 0, Valid Loss 890.048583984375 \n\nEpoch 234/1000, Loss 188.3445587158203\nFold 0, Valid Loss 900.1697387695312 \n\nEpoch 235/1000, Loss 184.36639404296875\nFold 0, Valid Loss 903.0294799804688 \n\nEpoch 236/1000, Loss 186.13214111328125\nFold 0, Valid Loss 908.1573486328125 \n\nEpoch   238: reducing learning rate of group 0 to 1.3720e-05.\nEpoch 237/1000, Loss 181.6051788330078\nFold 0, Valid Loss 901.1490478515625 \n\nEpoch 238/1000, Loss 176.18649291992188\nFold 0, Valid Loss 930.7733154296875 \n\nEpoch 239/1000, Loss 174.1819610595703\nFold 0, Valid Loss 923.3740234375 \n\nEpoch 240/1000, Loss 169.70791625976562\nFold 0, Valid Loss 917.1271362304688 \n\nEpoch 241/1000, Loss 170.56129455566406\nFold 0, Valid Loss 922.5244750976562 \n\nEpoch 242/1000, Loss 171.62586975097656\nFold 0, Valid Loss 932.4041748046875 \n\nEpoch 243/1000, Loss 173.9755859375\nFold 0, Valid Loss 933.907470703125 \n\nEpoch 244/1000, Loss 168.98770141601562\nFold 0, Valid Loss 923.4467163085938 \n\nEpoch 245/1000, Loss 173.2394256591797\nFold 0, Valid Loss 919.956298828125 \n\nEpoch 246/1000, Loss 167.16636657714844\nFold 0, Valid Loss 933.8397827148438 \n\nEpoch 247/1000, Loss 166.60360717773438\nFold 0, Valid Loss 937.3982543945312 \n\nEpoch 248/1000, Loss 167.02626037597656\nFold 0, Valid Loss 936.6216430664062 \n\nEpoch 249/1000, Loss 165.7634735107422\nFold 0, Valid Loss 945.6475219726562 \n\nEpoch 250/1000, Loss 165.40452575683594\nFold 0, Valid Loss 945.5586547851562 \n\nEpoch 251/1000, Loss 165.162353515625\nFold 0, Valid Loss 951.7565307617188 \n\nEpoch 252/1000, Loss 161.00465393066406\nFold 0, Valid Loss 947.208251953125 \n\nEpoch 253/1000, Loss 162.12303161621094\nFold 0, Valid Loss 954.0150146484375 \n\nEpoch 254/1000, Loss 164.66555786132812\nFold 0, Valid Loss 943.9829711914062 \n\nEpoch 255/1000, Loss 159.3536834716797\nFold 0, Valid Loss 950.4695434570312 \n\nEpoch 256/1000, Loss 162.38099670410156\nFold 0, Valid Loss 922.2430419921875 \n\nEpoch 257/1000, Loss 161.27748107910156\nFold 0, Valid Loss 947.8809204101562 \n\nEpoch 258/1000, Loss 160.7827911376953\nFold 0, Valid Loss 949.4757080078125 \n\nEpoch 259/1000, Loss 158.23841857910156\nFold 0, Valid Loss 964.7142333984375 \n\nEpoch 260/1000, Loss 159.5265655517578\nFold 0, Valid Loss 955.0266723632812 \n\nEpoch 261/1000, Loss 160.12896728515625\nFold 0, Valid Loss 980.1286010742188 \n\nEpoch 262/1000, Loss 158.5583038330078\nFold 0, Valid Loss 970.5231323242188 \n\nEpoch 263/1000, Loss 153.61012268066406\nFold 0, Valid Loss 954.9996948242188 \n\nEpoch 264/1000, Loss 155.54925537109375\nFold 0, Valid Loss 966.3781127929688 \n\nEpoch 265/1000, Loss 152.8681182861328\nFold 0, Valid Loss 971.4901123046875 \n\nEpoch 266/1000, Loss 152.60440063476562\nFold 0, Valid Loss 962.2468872070312 \n\nEpoch 267/1000, Loss 157.42919921875\nFold 0, Valid Loss 957.0672607421875 \n\nEpoch 268/1000, Loss 151.7831268310547\nFold 0, Valid Loss 985.5604248046875 \n\nEpoch 269/1000, Loss 151.2261962890625\nFold 0, Valid Loss 972.635986328125 \n\nEpoch 270/1000, Loss 151.1609344482422\nFold 0, Valid Loss 970.2405395507812 \n\nEpoch 271/1000, Loss 148.94871520996094\nFold 0, Valid Loss 976.0188598632812 \n\nEpoch 272/1000, Loss 150.1499786376953\nFold 0, Valid Loss 987.9551391601562 \n\nEpoch 273/1000, Loss 150.27781677246094\nFold 0, Valid Loss 971.2064819335938 \n\nEpoch 274/1000, Loss 148.06527709960938\nFold 0, Valid Loss 987.4336547851562 \n\nEpoch 275/1000, Loss 149.28968811035156\nFold 0, Valid Loss 994.3106079101562 \n\nEpoch 276/1000, Loss 146.57081604003906\nFold 0, Valid Loss 970.3624267578125 \n\nEpoch 277/1000, Loss 148.67926025390625\nFold 0, Valid Loss 997.1773071289062 \n\nEpoch 278/1000, Loss 149.9326629638672\nFold 0, Valid Loss 972.4735107421875 \n\nEpoch 279/1000, Loss 148.851318359375\nFold 0, Valid Loss 971.4261474609375 \n\nEpoch 280/1000, Loss 148.25872802734375\nFold 0, Valid Loss 977.44580078125 \n\nEpoch 281/1000, Loss 146.44985961914062\nFold 0, Valid Loss 996.233154296875 \n\nEpoch 282/1000, Loss 147.06747436523438\nFold 0, Valid Loss 1003.09716796875 \n\nEpoch 283/1000, Loss 144.37857055664062\nFold 0, Valid Loss 969.66455078125 \n\nEpoch 284/1000, Loss 146.7171173095703\nFold 0, Valid Loss 995.236083984375 \n\nEpoch 285/1000, Loss 145.2364044189453\nFold 0, Valid Loss 1002.6055908203125 \n\nEpoch 286/1000, Loss 147.05319213867188\nFold 0, Valid Loss 977.2654418945312 \n\nEpoch 287/1000, Loss 144.40890502929688\nFold 0, Valid Loss 982.64013671875 \n\nEpoch   289: reducing learning rate of group 0 to 9.6040e-06.\nEpoch 288/1000, Loss 143.94683837890625\nFold 0, Valid Loss 992.7369995117188 \n\nEpoch 289/1000, Loss 140.5531463623047\nFold 0, Valid Loss 1007.177490234375 \n\nEpoch 290/1000, Loss 138.10833740234375\nFold 0, Valid Loss 994.388671875 \n\nEpoch 291/1000, Loss 137.80787658691406\nFold 0, Valid Loss 1000.4019775390625 \n\nEpoch 292/1000, Loss 139.70294189453125\nFold 0, Valid Loss 996.3934326171875 \n\nEpoch 293/1000, Loss 136.0189666748047\nFold 0, Valid Loss 995.1221923828125 \n\nEpoch 294/1000, Loss 135.1575164794922\nFold 0, Valid Loss 983.62353515625 \n\nEpoch 295/1000, Loss 137.17710876464844\nFold 0, Valid Loss 1003.7245483398438 \n\nEpoch 296/1000, Loss 137.17807006835938\nFold 0, Valid Loss 1005.1271362304688 \n\nEpoch 297/1000, Loss 136.81936645507812\nFold 0, Valid Loss 1002.0205078125 \n\nEpoch 298/1000, Loss 135.14599609375\nFold 0, Valid Loss 999.2156372070312 \n\nEpoch 299/1000, Loss 134.42477416992188\nFold 0, Valid Loss 994.2178955078125 \n\nEpoch 300/1000, Loss 134.25787353515625\nFold 0, Valid Loss 997.86181640625 \n\nEpoch 301/1000, Loss 132.5541534423828\nFold 0, Valid Loss 995.065673828125 \n\nEpoch 302/1000, Loss 133.14405822753906\nFold 0, Valid Loss 1000.540283203125 \n\nEpoch 303/1000, Loss 135.24301147460938\nFold 0, Valid Loss 1017.0217895507812 \n\nEpoch 304/1000, Loss 135.09246826171875\nFold 0, Valid Loss 1005.287353515625 \n\nEpoch 305/1000, Loss 133.97467041015625\nFold 0, Valid Loss 1009.8525390625 \n\nEpoch 306/1000, Loss 132.38177490234375\nFold 0, Valid Loss 1001.53515625 \n\nEpoch 307/1000, Loss 131.3367919921875\nFold 0, Valid Loss 1003.6515502929688 \n\nEpoch 308/1000, Loss 131.36917114257812\nFold 0, Valid Loss 1002.921875 \n\nEpoch 309/1000, Loss 137.13571166992188\nFold 0, Valid Loss 1003.7149047851562 \n\nEpoch 310/1000, Loss 133.77725219726562\nFold 0, Valid Loss 1015.135986328125 \n\nEpoch 311/1000, Loss 132.07379150390625\nFold 0, Valid Loss 1019.0951538085938 \n\nEpoch 312/1000, Loss 131.8498077392578\nFold 0, Valid Loss 1015.0868530273438 \n\nEpoch 313/1000, Loss 130.7313995361328\nFold 0, Valid Loss 1013.933837890625 \n\nEpoch 314/1000, Loss 128.49778747558594\nFold 0, Valid Loss 1002.3594970703125 \n\nEpoch 315/1000, Loss 132.40638732910156\nFold 0, Valid Loss 1005.1235961914062 \n\nEpoch 316/1000, Loss 130.3599090576172\nFold 0, Valid Loss 1009.3008422851562 \n\nEpoch 317/1000, Loss 132.5270233154297\nFold 0, Valid Loss 1008.6044311523438 \n\nEpoch 318/1000, Loss 133.41976928710938\nFold 0, Valid Loss 1016.2469482421875 \n\nEpoch 319/1000, Loss 129.31861877441406\nFold 0, Valid Loss 988.9437866210938 \n\nEpoch 320/1000, Loss 128.87225341796875\nFold 0, Valid Loss 1028.799560546875 \n\nEpoch 321/1000, Loss 129.1678924560547\nFold 0, Valid Loss 1014.7076416015625 \n\nEpoch 322/1000, Loss 128.59402465820312\nFold 0, Valid Loss 1023.3557739257812 \n\nEpoch 323/1000, Loss 126.56884765625\nFold 0, Valid Loss 1010.2876586914062 \n\nEpoch 324/1000, Loss 127.7847671508789\nFold 0, Valid Loss 1022.8778686523438 \n\nEpoch 325/1000, Loss 127.25382995605469\nFold 0, Valid Loss 1026.9630126953125 \n\nEpoch 326/1000, Loss 126.19834899902344\nFold 0, Valid Loss 1026.5059814453125 \n\nEpoch 327/1000, Loss 126.08246612548828\nFold 0, Valid Loss 1015.9173583984375 \n\nEpoch 328/1000, Loss 127.60392761230469\nFold 0, Valid Loss 1029.01318359375 \n\nEpoch 329/1000, Loss 128.4720916748047\nFold 0, Valid Loss 1006.8273315429688 \n\nEpoch 330/1000, Loss 126.88053131103516\nFold 0, Valid Loss 1019.4948120117188 \n\nEpoch 331/1000, Loss 127.42344665527344\nFold 0, Valid Loss 1019.3795166015625 \n\nEpoch 332/1000, Loss 126.88810729980469\nFold 0, Valid Loss 1034.230712890625 \n\nEpoch 333/1000, Loss 125.5283203125\nFold 0, Valid Loss 1021.0255126953125 \n\nEpoch 334/1000, Loss 127.63909912109375\nFold 0, Valid Loss 1032.88232421875 \n\nEpoch 335/1000, Loss 126.6706314086914\nFold 0, Valid Loss 1017.958740234375 \n\nEpoch 336/1000, Loss 128.29270935058594\nFold 0, Valid Loss 1021.5486450195312 \n\nEpoch 337/1000, Loss 123.75362396240234\nFold 0, Valid Loss 1018.1505126953125 \n\nEpoch 338/1000, Loss 126.45134735107422\nFold 0, Valid Loss 1028.2744140625 \n\nEpoch   340: reducing learning rate of group 0 to 6.7228e-06.\nEpoch 339/1000, Loss 123.73328399658203\nFold 0, Valid Loss 1013.26806640625 \n\nEpoch 340/1000, Loss 122.33013153076172\nFold 0, Valid Loss 1019.7050170898438 \n\nEpoch 341/1000, Loss 120.12933349609375\nFold 0, Valid Loss 1017.4863891601562 \n\nEpoch 342/1000, Loss 122.0931396484375\nFold 0, Valid Loss 1025.2652587890625 \n\nEpoch 343/1000, Loss 121.09121704101562\nFold 0, Valid Loss 1033.0543212890625 \n\nEpoch 344/1000, Loss 120.61334991455078\nFold 0, Valid Loss 1034.79150390625 \n\nEpoch 345/1000, Loss 121.92166137695312\nFold 0, Valid Loss 1022.8204956054688 \n\nEpoch 346/1000, Loss 120.521728515625\nFold 0, Valid Loss 1026.942138671875 \n\nEpoch 347/1000, Loss 119.79276275634766\nFold 0, Valid Loss 1025.37744140625 \n\nEpoch 348/1000, Loss 124.44251251220703\nFold 0, Valid Loss 1020.545654296875 \n\nEpoch 349/1000, Loss 121.54741668701172\nFold 0, Valid Loss 1023.2423095703125 \n\nEpoch 350/1000, Loss 119.35196685791016\nFold 0, Valid Loss 1009.9381103515625 \n\nEpoch 351/1000, Loss 120.0739517211914\nFold 0, Valid Loss 1012.1697998046875 \n\nEpoch 352/1000, Loss 119.87704467773438\nFold 0, Valid Loss 1021.8973999023438 \n\nEpoch 353/1000, Loss 120.84158325195312\nFold 0, Valid Loss 1021.9453735351562 \n\nEpoch 354/1000, Loss 121.0776596069336\nFold 0, Valid Loss 1023.8572387695312 \n\nEpoch 355/1000, Loss 120.05928802490234\nFold 0, Valid Loss 1022.8316040039062 \n\nEpoch 356/1000, Loss 119.23625946044922\nFold 0, Valid Loss 1034.3037109375 \n\nEpoch 357/1000, Loss 118.42247009277344\nFold 0, Valid Loss 1030.8133544921875 \n\nEpoch 358/1000, Loss 118.7542724609375\nFold 0, Valid Loss 1023.9305419921875 \n\nEpoch 359/1000, Loss 121.4209976196289\nFold 0, Valid Loss 1024.7923583984375 \n\nEpoch 360/1000, Loss 121.33308410644531\nFold 0, Valid Loss 1025.4703369140625 \n\nEpoch 361/1000, Loss 118.03163146972656\nFold 0, Valid Loss 1019.3914794921875 \n\nEpoch 362/1000, Loss 118.76345825195312\nFold 0, Valid Loss 1028.90966796875 \n\nEpoch 363/1000, Loss 118.34880828857422\nFold 0, Valid Loss 1023.2081298828125 \n\nEpoch 364/1000, Loss 118.18436431884766\nFold 0, Valid Loss 1030.1842041015625 \n\nEpoch 365/1000, Loss 120.72477722167969\nFold 0, Valid Loss 1022.7273559570312 \n\nEpoch 366/1000, Loss 119.32804107666016\nFold 0, Valid Loss 1019.69287109375 \n\nEpoch 367/1000, Loss 118.82064819335938\nFold 0, Valid Loss 1027.13916015625 \n\nEpoch 368/1000, Loss 119.6781005859375\nFold 0, Valid Loss 1027.8756103515625 \n\nEpoch 369/1000, Loss 120.42347717285156\nFold 0, Valid Loss 1024.272216796875 \n\nEpoch 370/1000, Loss 117.41123962402344\nFold 0, Valid Loss 1033.943115234375 \n\nEpoch 371/1000, Loss 119.0627212524414\nFold 0, Valid Loss 1021.989501953125 \n\nEpoch 372/1000, Loss 117.53715515136719\nFold 0, Valid Loss 1027.6298828125 \n\nEpoch 373/1000, Loss 121.44180297851562\nFold 0, Valid Loss 1022.3800659179688 \n\nEpoch 374/1000, Loss 117.81462860107422\nFold 0, Valid Loss 1030.8358154296875 \n\nEpoch 375/1000, Loss 117.83807373046875\nFold 0, Valid Loss 1031.62109375 \n\nEpoch 376/1000, Loss 121.26428985595703\nFold 0, Valid Loss 1022.4483032226562 \n\nEpoch 377/1000, Loss 117.37236022949219\nFold 0, Valid Loss 1025.662353515625 \n\nEpoch 378/1000, Loss 117.71668243408203\nFold 0, Valid Loss 1033.7081298828125 \n\nEpoch 379/1000, Loss 118.0149917602539\nFold 0, Valid Loss 1038.140380859375 \n\nEpoch 380/1000, Loss 117.70828247070312\nFold 0, Valid Loss 1041.5191650390625 \n\nEpoch 381/1000, Loss 117.5352554321289\nFold 0, Valid Loss 1028.443359375 \n\nEpoch 382/1000, Loss 115.96514892578125\nFold 0, Valid Loss 1029.18603515625 \n\nEpoch 383/1000, Loss 117.47348022460938\nFold 0, Valid Loss 1021.3110961914062 \n\nEpoch 384/1000, Loss 117.25210571289062\nFold 0, Valid Loss 1039.42578125 \n\nEpoch 385/1000, Loss 117.95821380615234\nFold 0, Valid Loss 1022.308349609375 \n\nEpoch 386/1000, Loss 118.18414306640625\nFold 0, Valid Loss 1030.16064453125 \n\nEpoch 387/1000, Loss 116.00630187988281\nFold 0, Valid Loss 1037.70263671875 \n\nEpoch 388/1000, Loss 116.97379302978516\nFold 0, Valid Loss 1022.5401611328125 \n\nEpoch 389/1000, Loss 116.59872436523438\nFold 0, Valid Loss 1033.9700927734375 \n\nEpoch   391: reducing learning rate of group 0 to 4.7060e-06.\nEpoch 390/1000, Loss 115.3936996459961\nFold 0, Valid Loss 1027.4306640625 \n\nEpoch 391/1000, Loss 113.08830261230469\nFold 0, Valid Loss 1023.8584594726562 \n\nEpoch 392/1000, Loss 112.68553924560547\nFold 0, Valid Loss 1030.5028076171875 \n\nEpoch 393/1000, Loss 113.12089538574219\nFold 0, Valid Loss 1033.694580078125 \n\nEpoch 394/1000, Loss 114.9328842163086\nFold 0, Valid Loss 1027.59130859375 \n\nEpoch 395/1000, Loss 112.31966400146484\nFold 0, Valid Loss 1041.2030029296875 \n\nEpoch 396/1000, Loss 113.59358215332031\nFold 0, Valid Loss 1022.1461791992188 \n\nEpoch 397/1000, Loss 112.37806701660156\nFold 0, Valid Loss 1036.90087890625 \n\nEpoch 398/1000, Loss 116.23748779296875\nFold 0, Valid Loss 1025.712646484375 \n\nEpoch 399/1000, Loss 114.71257781982422\nFold 0, Valid Loss 1032.3873291015625 \n\nEpoch 400/1000, Loss 114.3660659790039\nFold 0, Valid Loss 1034.7322998046875 \n\nEpoch 401/1000, Loss 113.90532684326172\nFold 0, Valid Loss 1032.0689697265625 \n\nEpoch 402/1000, Loss 112.3919677734375\nFold 0, Valid Loss 1032.4188232421875 \n\nEpoch 403/1000, Loss 112.62321472167969\nFold 0, Valid Loss 1041.038818359375 \n\nEpoch 404/1000, Loss 111.98820495605469\nFold 0, Valid Loss 1034.5489501953125 \n\nEpoch 405/1000, Loss 111.4753189086914\nFold 0, Valid Loss 1026.9532470703125 \n\nEpoch 406/1000, Loss 112.26109313964844\nFold 0, Valid Loss 1031.923583984375 \n\nEpoch 407/1000, Loss 113.56712341308594\nFold 0, Valid Loss 1031.358154296875 \n\nEpoch 408/1000, Loss 111.78448486328125\nFold 0, Valid Loss 1038.0223388671875 \n\nEpoch 409/1000, Loss 114.01595306396484\nFold 0, Valid Loss 1030.0413818359375 \n\nEpoch 410/1000, Loss 112.4391860961914\nFold 0, Valid Loss 1037.6109619140625 \n\nEpoch 411/1000, Loss 112.22855377197266\nFold 0, Valid Loss 1017.361083984375 \n\nEpoch 412/1000, Loss 112.25885772705078\nFold 0, Valid Loss 1030.0361328125 \n\nEpoch 413/1000, Loss 112.48409271240234\nFold 0, Valid Loss 1027.7156982421875 \n\nEpoch 414/1000, Loss 112.65316009521484\nFold 0, Valid Loss 1033.7991943359375 \n\nEpoch 415/1000, Loss 112.19535064697266\nFold 0, Valid Loss 1036.092529296875 \n\nEpoch 416/1000, Loss 111.94825744628906\nFold 0, Valid Loss 1035.5628662109375 \n\nEpoch 417/1000, Loss 112.38728332519531\nFold 0, Valid Loss 1038.597900390625 \n\nEpoch 418/1000, Loss 112.60362243652344\nFold 0, Valid Loss 1031.8543701171875 \n\nEpoch 419/1000, Loss 112.63382720947266\nFold 0, Valid Loss 1031.0126953125 \n\nEpoch 420/1000, Loss 112.31595611572266\nFold 0, Valid Loss 1029.921630859375 \n\nEpoch 421/1000, Loss 110.45291137695312\nFold 0, Valid Loss 1029.148193359375 \n\nEpoch 422/1000, Loss 110.12637329101562\nFold 0, Valid Loss 1034.5238037109375 \n\nEpoch 423/1000, Loss 111.64053344726562\nFold 0, Valid Loss 1031.0611572265625 \n\nEpoch 424/1000, Loss 111.00526428222656\nFold 0, Valid Loss 1029.6748046875 \n\nEpoch 425/1000, Loss 111.34209442138672\nFold 0, Valid Loss 1025.9600830078125 \n\nEpoch 426/1000, Loss 111.63831329345703\nFold 0, Valid Loss 1031.6953125 \n\nEpoch 427/1000, Loss 111.58233642578125\nFold 0, Valid Loss 1029.0806884765625 \n\nEpoch 428/1000, Loss 111.60826873779297\nFold 0, Valid Loss 1032.37939453125 \n\nEpoch 429/1000, Loss 112.43528747558594\nFold 0, Valid Loss 1043.294677734375 \n\nEpoch 430/1000, Loss 111.61309051513672\nFold 0, Valid Loss 1029.5662841796875 \n\nEpoch 431/1000, Loss 112.24800872802734\nFold 0, Valid Loss 1029.3599853515625 \n\nEpoch 432/1000, Loss 110.37657165527344\nFold 0, Valid Loss 1033.7279052734375 \n\nEpoch 433/1000, Loss 110.11954498291016\nFold 0, Valid Loss 1025.664794921875 \n\nEpoch 434/1000, Loss 110.42642974853516\nFold 0, Valid Loss 1024.4722900390625 \n\nEpoch 435/1000, Loss 111.69530487060547\nFold 0, Valid Loss 1027.5155029296875 \n\nEpoch 436/1000, Loss 110.0232925415039\nFold 0, Valid Loss 1034.9248046875 \n\nEpoch 437/1000, Loss 113.1949234008789\nFold 0, Valid Loss 1028.915771484375 \n\nEpoch 438/1000, Loss 112.71476745605469\nFold 0, Valid Loss 1029.5491943359375 \n\nEpoch 439/1000, Loss 110.59717559814453\nFold 0, Valid Loss 1037.339111328125 \n\nEpoch 440/1000, Loss 110.68505859375\nFold 0, Valid Loss 1027.7880859375 \n\nEpoch   442: reducing learning rate of group 0 to 3.2942e-06.\nEpoch 441/1000, Loss 109.4330825805664\nFold 0, Valid Loss 1031.909912109375 \n\nEpoch 442/1000, Loss 108.87320709228516\nFold 0, Valid Loss 1025.611328125 \n\nEpoch 443/1000, Loss 109.80033111572266\nFold 0, Valid Loss 1036.176513671875 \n\nEpoch 444/1000, Loss 108.68939971923828\nFold 0, Valid Loss 1032.2412109375 \n\nEpoch 445/1000, Loss 109.13246154785156\nFold 0, Valid Loss 1028.0916748046875 \n\nEpoch 446/1000, Loss 109.95214080810547\nFold 0, Valid Loss 1030.1719970703125 \n\nEpoch 447/1000, Loss 109.76095581054688\nFold 0, Valid Loss 1030.798095703125 \n\nEpoch 448/1000, Loss 108.77750396728516\nFold 0, Valid Loss 1032.8875732421875 \n\nEpoch 449/1000, Loss 107.85372161865234\nFold 0, Valid Loss 1030.511962890625 \n\nEpoch 450/1000, Loss 108.49961853027344\nFold 0, Valid Loss 1039.82421875 \n\nEpoch 451/1000, Loss 108.8224868774414\nFold 0, Valid Loss 1033.310791015625 \n\nEpoch 452/1000, Loss 108.90418243408203\nFold 0, Valid Loss 1027.6611328125 \n\nEpoch 453/1000, Loss 108.5811996459961\nFold 0, Valid Loss 1030.6287841796875 \n\nEpoch 454/1000, Loss 108.51045227050781\nFold 0, Valid Loss 1029.559814453125 \n\nEpoch 455/1000, Loss 108.64088439941406\nFold 0, Valid Loss 1031.6427001953125 \n\nEpoch 456/1000, Loss 108.73754119873047\nFold 0, Valid Loss 1030.28076171875 \n\nEpoch 457/1000, Loss 108.58218383789062\nFold 0, Valid Loss 1033.580078125 \n\nEpoch 458/1000, Loss 108.08711242675781\nFold 0, Valid Loss 1033.501220703125 \n\nEpoch 459/1000, Loss 108.55760192871094\nFold 0, Valid Loss 1041.5640869140625 \n\nEpoch 460/1000, Loss 111.05294799804688\nFold 0, Valid Loss 1035.5250244140625 \n\nEpoch 461/1000, Loss 110.22053527832031\nFold 0, Valid Loss 1031.190673828125 \n\nEpoch 462/1000, Loss 108.02703094482422\nFold 0, Valid Loss 1038.1434326171875 \n\nEpoch 463/1000, Loss 109.1944351196289\nFold 0, Valid Loss 1029.230224609375 \n\nEpoch 464/1000, Loss 108.87397766113281\nFold 0, Valid Loss 1034.529541015625 \n\nEpoch 465/1000, Loss 107.87226104736328\nFold 0, Valid Loss 1033.3682861328125 \n\nEpoch 466/1000, Loss 107.84443664550781\nFold 0, Valid Loss 1034.9404296875 \n\nEpoch 467/1000, Loss 107.68450164794922\nFold 0, Valid Loss 1025.8758544921875 \n\nEpoch 468/1000, Loss 108.08683776855469\nFold 0, Valid Loss 1037.7618408203125 \n\nEpoch 469/1000, Loss 109.56427764892578\nFold 0, Valid Loss 1036.223388671875 \n\nEpoch 470/1000, Loss 107.49957275390625\nFold 0, Valid Loss 1030.081298828125 \n\nEpoch 471/1000, Loss 107.89970397949219\nFold 0, Valid Loss 1036.140625 \n\nEpoch 472/1000, Loss 107.57575988769531\nFold 0, Valid Loss 1032.949462890625 \n\nEpoch 473/1000, Loss 107.7210464477539\nFold 0, Valid Loss 1035.584716796875 \n\nEpoch 474/1000, Loss 107.248291015625\nFold 0, Valid Loss 1032.7786865234375 \n\nEpoch 475/1000, Loss 107.76172637939453\nFold 0, Valid Loss 1033.4332275390625 \n\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-194414bbcc4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAverageMeter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0meval_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-2c3051ebc0a2>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, train_data_loader, optimizer, train_loss)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquantile_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQUANTILES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for fold, (train_index, test_index) in enumerate(kf.split(ALL_TRAIN_PATIENTS)):\n",
    "    model = PulmonaryModel(5, len(FV))\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    train_ids = ALL_TRAIN_PATIENTS[train_index]\n",
    "    test_ids = ALL_TRAIN_PATIENTS[test_index]\n",
    "\n",
    "    df_train = combined_df[combined_df['Patient'].isin(train_ids)].reset_index(drop=True)\n",
    "    df_valid = combined_df[combined_df['Patient'].isin(test_ids)].reset_index(drop=True)\n",
    "\n",
    "    train_dataset = PulmonaryDataset(df_train, FV)\n",
    "    valid_dataset = PulmonaryDataset(df_valid, FV)\n",
    "\n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=10,\n",
    "        shuffle=True,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    valid_data_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=4,\n",
    "        shuffle=False,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=50, factor=0.7, verbose=True)\n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    \n",
    "    # tq = tqdm(range(NUM_EPOCHS), desc=f\"Fold {fold}\")\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_loss = AverageMeter()\n",
    "        valid_loss = AverageMeter()\n",
    "\n",
    "        train_one_epoch(model, train_data_loader, optimizer, train_loss)\n",
    "        eval_one_epoch(model, valid_data_loader, valid_loss, lr_scheduler)\n",
    "\n",
    "        print(f\"Epoch {epoch}/{NUM_EPOCHS}, Loss {train_loss.avg}\")\n",
    "        print(f\"Fold {fold}, Valid Loss {valid_loss.avg} \\n\")\n",
    "        \n",
    "        # tq.set_postfix(val_loss=valid_loss.avg.item())\n",
    "\n",
    "        if valid_loss.avg < best_valid_loss:\n",
    "            best_valid_loss = valid_loss.avg\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, os.path.join(CONFIG.CFG.DATA.MODELS_OUT, f\"model_fold_{fold}.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PulmonaryDataset(train_df, FV, test=False)\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, data in enumerate(train_data_loader):\n",
    "    imgarray = data['imgarray'].to(DEVICE).float()\n",
    "    out = model(imgarray)\n",
    "    print(out)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for key in array_from_id:\n",
    "    print(array_from_id[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}