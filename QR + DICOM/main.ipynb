{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "if os.path.abspath(os.pardir) not in sys.path:\n",
    "    sys.path.insert(0, os.path.abspath(os.pardir))\n",
    "import CONFIG\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pydicom\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = CONFIG.CFG.DATA.BASE\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "QUANTILES = [0.2, 0.5, 0.8]\n",
    "SCALE_COLUMNS = ['Weeks', 'FVC', 'Percent', 'Age'] #'Percent'\n",
    "SCALE_COLUMNS = ['Weeks_Passed', 'Base_FVC', 'Base_Percent', 'Base_Age']\n",
    "SEX_COLUMNS = ['Male', 'Female']\n",
    "SMOKING_STATUS_COLUMNS = ['Currently smokes', 'Ex-smoker', 'Never smoked']\n",
    "FV = SEX_COLUMNS + SMOKING_STATUS_COLUMNS + SCALE_COLUMNS\n",
    "\n",
    "# number of images used to create a single 3D array of the scan\n",
    "NUM_IMAGES = 8\n",
    "IMG_SIZE = 256\n",
    "K_FOLDS = 5\n",
    "LEARNING_RATE = 4e-5\n",
    "NUM_EPOCHS = 1000\n",
    "PRINT_EVERY = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = model_selection.KFold(K_FOLDS)\n",
    "MIN_MAX_SCALER = preprocessing.MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(DATA_DIR, \"test.csv\"))\n",
    "sub_df = pd.read_csv(os.path.join(DATA_DIR, \"sample_submission.csv\"))\n",
    "# remove the duplicates from the train_df\n",
    "train_df.drop_duplicates(keep=False, inplace=True, subset=['Patient', 'Weeks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Patient_Week</th>\n      <th>FVC</th>\n      <th>Confidence</th>\n      <th>Patient</th>\n      <th>Weeks</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ID00419637202311204720264_-12</td>\n      <td>2000</td>\n      <td>100</td>\n      <td>ID00419637202311204720264</td>\n      <td>-12</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ID00421637202311550012437_-12</td>\n      <td>2000</td>\n      <td>100</td>\n      <td>ID00421637202311550012437</td>\n      <td>-12</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ID00422637202311677017371_-12</td>\n      <td>2000</td>\n      <td>100</td>\n      <td>ID00422637202311677017371</td>\n      <td>-12</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ID00423637202312137826377_-12</td>\n      <td>2000</td>\n      <td>100</td>\n      <td>ID00423637202312137826377</td>\n      <td>-12</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ID00426637202313170790466_-12</td>\n      <td>2000</td>\n      <td>100</td>\n      <td>ID00426637202313170790466</td>\n      <td>-12</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                    Patient_Week   FVC  ...                    Patient Weeks\n0  ID00419637202311204720264_-12  2000  ...  ID00419637202311204720264   -12\n1  ID00421637202311550012437_-12  2000  ...  ID00421637202311550012437   -12\n2  ID00422637202311677017371_-12  2000  ...  ID00422637202311677017371   -12\n3  ID00423637202312137826377_-12  2000  ...  ID00423637202312137826377   -12\n4  ID00426637202313170790466_-12  2000  ...  ID00426637202313170790466   -12\n\n[5 rows x 5 columns]"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# extract the Patient and weeks from the Patient_Week column\n",
    "sub_df['Patient'] = sub_df['Patient_Week'].apply(lambda x: x.split('_')[0])\n",
    "sub_df['Weeks'] = sub_df['Patient_Week'].apply(lambda x: int(x.split('_')[-1]))\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Patient_Week</th>\n      <th>Confidence</th>\n      <th>Patient</th>\n      <th>Weeks</th>\n      <th>FVC</th>\n      <th>Percent</th>\n      <th>Age</th>\n      <th>Sex</th>\n      <th>SmokingStatus</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ID00419637202311204720264_-12</td>\n      <td>100</td>\n      <td>ID00419637202311204720264</td>\n      <td>-12</td>\n      <td>3020</td>\n      <td>70.186855</td>\n      <td>73</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ID00419637202311204720264_-11</td>\n      <td>100</td>\n      <td>ID00419637202311204720264</td>\n      <td>-11</td>\n      <td>3020</td>\n      <td>70.186855</td>\n      <td>73</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ID00419637202311204720264_-10</td>\n      <td>100</td>\n      <td>ID00419637202311204720264</td>\n      <td>-10</td>\n      <td>3020</td>\n      <td>70.186855</td>\n      <td>73</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ID00419637202311204720264_-9</td>\n      <td>100</td>\n      <td>ID00419637202311204720264</td>\n      <td>-9</td>\n      <td>3020</td>\n      <td>70.186855</td>\n      <td>73</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ID00419637202311204720264_-8</td>\n      <td>100</td>\n      <td>ID00419637202311204720264</td>\n      <td>-8</td>\n      <td>3020</td>\n      <td>70.186855</td>\n      <td>73</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                    Patient_Week  Confidence  ...   Sex  SmokingStatus\n0  ID00419637202311204720264_-12         100  ...  Male      Ex-smoker\n1  ID00419637202311204720264_-11         100  ...  Male      Ex-smoker\n2  ID00419637202311204720264_-10         100  ...  Male      Ex-smoker\n3   ID00419637202311204720264_-9         100  ...  Male      Ex-smoker\n4   ID00419637202311204720264_-8         100  ...  Male      Ex-smoker\n\n[5 rows x 9 columns]"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# merge the sub_df with the test_df\n",
    "sub_df = sub_df.drop('FVC', axis=1).merge(test_df.drop('Weeks', axis=1), on='Patient')\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['FROM'] = 'train'\n",
    "test_df['FROM'] = 'val'\n",
    "sub_df['FROM'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = train_df.append([test_df, sub_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize base_week column\n",
    "combined_df['Base_Week'] = combined_df['Weeks']\n",
    "# make the weeks from sub_df to be np.nan so that when we calculate the base_week it comes from the test_df\n",
    "combined_df.loc[combined_df['FROM'] == 'test', 'Base_Week'] = np.nan\n",
    "# now calculate the min for each patient group and set it to the Base_Week column\n",
    "combined_df['Base_Week'] = combined_df.groupby('Patient')['Base_Week'].transform('min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the base_df (where the Base_Week == the min_week we calculated) so that we can get the base_fvc, base_age and base_percentage\n",
    "base_df = combined_df[combined_df['Weeks'] == combined_df['Base_Week']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:4133: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  errors=errors,\n"
    }
   ],
   "source": [
    "base_df.rename(columns={\n",
    "    'FVC': 'Base_FVC',\n",
    "    'Percent': 'Base_Percent',\n",
    "    'Age': 'Base_Age'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.merge(base_df[['Patient', 'Base_FVC', 'Base_Percent', 'Base_Age']], on='Patient', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['Weeks_Passed'] = combined_df['Weeks'] - combined_df['Base_Week']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "MinMaxScaler(copy=True, feature_range=(0, 1))"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "MIN_MAX_SCALER.fit(combined_df[combined_df['FROM'] == 'train'][['Weeks_Passed', 'FVC', 'Percent', 'Age']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df[['Weeks_Passed', 'Base_FVC', 'Base_Percent', 'Base_Age']] = MIN_MAX_SCALER.transform(combined_df[['Weeks_Passed', 'Base_FVC', 'Base_Percent', 'Base_Age']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert categoricals into dummies\n",
    "combined_df['Sex'] = pd.Categorical(combined_df['Sex'], categories=SEX_COLUMNS)\n",
    "combined_df['SmokingStatus'] = pd.Categorical(combined_df['SmokingStatus'], categories=SMOKING_STATUS_COLUMNS)\n",
    "combined_df = combined_df.join(pd.get_dummies(combined_df['Sex']))\n",
    "combined_df = combined_df.join(pd.get_dummies(combined_df['SmokingStatus']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Patient</th>\n      <th>Weeks</th>\n      <th>FVC</th>\n      <th>Percent</th>\n      <th>Age</th>\n      <th>Sex</th>\n      <th>SmokingStatus</th>\n      <th>FROM</th>\n      <th>Patient_Week</th>\n      <th>Confidence</th>\n      <th>Base_Week</th>\n      <th>Base_FVC</th>\n      <th>Base_Percent</th>\n      <th>Base_Age</th>\n      <th>Weeks_Passed</th>\n      <th>Male</th>\n      <th>Female</th>\n      <th>Currently smokes</th>\n      <th>Ex-smoker</th>\n      <th>Never smoked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ID00007637202177411956430</td>\n      <td>-4</td>\n      <td>2315</td>\n      <td>58.253649</td>\n      <td>79</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n      <td>train</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-4.0</td>\n      <td>0.267050</td>\n      <td>0.236393</td>\n      <td>0.769231</td>\n      <td>0.000000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ID00007637202177411956430</td>\n      <td>5</td>\n      <td>2214</td>\n      <td>55.712129</td>\n      <td>79</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n      <td>train</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-4.0</td>\n      <td>0.267050</td>\n      <td>0.236393</td>\n      <td>0.769231</td>\n      <td>0.142857</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ID00007637202177411956430</td>\n      <td>7</td>\n      <td>2061</td>\n      <td>51.862104</td>\n      <td>79</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n      <td>train</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-4.0</td>\n      <td>0.267050</td>\n      <td>0.236393</td>\n      <td>0.769231</td>\n      <td>0.174603</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ID00007637202177411956430</td>\n      <td>9</td>\n      <td>2144</td>\n      <td>53.950679</td>\n      <td>79</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n      <td>train</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-4.0</td>\n      <td>0.267050</td>\n      <td>0.236393</td>\n      <td>0.769231</td>\n      <td>0.206349</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ID00007637202177411956430</td>\n      <td>11</td>\n      <td>2069</td>\n      <td>52.063412</td>\n      <td>79</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n      <td>train</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-4.0</td>\n      <td>0.267050</td>\n      <td>0.236393</td>\n      <td>0.769231</td>\n      <td>0.238095</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2265</th>\n      <td>ID00426637202313170790466</td>\n      <td>129</td>\n      <td>2925</td>\n      <td>71.824968</td>\n      <td>73</td>\n      <td>Male</td>\n      <td>Never smoked</td>\n      <td>test</td>\n      <td>ID00426637202313170790466_129</td>\n      <td>100.0</td>\n      <td>0.0</td>\n      <td>0.376525</td>\n      <td>0.345604</td>\n      <td>0.615385</td>\n      <td>2.047619</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2266</th>\n      <td>ID00426637202313170790466</td>\n      <td>130</td>\n      <td>2925</td>\n      <td>71.824968</td>\n      <td>73</td>\n      <td>Male</td>\n      <td>Never smoked</td>\n      <td>test</td>\n      <td>ID00426637202313170790466_130</td>\n      <td>100.0</td>\n      <td>0.0</td>\n      <td>0.376525</td>\n      <td>0.345604</td>\n      <td>0.615385</td>\n      <td>2.063492</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2267</th>\n      <td>ID00426637202313170790466</td>\n      <td>131</td>\n      <td>2925</td>\n      <td>71.824968</td>\n      <td>73</td>\n      <td>Male</td>\n      <td>Never smoked</td>\n      <td>test</td>\n      <td>ID00426637202313170790466_131</td>\n      <td>100.0</td>\n      <td>0.0</td>\n      <td>0.376525</td>\n      <td>0.345604</td>\n      <td>0.615385</td>\n      <td>2.079365</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2268</th>\n      <td>ID00426637202313170790466</td>\n      <td>132</td>\n      <td>2925</td>\n      <td>71.824968</td>\n      <td>73</td>\n      <td>Male</td>\n      <td>Never smoked</td>\n      <td>test</td>\n      <td>ID00426637202313170790466_132</td>\n      <td>100.0</td>\n      <td>0.0</td>\n      <td>0.376525</td>\n      <td>0.345604</td>\n      <td>0.615385</td>\n      <td>2.095238</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2269</th>\n      <td>ID00426637202313170790466</td>\n      <td>133</td>\n      <td>2925</td>\n      <td>71.824968</td>\n      <td>73</td>\n      <td>Male</td>\n      <td>Never smoked</td>\n      <td>test</td>\n      <td>ID00426637202313170790466_133</td>\n      <td>100.0</td>\n      <td>0.0</td>\n      <td>0.376525</td>\n      <td>0.345604</td>\n      <td>0.615385</td>\n      <td>2.111111</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>2270 rows × 20 columns</p>\n</div>",
      "text/plain": "                        Patient  Weeks  ...  Ex-smoker  Never smoked\n0     ID00007637202177411956430     -4  ...          1             0\n1     ID00007637202177411956430      5  ...          1             0\n2     ID00007637202177411956430      7  ...          1             0\n3     ID00007637202177411956430      9  ...          1             0\n4     ID00007637202177411956430     11  ...          1             0\n...                         ...    ...  ...        ...           ...\n2265  ID00426637202313170790466    129  ...          0             1\n2266  ID00426637202313170790466    130  ...          0             1\n2267  ID00426637202313170790466    131  ...          0             1\n2268  ID00426637202313170790466    132  ...          0             1\n2269  ID00426637202313170790466    133  ...          0             1\n\n[2270 rows x 20 columns]"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "combined_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATIENTS = train_df['Patient'].unique().tolist()\n",
    "# gave the gdcm error\n",
    "BAD_PATIENT_IDS = ['ID00011637202177653955184', 'ID00052637202186188008618']\n",
    "ALL_TRAIN_PATIENTS = np.array([pat for pat in TRAIN_PATIENTS if pat not in BAD_PATIENT_IDS])\n",
    "ALL_TEST_PATIENTS = test_df['Patient'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_averaged_slices(patient_id, folder_path, num_images):\n",
    "    # the preprocessed array with NUM_SLICES elements\n",
    "    # TODO: Handle the case when the NUM_SLICES > the actual total slices\n",
    "    # TODO: resize the image to 256 X 256?\n",
    "\n",
    "    full_path = os.path.join(folder_path, patient_id)\n",
    "    # list of all files in that path and sort them\n",
    "    all_files = os.listdir(full_path)\n",
    "    # sorted using the first number part of the file name\n",
    "    all_files.sort(key = lambda x: int(x.split('.')[0]))\n",
    "\n",
    "    # read all the dicom files for the patient into the slices list\n",
    "    slices = [pydicom.dcmread(os.path.join(full_path, s)) for s in all_files]\n",
    "    # sort the slices using their order (file number works too)\n",
    "    # slices.sort(key = lambda x: int(x.ImagePositionPatient[2]))\n",
    "\n",
    "    # final array containing averaged num_images images\n",
    "    out_array = []\n",
    "\n",
    "    # how many extra files while averaging all images into (num_images) images\n",
    "    remainder_array_size = len(slices)%num_images\n",
    "\n",
    "    # how many to average to get a single averaged image\n",
    "    avging_array_size = len(slices)//num_images\n",
    "\n",
    "    # get the first one with the remainder images\n",
    "    first_array = []\n",
    "    # select the first remainder + avg_arrray_size imgaes and average into one\n",
    "    for slice in slices[:remainder_array_size+avging_array_size]:\n",
    "        first_array.append(slice.pixel_array)\n",
    "    first_avged_array = np.average(first_array, axis=0)\n",
    "    first_resized = cv2.resize(first_avged_array / 2**11, (IMG_SIZE, IMG_SIZE))\n",
    "    out_array.append(first_resized)\n",
    "\n",
    "    # after the first one get the remaining ones into out_array rolling averaging (avging_array_size) at a time.\n",
    "    for i in range(remainder_array_size + avging_array_size, len(slices), avging_array_size):\n",
    "        temp_array = []\n",
    "        for slice in slices[i:i+avging_array_size]:\n",
    "            temp_array.append(slice.pixel_array)\n",
    "        avged_temp_array = np.average(temp_array, axis=0)\n",
    "        avged_resized = cv2.resize(avged_temp_array / 2**11, (IMG_SIZE, IMG_SIZE))\n",
    "        out_array.append(avged_resized)\n",
    "    \n",
    "    return np.array(out_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_from_id = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the train and test images in array_from_id\n",
    "for id in ALL_TRAIN_PATIENTS:\n",
    "    array_from_id[id] = get_averaged_slices(id, os.path.join(DATA_DIR, \"train\"), NUM_IMAGES)\n",
    "\n",
    "for id in ALL_TEST_PATIENTS:\n",
    "    array_from_id[id] = get_averaged_slices(id, os.path.join(DATA_DIR, \"test\"), NUM_IMAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PulmonaryDataset(Dataset):\n",
    "    def __init__(self, df, FV, test=False):\n",
    "        self.df = df\n",
    "        self.test = test\n",
    "        self.FV = FV\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'imgarray': torch.from_numpy(array_from_id[self.df.iloc[idx]['Patient']]).unsqueeze(0),\n",
    "            'features': torch.tensor(self.df[self.FV].iloc[idx].values),\n",
    "            'target': torch.tensor(self.df['FVC'].iloc[idx])\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PulmonaryModel(nn.Module):\n",
    "    def __init__(self, cnn_output_size=5, in_features=9, out_quantiles=3):\n",
    "        super(PulmonaryModel, self).__init__()\n",
    "\n",
    "        self.conv_layer1 = self._make_conv_layer(1, 8)\n",
    "        self.conv_layer2 = self._make_conv_layer(8, 32)\n",
    "        self.conv_layer3 = self._make_conv_layer(32, 64)\n",
    "        self.conv_layer4 = nn.Conv3d(64, 128, kernel_size=(1, 3, 3))\n",
    "        self.conv_layer5 = nn.Conv3d(128, 128, kernel_size=(1,3,3), padding=0)\n",
    "\n",
    "        self.fc1 = nn.Linear(86528, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, cnn_output_size)\n",
    "        \n",
    "        self.fc4 = nn.Linear(cnn_output_size + in_features, 100)\n",
    "        self.fc5 = nn.Linear(100, out_quantiles)\n",
    "\n",
    "    def _make_conv_layer(self, in_c, out_c):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv3d(in_c, out_c, kernel_size=(2,3,3), padding=0),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv3d(out_c, out_c, kernel_size=(2, 3, 3), padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool3d((2,2,2)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = self.conv_layer1(x)\n",
    "        x = self.conv_layer2(x)\n",
    "        x = self.conv_layer3(x)\n",
    "        x = self.conv_layer4(x)\n",
    "        x = self.conv_layer5(x)\n",
    "\n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # concatenate the in_features\n",
    "        x = torch.cat((x, y), dim=1)\n",
    "        x = self.fc4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_one, clip_two = torch.tensor(70, dtype=torch.float32).to(DEVICE), torch.tensor(1000, dtype=torch.float32).to(DEVICE)\n",
    "def score(y_true, y_pred):\n",
    "    sigma = y_pred[:, 2] - y_pred[:, 0]\n",
    "    fvc_pred = y_pred[:, 1]\n",
    "\n",
    "    sigma_clip = torch.max(sigma, clip_one)\n",
    "    delta = torch.abs(y_true - fvc_pred)\n",
    "    delta = torch.min(delta, clip_two)\n",
    "    sq2 = torch.sqrt(torch.tensor(2, dtype=torch.float32))\n",
    "    metric = (delta / sigma_clip) * sq2 + torch.log(sigma_clip * sq2)\n",
    "    return torch.mean(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_loss(preds, target, quantiles, _lambda):\n",
    "    assert not target.requires_grad\n",
    "    assert preds.size(0) == target.size(0)\n",
    "    losses = []\n",
    "    for i, q in enumerate(quantiles):\n",
    "        errors = target - preds[:, i]\n",
    "        losses.append(torch.max((q - 1) * errors, q * errors).unsqueeze(1))\n",
    "    loss = torch.mean(torch.sum(torch.cat(losses, dim=1), dim=1))\n",
    "    return loss\n",
    "    # return _lambda * loss + (1 - _lambda) * score(target, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_data_loader, optimizer, train_loss):\n",
    "    model.train()\n",
    "    for i, data in enumerate(train_data_loader):\n",
    "        images = data['imgarray']\n",
    "        features = data['features']\n",
    "        targets = data['target']\n",
    "\n",
    "        images = images.to(DEVICE).float()\n",
    "        features = features.to(DEVICE).float()\n",
    "        targets = targets.to(DEVICE).float()\n",
    "\n",
    "        model.zero_grad()\n",
    "        out = model(images, features)\n",
    "        loss = quantile_loss(out, targets, QUANTILES, 0.6)\n",
    "        train_loss.update(loss, features.size(0))\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_one_epoch(model, valid_data_loader, valid_loss, lr_scheduler):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(valid_data_loader):\n",
    "            images = data['imgarray']\n",
    "            features = data['features']\n",
    "            targets = data['target']\n",
    "\n",
    "            images = images.to(DEVICE).float()\n",
    "            features = features.to(DEVICE).float()\n",
    "            targets = targets.to(DEVICE).float()\n",
    "            \n",
    "            out = model(images, features)\n",
    "            loss = quantile_loss(out, targets, QUANTILES, 0.6)\n",
    "            valid_loss.update(loss, features.size(0))\n",
    "    \n",
    "    if lr_scheduler is not None:\n",
    "        lr_scheduler.step(valid_loss.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 0/1000, Loss 1805.4525146484375\nFold 0, Valid Loss 893.6427612304688 \n\nEpoch 1/1000, Loss 678.4828491210938\nFold 0, Valid Loss 963.680908203125 \n\nEpoch 2/1000, Loss 651.6570434570312\nFold 0, Valid Loss 834.385009765625 \n\nEpoch 3/1000, Loss 589.5048217773438\nFold 0, Valid Loss 795.1798095703125 \n\nEpoch 4/1000, Loss 574.44677734375\nFold 0, Valid Loss 772.63134765625 \n\nEpoch 5/1000, Loss 557.0441284179688\nFold 0, Valid Loss 746.9144897460938 \n\nEpoch 6/1000, Loss 533.31298828125\nFold 0, Valid Loss 740.4525146484375 \n\nEpoch 7/1000, Loss 517.7510375976562\nFold 0, Valid Loss 788.6326904296875 \n\nEpoch 8/1000, Loss 514.7637329101562\nFold 0, Valid Loss 765.7257690429688 \n\nEpoch 9/1000, Loss 501.17926025390625\nFold 0, Valid Loss 765.1182861328125 \n\nEpoch 10/1000, Loss 485.8493347167969\nFold 0, Valid Loss 753.607421875 \n\nEpoch 11/1000, Loss 468.3171691894531\nFold 0, Valid Loss 790.8385620117188 \n\nEpoch 12/1000, Loss 455.6199645996094\nFold 0, Valid Loss 798.25 \n\nEpoch 13/1000, Loss 454.7309875488281\nFold 0, Valid Loss 790.878173828125 \n\nEpoch 14/1000, Loss 439.27069091796875\nFold 0, Valid Loss 765.1514282226562 \n\nEpoch 15/1000, Loss 446.1888427734375\nFold 0, Valid Loss 783.2210083007812 \n\nEpoch 16/1000, Loss 414.2830505371094\nFold 0, Valid Loss 814.3856811523438 \n\nEpoch 17/1000, Loss 410.62152099609375\nFold 0, Valid Loss 783.1650390625 \n\nEpoch 18/1000, Loss 398.2400207519531\nFold 0, Valid Loss 822.6455078125 \n\nEpoch 19/1000, Loss 381.4746398925781\nFold 0, Valid Loss 788.705322265625 \n\nEpoch 20/1000, Loss 370.29974365234375\nFold 0, Valid Loss 790.2823486328125 \n\nEpoch 21/1000, Loss 360.7236022949219\nFold 0, Valid Loss 799.66259765625 \n\nEpoch 22/1000, Loss 355.3497314453125\nFold 0, Valid Loss 813.1083984375 \n\nEpoch 23/1000, Loss 362.459716796875\nFold 0, Valid Loss 765.9263916015625 \n\nEpoch 24/1000, Loss 342.2502746582031\nFold 0, Valid Loss 795.6919555664062 \n\nEpoch 25/1000, Loss 318.2550354003906\nFold 0, Valid Loss 789.177490234375 \n\nEpoch 26/1000, Loss 324.1520080566406\nFold 0, Valid Loss 832.0816650390625 \n\nEpoch 27/1000, Loss 322.8592834472656\nFold 0, Valid Loss 792.13525390625 \n\nEpoch 28/1000, Loss 289.2792053222656\nFold 0, Valid Loss 880.188720703125 \n\nEpoch 29/1000, Loss 305.8433837890625\nFold 0, Valid Loss 798.5111083984375 \n\nEpoch 30/1000, Loss 287.7212219238281\nFold 0, Valid Loss 866.4052124023438 \n\nEpoch 31/1000, Loss 285.5097961425781\nFold 0, Valid Loss 827.5186157226562 \n\nEpoch 32/1000, Loss 276.752685546875\nFold 0, Valid Loss 809.8748779296875 \n\nEpoch 33/1000, Loss 254.04295349121094\nFold 0, Valid Loss 805.7815551757812 \n\nEpoch 34/1000, Loss 247.942138671875\nFold 0, Valid Loss 836.03564453125 \n\nEpoch 35/1000, Loss 245.56565856933594\nFold 0, Valid Loss 823.0910034179688 \n\nEpoch 36/1000, Loss 253.18826293945312\nFold 0, Valid Loss 852.9822387695312 \n\nEpoch 37/1000, Loss 235.35052490234375\nFold 0, Valid Loss 894.719482421875 \n\nEpoch 38/1000, Loss 231.4797821044922\nFold 0, Valid Loss 881.4727783203125 \n\nEpoch 39/1000, Loss 217.80975341796875\nFold 0, Valid Loss 854.8445434570312 \n\nEpoch 40/1000, Loss 214.8763427734375\nFold 0, Valid Loss 834.8124389648438 \n\nEpoch 41/1000, Loss 212.55453491210938\nFold 0, Valid Loss 859.0125122070312 \n\nEpoch 42/1000, Loss 201.95423889160156\nFold 0, Valid Loss 870.4305419921875 \n\nEpoch 43/1000, Loss 198.15428161621094\nFold 0, Valid Loss 963.2488403320312 \n\nEpoch 44/1000, Loss 198.21844482421875\nFold 0, Valid Loss 934.5615234375 \n\nEpoch 45/1000, Loss 181.36666870117188\nFold 0, Valid Loss 946.422607421875 \n\nEpoch 46/1000, Loss 181.04263305664062\nFold 0, Valid Loss 960.0585327148438 \n\nEpoch 47/1000, Loss 183.9530029296875\nFold 0, Valid Loss 908.3381958007812 \n\nEpoch 48/1000, Loss 180.7808380126953\nFold 0, Valid Loss 974.3291625976562 \n\nEpoch 49/1000, Loss 182.9608917236328\nFold 0, Valid Loss 931.660888671875 \n\nEpoch 50/1000, Loss 173.45733642578125\nFold 0, Valid Loss 946.881591796875 \n\nEpoch 51/1000, Loss 178.79421997070312\nFold 0, Valid Loss 937.8007202148438 \n\nEpoch 52/1000, Loss 165.62974548339844\nFold 0, Valid Loss 974.4418334960938 \n\nEpoch 53/1000, Loss 168.14634704589844\nFold 0, Valid Loss 999.3052368164062 \n\nEpoch 54/1000, Loss 161.6722869873047\nFold 0, Valid Loss 966.00830078125 \n\nEpoch 55/1000, Loss 170.16912841796875\nFold 0, Valid Loss 937.0739135742188 \n\nEpoch 56/1000, Loss 162.27940368652344\nFold 0, Valid Loss 967.9991455078125 \n\nEpoch    58: reducing learning rate of group 0 to 2.8000e-05.\nEpoch 57/1000, Loss 158.612548828125\nFold 0, Valid Loss 994.8578491210938 \n\nEpoch 58/1000, Loss 143.98855590820312\nFold 0, Valid Loss 1005.5695190429688 \n\nEpoch 59/1000, Loss 144.69647216796875\nFold 0, Valid Loss 1034.7447509765625 \n\nEpoch 60/1000, Loss 146.88059997558594\nFold 0, Valid Loss 1009.36572265625 \n\nEpoch 61/1000, Loss 141.43028259277344\nFold 0, Valid Loss 1007.835205078125 \n\nEpoch 62/1000, Loss 141.90716552734375\nFold 0, Valid Loss 990.9442138671875 \n\nEpoch 63/1000, Loss 135.37989807128906\nFold 0, Valid Loss 989.645751953125 \n\nEpoch 64/1000, Loss 132.1840057373047\nFold 0, Valid Loss 1028.841796875 \n\nEpoch 65/1000, Loss 130.63040161132812\nFold 0, Valid Loss 1016.0946655273438 \n\nEpoch 66/1000, Loss 130.53286743164062\nFold 0, Valid Loss 1050.2822265625 \n\nEpoch 67/1000, Loss 135.55361938476562\nFold 0, Valid Loss 1029.0264892578125 \n\nEpoch 68/1000, Loss 125.5501480102539\nFold 0, Valid Loss 1019.0021362304688 \n\nEpoch 69/1000, Loss 131.98251342773438\nFold 0, Valid Loss 1009.2202758789062 \n\nEpoch 70/1000, Loss 139.57818603515625\nFold 0, Valid Loss 995.4125366210938 \n\nEpoch 71/1000, Loss 124.66655731201172\nFold 0, Valid Loss 1039.0087890625 \n\nEpoch 72/1000, Loss 128.2386016845703\nFold 0, Valid Loss 1013.17333984375 \n\nEpoch 73/1000, Loss 126.05399322509766\nFold 0, Valid Loss 1003.3757934570312 \n\nEpoch 74/1000, Loss 123.22444915771484\nFold 0, Valid Loss 1025.3109130859375 \n\nEpoch 75/1000, Loss 131.15960693359375\nFold 0, Valid Loss 1026.977783203125 \n\nEpoch 76/1000, Loss 131.7040557861328\nFold 0, Valid Loss 1019.951904296875 \n\nEpoch 77/1000, Loss 128.83384704589844\nFold 0, Valid Loss 1013.94287109375 \n\nEpoch 78/1000, Loss 124.814208984375\nFold 0, Valid Loss 1036.3349609375 \n\nEpoch 79/1000, Loss 115.98966217041016\nFold 0, Valid Loss 998.27001953125 \n\nEpoch 80/1000, Loss 124.03842163085938\nFold 0, Valid Loss 1013.9254150390625 \n\nEpoch 81/1000, Loss 120.10950469970703\nFold 0, Valid Loss 1033.966796875 \n\nEpoch 82/1000, Loss 124.82188415527344\nFold 0, Valid Loss 993.5010375976562 \n\nEpoch 83/1000, Loss 123.71591186523438\nFold 0, Valid Loss 1004.4281616210938 \n\nEpoch 84/1000, Loss 124.05843353271484\nFold 0, Valid Loss 1032.155029296875 \n\nEpoch 85/1000, Loss 117.82653045654297\nFold 0, Valid Loss 1001.7047119140625 \n\nEpoch 86/1000, Loss 122.17150115966797\nFold 0, Valid Loss 1023.0081787109375 \n\nEpoch 87/1000, Loss 115.21643829345703\nFold 0, Valid Loss 1011.8569946289062 \n\nEpoch 88/1000, Loss 121.2142333984375\nFold 0, Valid Loss 1037.2105712890625 \n\nEpoch 89/1000, Loss 125.9699935913086\nFold 0, Valid Loss 1015.3113403320312 \n\nEpoch 90/1000, Loss 127.62892150878906\nFold 0, Valid Loss 1005.2177124023438 \n\nEpoch 91/1000, Loss 117.83898162841797\nFold 0, Valid Loss 995.9370727539062 \n\nEpoch 92/1000, Loss 125.50345611572266\nFold 0, Valid Loss 1033.6107177734375 \n\nEpoch 93/1000, Loss 128.42906188964844\nFold 0, Valid Loss 1034.21240234375 \n\nEpoch 94/1000, Loss 126.10881805419922\nFold 0, Valid Loss 1007.6082763671875 \n\nEpoch 95/1000, Loss 130.35491943359375\nFold 0, Valid Loss 1015.3594970703125 \n\nEpoch 96/1000, Loss 132.04367065429688\nFold 0, Valid Loss 1004.887939453125 \n\nEpoch 97/1000, Loss 125.62167358398438\nFold 0, Valid Loss 1015.1264038085938 \n\nEpoch 98/1000, Loss 119.71922302246094\nFold 0, Valid Loss 1027.57275390625 \n\nEpoch 99/1000, Loss 118.15577697753906\nFold 0, Valid Loss 1015.6378784179688 \n\nEpoch 100/1000, Loss 121.53702545166016\nFold 0, Valid Loss 995.0809326171875 \n\nEpoch 101/1000, Loss 129.28273010253906\nFold 0, Valid Loss 1002.3616943359375 \n\nEpoch 102/1000, Loss 124.43301391601562\nFold 0, Valid Loss 1032.306884765625 \n\nEpoch 103/1000, Loss 127.24364471435547\nFold 0, Valid Loss 1024.9464111328125 \n\nEpoch 104/1000, Loss 123.62654876708984\nFold 0, Valid Loss 994.160400390625 \n\nEpoch 105/1000, Loss 120.84842681884766\nFold 0, Valid Loss 1030.9283447265625 \n\nEpoch 106/1000, Loss 132.158447265625\nFold 0, Valid Loss 1011.89697265625 \n\nEpoch 107/1000, Loss 116.59379577636719\nFold 0, Valid Loss 999.610107421875 \n\nEpoch   109: reducing learning rate of group 0 to 1.9600e-05.\nEpoch 108/1000, Loss 127.24658966064453\nFold 0, Valid Loss 996.2007446289062 \n\nEpoch 109/1000, Loss 111.06090545654297\nFold 0, Valid Loss 1013.7562255859375 \n\nEpoch 110/1000, Loss 110.36194610595703\nFold 0, Valid Loss 1014.0018310546875 \n\nEpoch 111/1000, Loss 106.15408325195312\nFold 0, Valid Loss 1026.060546875 \n\nEpoch 112/1000, Loss 112.54312896728516\nFold 0, Valid Loss 1021.0966186523438 \n\nEpoch 113/1000, Loss 112.26327514648438\nFold 0, Valid Loss 1011.820068359375 \n\nEpoch 114/1000, Loss 109.60685729980469\nFold 0, Valid Loss 996.51806640625 \n\nEpoch 115/1000, Loss 113.4353256225586\nFold 0, Valid Loss 1008.1141967773438 \n\nEpoch 116/1000, Loss 110.76610565185547\nFold 0, Valid Loss 1020.45556640625 \n\nEpoch 117/1000, Loss 113.42329406738281\nFold 0, Valid Loss 1013.6112670898438 \n\nEpoch 118/1000, Loss 106.50653839111328\nFold 0, Valid Loss 1016.5911254882812 \n\nEpoch 119/1000, Loss 110.65563201904297\nFold 0, Valid Loss 1009.9155883789062 \n\nEpoch 120/1000, Loss 110.90164947509766\nFold 0, Valid Loss 989.2057495117188 \n\nEpoch 121/1000, Loss 114.68167114257812\nFold 0, Valid Loss 1006.9667358398438 \n\nEpoch 122/1000, Loss 113.36186218261719\nFold 0, Valid Loss 1028.5023193359375 \n\nEpoch 123/1000, Loss 112.34815216064453\nFold 0, Valid Loss 998.7930908203125 \n\nEpoch 124/1000, Loss 112.77002716064453\nFold 0, Valid Loss 991.7329711914062 \n\nEpoch 125/1000, Loss 116.96393585205078\nFold 0, Valid Loss 991.1416015625 \n\nEpoch 126/1000, Loss 109.3898696899414\nFold 0, Valid Loss 1037.2022705078125 \n\nEpoch 127/1000, Loss 111.77577209472656\nFold 0, Valid Loss 1014.9375 \n\nEpoch 128/1000, Loss 109.64945983886719\nFold 0, Valid Loss 1004.4111938476562 \n\nEpoch 129/1000, Loss 115.15364837646484\nFold 0, Valid Loss 995.2161254882812 \n\nEpoch 130/1000, Loss 109.4892807006836\nFold 0, Valid Loss 999.6129150390625 \n\nEpoch 131/1000, Loss 108.73411560058594\nFold 0, Valid Loss 1018.795654296875 \n\nEpoch 132/1000, Loss 112.9085922241211\nFold 0, Valid Loss 994.1524047851562 \n\nEpoch 133/1000, Loss 112.6971664428711\nFold 0, Valid Loss 1023.4791259765625 \n\nEpoch 134/1000, Loss 114.5235595703125\nFold 0, Valid Loss 1007.0711669921875 \n\nEpoch 135/1000, Loss 108.85591125488281\nFold 0, Valid Loss 1016.4266967773438 \n\nEpoch 136/1000, Loss 111.39128875732422\nFold 0, Valid Loss 1011.763671875 \n\nEpoch 137/1000, Loss 106.45926666259766\nFold 0, Valid Loss 995.9864501953125 \n\nEpoch 138/1000, Loss 109.38467407226562\nFold 0, Valid Loss 972.9430541992188 \n\nEpoch 139/1000, Loss 116.6757583618164\nFold 0, Valid Loss 974.677001953125 \n\nEpoch 140/1000, Loss 114.42001342773438\nFold 0, Valid Loss 989.5585327148438 \n\nEpoch 141/1000, Loss 110.63078308105469\nFold 0, Valid Loss 987.3282470703125 \n\nEpoch 142/1000, Loss 106.94309997558594\nFold 0, Valid Loss 987.92333984375 \n\nEpoch 143/1000, Loss 120.54108428955078\nFold 0, Valid Loss 983.6383666992188 \n\nEpoch 144/1000, Loss 114.02876281738281\nFold 0, Valid Loss 984.7682495117188 \n\nEpoch 145/1000, Loss 111.43720245361328\nFold 0, Valid Loss 979.750732421875 \n\nEpoch 146/1000, Loss 116.22439575195312\nFold 0, Valid Loss 1010.0938110351562 \n\nEpoch 147/1000, Loss 108.72735595703125\nFold 0, Valid Loss 1005.9285278320312 \n\nEpoch 148/1000, Loss 107.85456848144531\nFold 0, Valid Loss 1020.4652709960938 \n\nEpoch 149/1000, Loss 112.8040542602539\nFold 0, Valid Loss 1000.5349731445312 \n\nEpoch 150/1000, Loss 112.59029388427734\nFold 0, Valid Loss 983.1546630859375 \n\nEpoch 151/1000, Loss 115.42213439941406\nFold 0, Valid Loss 994.90869140625 \n\nEpoch 152/1000, Loss 113.46166229248047\nFold 0, Valid Loss 988.7813110351562 \n\nEpoch 153/1000, Loss 110.05094146728516\nFold 0, Valid Loss 989.1131591796875 \n\nEpoch 154/1000, Loss 119.38269805908203\nFold 0, Valid Loss 993.7232666015625 \n\nEpoch 155/1000, Loss 107.19966125488281\nFold 0, Valid Loss 1002.9301147460938 \n\nEpoch 156/1000, Loss 110.52652740478516\nFold 0, Valid Loss 998.3603515625 \n\nEpoch 157/1000, Loss 109.25032043457031\nFold 0, Valid Loss 989.2860107421875 \n\nEpoch 158/1000, Loss 112.35269927978516\nFold 0, Valid Loss 1004.2384643554688 \n\nEpoch   160: reducing learning rate of group 0 to 1.3720e-05.\nEpoch 159/1000, Loss 113.18946838378906\nFold 0, Valid Loss 990.0322875976562 \n\nEpoch 160/1000, Loss 105.32789611816406\nFold 0, Valid Loss 1017.1373291015625 \n\nEpoch 161/1000, Loss 104.00680541992188\nFold 0, Valid Loss 996.2025146484375 \n\nEpoch 162/1000, Loss 103.55609893798828\nFold 0, Valid Loss 998.4520263671875 \n\nEpoch 163/1000, Loss 106.95265197753906\nFold 0, Valid Loss 982.0001831054688 \n\nEpoch 164/1000, Loss 104.06484985351562\nFold 0, Valid Loss 1000.1906127929688 \n\nEpoch 165/1000, Loss 104.0679702758789\nFold 0, Valid Loss 1024.09716796875 \n\nEpoch 166/1000, Loss 110.77021026611328\nFold 0, Valid Loss 1014.3553466796875 \n\nEpoch 167/1000, Loss 105.37213134765625\nFold 0, Valid Loss 977.625732421875 \n\nEpoch 168/1000, Loss 105.6666030883789\nFold 0, Valid Loss 1004.2545166015625 \n\nEpoch 169/1000, Loss 103.46260833740234\nFold 0, Valid Loss 1021.055419921875 \n\nEpoch 170/1000, Loss 107.73310852050781\nFold 0, Valid Loss 1007.0029907226562 \n\nEpoch 171/1000, Loss 110.78520965576172\nFold 0, Valid Loss 1019.269775390625 \n\nEpoch 172/1000, Loss 105.87068939208984\nFold 0, Valid Loss 1017.457275390625 \n\nEpoch 173/1000, Loss 105.10704803466797\nFold 0, Valid Loss 995.1170043945312 \n\nEpoch 174/1000, Loss 107.26378631591797\nFold 0, Valid Loss 1020.8735961914062 \n\nEpoch 175/1000, Loss 101.65608978271484\nFold 0, Valid Loss 1006.1805419921875 \n\nEpoch 176/1000, Loss 106.07141876220703\nFold 0, Valid Loss 987.86181640625 \n\nEpoch 177/1000, Loss 105.63795471191406\nFold 0, Valid Loss 1002.5250244140625 \n\nEpoch 178/1000, Loss 105.8778305053711\nFold 0, Valid Loss 993.0328979492188 \n\nEpoch 179/1000, Loss 105.64954376220703\nFold 0, Valid Loss 1000.31005859375 \n\nEpoch 180/1000, Loss 103.238525390625\nFold 0, Valid Loss 1009.7514038085938 \n\nEpoch 181/1000, Loss 104.81896209716797\nFold 0, Valid Loss 1012.2432250976562 \n\nEpoch 182/1000, Loss 106.26541900634766\nFold 0, Valid Loss 1017.2315063476562 \n\nEpoch 183/1000, Loss 105.72528076171875\nFold 0, Valid Loss 1012.8876342773438 \n\nEpoch 184/1000, Loss 104.40987396240234\nFold 0, Valid Loss 1002.4218139648438 \n\nEpoch 185/1000, Loss 107.67450714111328\nFold 0, Valid Loss 1011.6103515625 \n\nEpoch 186/1000, Loss 107.70243072509766\nFold 0, Valid Loss 984.5043334960938 \n\nEpoch 187/1000, Loss 111.79859161376953\nFold 0, Valid Loss 1000.4832763671875 \n\nEpoch 188/1000, Loss 103.59239959716797\nFold 0, Valid Loss 1002.3432006835938 \n\nEpoch 189/1000, Loss 105.67857360839844\nFold 0, Valid Loss 1018.3250122070312 \n\nEpoch 190/1000, Loss 108.78398895263672\nFold 0, Valid Loss 1000.8430786132812 \n\nEpoch 191/1000, Loss 105.28397369384766\nFold 0, Valid Loss 1007.0548706054688 \n\nEpoch 192/1000, Loss 107.10184478759766\nFold 0, Valid Loss 989.6848754882812 \n\nEpoch 193/1000, Loss 106.77291107177734\nFold 0, Valid Loss 998.3843383789062 \n\nEpoch 194/1000, Loss 105.08283996582031\nFold 0, Valid Loss 985.7980346679688 \n\nEpoch 195/1000, Loss 104.96571350097656\nFold 0, Valid Loss 980.8496704101562 \n\nEpoch 196/1000, Loss 105.74470520019531\nFold 0, Valid Loss 982.8115234375 \n\nEpoch 197/1000, Loss 105.85784149169922\nFold 0, Valid Loss 1004.4012451171875 \n\nEpoch 198/1000, Loss 105.84770965576172\nFold 0, Valid Loss 1013.4200439453125 \n\nEpoch 199/1000, Loss 103.96841430664062\nFold 0, Valid Loss 996.5703735351562 \n\nEpoch 200/1000, Loss 102.1299057006836\nFold 0, Valid Loss 1006.2771606445312 \n\nEpoch 201/1000, Loss 103.74950408935547\nFold 0, Valid Loss 987.9595947265625 \n\nEpoch 202/1000, Loss 103.00212097167969\nFold 0, Valid Loss 1001.6536254882812 \n\nEpoch 203/1000, Loss 106.76155090332031\nFold 0, Valid Loss 982.1795043945312 \n\nEpoch 204/1000, Loss 106.1779556274414\nFold 0, Valid Loss 983.3310546875 \n\nEpoch 205/1000, Loss 102.82781982421875\nFold 0, Valid Loss 1002.9739379882812 \n\nEpoch 206/1000, Loss 109.88854217529297\nFold 0, Valid Loss 989.1690063476562 \n\nEpoch 207/1000, Loss 101.74130249023438\nFold 0, Valid Loss 986.7894287109375 \n\nEpoch 208/1000, Loss 104.90198516845703\nFold 0, Valid Loss 1011.0610961914062 \n\nEpoch 209/1000, Loss 107.10026550292969\nFold 0, Valid Loss 977.975341796875 \n\nEpoch   211: reducing learning rate of group 0 to 9.6040e-06.\nEpoch 210/1000, Loss 104.43767547607422\nFold 0, Valid Loss 995.9715576171875 \n\nEpoch 211/1000, Loss 102.1146469116211\nFold 0, Valid Loss 996.2559204101562 \n\nEpoch 212/1000, Loss 101.52286529541016\nFold 0, Valid Loss 998.0086669921875 \n\nEpoch 213/1000, Loss 100.28541564941406\nFold 0, Valid Loss 996.1737670898438 \n\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-194414bbcc4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAverageMeter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0meval_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-2c3051ebc0a2>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, train_data_loader, optimizer, train_loss)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for fold, (train_index, test_index) in enumerate(kf.split(ALL_TRAIN_PATIENTS)):\n",
    "    model = PulmonaryModel(5, len(FV))\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    train_ids = ALL_TRAIN_PATIENTS[train_index]\n",
    "    test_ids = ALL_TRAIN_PATIENTS[test_index]\n",
    "\n",
    "    df_train = combined_df[combined_df['Patient'].isin(train_ids)].reset_index(drop=True)\n",
    "    df_valid = combined_df[combined_df['Patient'].isin(test_ids)].reset_index(drop=True)\n",
    "\n",
    "    train_dataset = PulmonaryDataset(df_train, FV)\n",
    "    valid_dataset = PulmonaryDataset(df_valid, FV)\n",
    "\n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=10,\n",
    "        shuffle=True,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    valid_data_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=4,\n",
    "        shuffle=False,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=50, factor=0.7, verbose=True)\n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    \n",
    "    # tq = tqdm(range(NUM_EPOCHS), desc=f\"Fold {fold}\")\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_loss = AverageMeter()\n",
    "        valid_loss = AverageMeter()\n",
    "\n",
    "        train_one_epoch(model, train_data_loader, optimizer, train_loss)\n",
    "        eval_one_epoch(model, valid_data_loader, valid_loss, lr_scheduler)\n",
    "\n",
    "        print(f\"Epoch {epoch}/{NUM_EPOCHS}, Loss {train_loss.avg}\")\n",
    "        print(f\"Fold {fold}, Valid Loss {valid_loss.avg} \\n\")\n",
    "        \n",
    "        # tq.set_postfix(val_loss=valid_loss.avg.item())\n",
    "\n",
    "        if valid_loss.avg < best_valid_loss:\n",
    "            best_valid_loss = valid_loss.avg\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, os.path.join(CONFIG.CFG.DATA.MODELS_OUT, f\"model_fold_{fold}.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PulmonaryDataset(train_df, FV, test=False)\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, data in enumerate(train_data_loader):\n",
    "    imgarray = data['imgarray'].to(DEVICE).float()\n",
    "    out = model(imgarray)\n",
    "    print(out)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for key in array_from_id:\n",
    "    print(array_from_id[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}