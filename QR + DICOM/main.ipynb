{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "if os.path.abspath(os.pardir) not in sys.path:\n",
    "    sys.path.insert(0, os.path.abspath(os.pardir))\n",
    "import CONFIG\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pydicom\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = CONFIG.CFG.DATA.BASE\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "QUANTILES = [0.2, 0.5, 0.8]\n",
    "SCALE_COLUMNS = ['Weeks', 'FVC', 'Percent', 'Age'] #'Percent'\n",
    "SCALE_COLUMNS = ['Weeks_Passed', 'Base_FVC', 'Base_Percent', 'Base_Age']\n",
    "SEX_COLUMNS = ['Male', 'Female']\n",
    "SMOKING_STATUS_COLUMNS = ['Currently smokes', 'Ex-smoker', 'Never smoked']\n",
    "FV = SEX_COLUMNS + SMOKING_STATUS_COLUMNS + SCALE_COLUMNS\n",
    "\n",
    "# number of images used to create a single 3D array of the scan\n",
    "NUM_IMAGES = 8\n",
    "IMG_SIZE = 256\n",
    "K_FOLDS = 5\n",
    "LEARNING_RATE = 4e-5\n",
    "NUM_EPOCHS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = model_selection.KFold(K_FOLDS)\n",
    "MIN_MAX_SCALER = preprocessing.MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(DATA_DIR, \"test.csv\"))\n",
    "sub_df = pd.read_csv(os.path.join(DATA_DIR, \"sample_submission.csv\"))\n",
    "# remove the duplicates from the train_df\n",
    "train_df.drop_duplicates(keep=False, inplace=True, subset=['Patient', 'Weeks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Patient_Week</th>\n      <th>FVC</th>\n      <th>Confidence</th>\n      <th>Patient</th>\n      <th>Weeks</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ID00419637202311204720264_-12</td>\n      <td>2000</td>\n      <td>100</td>\n      <td>ID00419637202311204720264</td>\n      <td>-12</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ID00421637202311550012437_-12</td>\n      <td>2000</td>\n      <td>100</td>\n      <td>ID00421637202311550012437</td>\n      <td>-12</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ID00422637202311677017371_-12</td>\n      <td>2000</td>\n      <td>100</td>\n      <td>ID00422637202311677017371</td>\n      <td>-12</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ID00423637202312137826377_-12</td>\n      <td>2000</td>\n      <td>100</td>\n      <td>ID00423637202312137826377</td>\n      <td>-12</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ID00426637202313170790466_-12</td>\n      <td>2000</td>\n      <td>100</td>\n      <td>ID00426637202313170790466</td>\n      <td>-12</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                    Patient_Week   FVC  ...                    Patient Weeks\n0  ID00419637202311204720264_-12  2000  ...  ID00419637202311204720264   -12\n1  ID00421637202311550012437_-12  2000  ...  ID00421637202311550012437   -12\n2  ID00422637202311677017371_-12  2000  ...  ID00422637202311677017371   -12\n3  ID00423637202312137826377_-12  2000  ...  ID00423637202312137826377   -12\n4  ID00426637202313170790466_-12  2000  ...  ID00426637202313170790466   -12\n\n[5 rows x 5 columns]"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# extract the Patient and weeks from the Patient_Week column\n",
    "sub_df['Patient'] = sub_df['Patient_Week'].apply(lambda x: x.split('_')[0])\n",
    "sub_df['Weeks'] = sub_df['Patient_Week'].apply(lambda x: int(x.split('_')[-1]))\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Patient_Week</th>\n      <th>Confidence</th>\n      <th>Patient</th>\n      <th>Weeks</th>\n      <th>FVC</th>\n      <th>Percent</th>\n      <th>Age</th>\n      <th>Sex</th>\n      <th>SmokingStatus</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ID00419637202311204720264_-12</td>\n      <td>100</td>\n      <td>ID00419637202311204720264</td>\n      <td>-12</td>\n      <td>3020</td>\n      <td>70.186855</td>\n      <td>73</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ID00419637202311204720264_-11</td>\n      <td>100</td>\n      <td>ID00419637202311204720264</td>\n      <td>-11</td>\n      <td>3020</td>\n      <td>70.186855</td>\n      <td>73</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ID00419637202311204720264_-10</td>\n      <td>100</td>\n      <td>ID00419637202311204720264</td>\n      <td>-10</td>\n      <td>3020</td>\n      <td>70.186855</td>\n      <td>73</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ID00419637202311204720264_-9</td>\n      <td>100</td>\n      <td>ID00419637202311204720264</td>\n      <td>-9</td>\n      <td>3020</td>\n      <td>70.186855</td>\n      <td>73</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ID00419637202311204720264_-8</td>\n      <td>100</td>\n      <td>ID00419637202311204720264</td>\n      <td>-8</td>\n      <td>3020</td>\n      <td>70.186855</td>\n      <td>73</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                    Patient_Week  Confidence  ...   Sex  SmokingStatus\n0  ID00419637202311204720264_-12         100  ...  Male      Ex-smoker\n1  ID00419637202311204720264_-11         100  ...  Male      Ex-smoker\n2  ID00419637202311204720264_-10         100  ...  Male      Ex-smoker\n3   ID00419637202311204720264_-9         100  ...  Male      Ex-smoker\n4   ID00419637202311204720264_-8         100  ...  Male      Ex-smoker\n\n[5 rows x 9 columns]"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# merge the sub_df with the test_df\n",
    "sub_df = sub_df.drop('FVC', axis=1).merge(test_df.drop('Weeks', axis=1), on='Patient')\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['FROM'] = 'train'\n",
    "test_df['FROM'] = 'val'\n",
    "sub_df['FROM'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = train_df.append([test_df, sub_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize base_week column\n",
    "combined_df['Base_Week'] = combined_df['Weeks']\n",
    "# make the weeks from sub_df to be np.nan so that when we calculate the base_week it comes from the test_df\n",
    "combined_df.loc[combined_df['FROM'] == 'test', 'Base_Week'] = np.nan\n",
    "# now calculate the min for each patient group and set it to the Base_Week column\n",
    "combined_df['Base_Week'] = combined_df.groupby('Patient')['Base_Week'].transform('min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the base_df (where the Base_Week == the min_week we calculated) so that we can get the base_fvc, base_age and base_percentage\n",
    "base_df = combined_df[combined_df['Weeks'] == combined_df['Base_Week']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:4133: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  errors=errors,\n"
    }
   ],
   "source": [
    "base_df.rename(columns={\n",
    "    'FVC': 'Base_FVC',\n",
    "    'Percent': 'Base_Percent',\n",
    "    'Age': 'Base_Age'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.merge(base_df[['Patient', 'Base_FVC', 'Base_Percent', 'Base_Age']], on='Patient', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['Weeks_Passed'] = combined_df['Weeks'] - combined_df['Base_Week']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "MinMaxScaler(copy=True, feature_range=(0, 1))"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "MIN_MAX_SCALER.fit(combined_df[combined_df['FROM'] == 'train'][['Weeks_Passed', 'FVC', 'Percent', 'Age']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df[['Weeks_Passed', 'Base_FVC', 'Base_Percent', 'Base_Age']] = MIN_MAX_SCALER.transform(combined_df[['Weeks_Passed', 'Base_FVC', 'Base_Percent', 'Base_Age']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert categoricals into dummies\n",
    "combined_df['Sex'] = pd.Categorical(combined_df['Sex'], categories=SEX_COLUMNS)\n",
    "combined_df['SmokingStatus'] = pd.Categorical(combined_df['SmokingStatus'], categories=SMOKING_STATUS_COLUMNS)\n",
    "combined_df = combined_df.join(pd.get_dummies(combined_df['Sex']))\n",
    "combined_df = combined_df.join(pd.get_dummies(combined_df['SmokingStatus']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Patient</th>\n      <th>Weeks</th>\n      <th>FVC</th>\n      <th>Percent</th>\n      <th>Age</th>\n      <th>Sex</th>\n      <th>SmokingStatus</th>\n      <th>FROM</th>\n      <th>Patient_Week</th>\n      <th>Confidence</th>\n      <th>Base_Week</th>\n      <th>Base_FVC</th>\n      <th>Base_Percent</th>\n      <th>Base_Age</th>\n      <th>Weeks_Passed</th>\n      <th>Male</th>\n      <th>Female</th>\n      <th>Currently smokes</th>\n      <th>Ex-smoker</th>\n      <th>Never smoked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ID00007637202177411956430</td>\n      <td>-4</td>\n      <td>2315</td>\n      <td>58.253649</td>\n      <td>79</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n      <td>train</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-4.0</td>\n      <td>0.267050</td>\n      <td>0.236393</td>\n      <td>0.769231</td>\n      <td>0.000000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ID00007637202177411956430</td>\n      <td>5</td>\n      <td>2214</td>\n      <td>55.712129</td>\n      <td>79</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n      <td>train</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-4.0</td>\n      <td>0.267050</td>\n      <td>0.236393</td>\n      <td>0.769231</td>\n      <td>0.142857</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ID00007637202177411956430</td>\n      <td>7</td>\n      <td>2061</td>\n      <td>51.862104</td>\n      <td>79</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n      <td>train</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-4.0</td>\n      <td>0.267050</td>\n      <td>0.236393</td>\n      <td>0.769231</td>\n      <td>0.174603</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ID00007637202177411956430</td>\n      <td>9</td>\n      <td>2144</td>\n      <td>53.950679</td>\n      <td>79</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n      <td>train</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-4.0</td>\n      <td>0.267050</td>\n      <td>0.236393</td>\n      <td>0.769231</td>\n      <td>0.206349</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ID00007637202177411956430</td>\n      <td>11</td>\n      <td>2069</td>\n      <td>52.063412</td>\n      <td>79</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n      <td>train</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-4.0</td>\n      <td>0.267050</td>\n      <td>0.236393</td>\n      <td>0.769231</td>\n      <td>0.238095</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2265</th>\n      <td>ID00426637202313170790466</td>\n      <td>129</td>\n      <td>2925</td>\n      <td>71.824968</td>\n      <td>73</td>\n      <td>Male</td>\n      <td>Never smoked</td>\n      <td>test</td>\n      <td>ID00426637202313170790466_129</td>\n      <td>100.0</td>\n      <td>0.0</td>\n      <td>0.376525</td>\n      <td>0.345604</td>\n      <td>0.615385</td>\n      <td>2.047619</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2266</th>\n      <td>ID00426637202313170790466</td>\n      <td>130</td>\n      <td>2925</td>\n      <td>71.824968</td>\n      <td>73</td>\n      <td>Male</td>\n      <td>Never smoked</td>\n      <td>test</td>\n      <td>ID00426637202313170790466_130</td>\n      <td>100.0</td>\n      <td>0.0</td>\n      <td>0.376525</td>\n      <td>0.345604</td>\n      <td>0.615385</td>\n      <td>2.063492</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2267</th>\n      <td>ID00426637202313170790466</td>\n      <td>131</td>\n      <td>2925</td>\n      <td>71.824968</td>\n      <td>73</td>\n      <td>Male</td>\n      <td>Never smoked</td>\n      <td>test</td>\n      <td>ID00426637202313170790466_131</td>\n      <td>100.0</td>\n      <td>0.0</td>\n      <td>0.376525</td>\n      <td>0.345604</td>\n      <td>0.615385</td>\n      <td>2.079365</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2268</th>\n      <td>ID00426637202313170790466</td>\n      <td>132</td>\n      <td>2925</td>\n      <td>71.824968</td>\n      <td>73</td>\n      <td>Male</td>\n      <td>Never smoked</td>\n      <td>test</td>\n      <td>ID00426637202313170790466_132</td>\n      <td>100.0</td>\n      <td>0.0</td>\n      <td>0.376525</td>\n      <td>0.345604</td>\n      <td>0.615385</td>\n      <td>2.095238</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2269</th>\n      <td>ID00426637202313170790466</td>\n      <td>133</td>\n      <td>2925</td>\n      <td>71.824968</td>\n      <td>73</td>\n      <td>Male</td>\n      <td>Never smoked</td>\n      <td>test</td>\n      <td>ID00426637202313170790466_133</td>\n      <td>100.0</td>\n      <td>0.0</td>\n      <td>0.376525</td>\n      <td>0.345604</td>\n      <td>0.615385</td>\n      <td>2.111111</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>2270 rows × 20 columns</p>\n</div>",
      "text/plain": "                        Patient  Weeks  ...  Ex-smoker  Never smoked\n0     ID00007637202177411956430     -4  ...          1             0\n1     ID00007637202177411956430      5  ...          1             0\n2     ID00007637202177411956430      7  ...          1             0\n3     ID00007637202177411956430      9  ...          1             0\n4     ID00007637202177411956430     11  ...          1             0\n...                         ...    ...  ...        ...           ...\n2265  ID00426637202313170790466    129  ...          0             1\n2266  ID00426637202313170790466    130  ...          0             1\n2267  ID00426637202313170790466    131  ...          0             1\n2268  ID00426637202313170790466    132  ...          0             1\n2269  ID00426637202313170790466    133  ...          0             1\n\n[2270 rows x 20 columns]"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "combined_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATIENTS = train_df['Patient'].unique().tolist()\n",
    "# gave the gdcm error\n",
    "BAD_PATIENT_IDS = ['ID00011637202177653955184', 'ID00052637202186188008618']\n",
    "ALL_TRAIN_PATIENTS = np.array([pat for pat in TRAIN_PATIENTS if pat not in BAD_PATIENT_IDS])\n",
    "ALL_TEST_PATIENTS = test_df['Patient'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_averaged_slices(patient_id, folder_path, num_images):\n",
    "    # the preprocessed array with NUM_SLICES elements\n",
    "    # TODO: Handle the case when the NUM_SLICES > the actual total slices\n",
    "    # TODO: resize the image to 256 X 256?\n",
    "\n",
    "    full_path = os.path.join(folder_path, patient_id)\n",
    "    # list of all files in that path and sort them\n",
    "    all_files = os.listdir(full_path)\n",
    "    # sorted using the first number part of the file name\n",
    "    all_files.sort(key = lambda x: int(x.split('.')[0]))\n",
    "\n",
    "    # read all the dicom files for the patient into the slices list\n",
    "    slices = [pydicom.read_file(os.path.join(full_path, s)) for s in all_files]\n",
    "    # sort the slices using their order (file number works too)\n",
    "    # slices.sort(key = lambda x: int(x.ImagePositionPatient[2]))\n",
    "\n",
    "    # final array containing averaged num_images images\n",
    "    out_array = []\n",
    "\n",
    "    # how many extra files while averaging all images into (num_images) images\n",
    "    remainder_array_size = len(slices)%num_images\n",
    "\n",
    "    # how many to average to get a single averaged image\n",
    "    avging_array_size = len(slices)//num_images\n",
    "\n",
    "    # get the first one with the remainder images\n",
    "    first_array = []\n",
    "    # select the first remainder + avg_arrray_size imgaes and average into one\n",
    "    for slice in slices[:remainder_array_size+avging_array_size]:\n",
    "        first_array.append(slice.pixel_array)\n",
    "    first_avged_array = np.average(first_array, axis=0)\n",
    "    first_resized = cv2.resize(first_avged_array, (IMG_SIZE, IMG_SIZE))\n",
    "    out_array.append(first_resized)\n",
    "\n",
    "    # after the first one get the remaining ones into out_array rolling averaging (avging_array_size) at a time.\n",
    "    for i in range(remainder_array_size + avging_array_size, len(slices), avging_array_size):\n",
    "        temp_array = []\n",
    "        for slice in slices[i:i+avging_array_size]:\n",
    "            temp_array.append(slice.pixel_array)\n",
    "        avged_temp_array = np.average(temp_array, axis=0)\n",
    "        avged_resized = cv2.resize(avged_temp_array, (IMG_SIZE, IMG_SIZE))\n",
    "        out_array.append(avged_resized)\n",
    "    \n",
    "    return np.array(out_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_from_id = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the train and test images in array_from_id\n",
    "for id in ALL_TRAIN_PATIENTS:\n",
    "    array_from_id[id] = get_averaged_slices(id, os.path.join(DATA_DIR, \"train\"), NUM_IMAGES)\n",
    "\n",
    "for id in ALL_TEST_PATIENTS:\n",
    "    array_from_id[id] = get_averaged_slices(id, os.path.join(DATA_DIR, \"test\"), NUM_IMAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PulmonaryDataset(Dataset):\n",
    "    def __init__(self, df, FV, test=False):\n",
    "        self.df = df\n",
    "        self.test = test\n",
    "        self.FV = FV\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'imgarray': torch.from_numpy(array_from_id[self.df.iloc[idx]['Patient']]).unsqueeze(0),\n",
    "            'features': torch.tensor(self.df[self.FV].iloc[idx].values),\n",
    "            'target': torch.tensor(self.df['FVC'].iloc[idx])\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PulmonaryModel(nn.Module):\n",
    "    def __init__(self, cnn_output_size=10, in_features=9, out_quantiles=3):\n",
    "        super(PulmonaryModel, self).__init__()\n",
    "\n",
    "        self.conv_layer1 = self._make_conv_layer(1, 8)\n",
    "        self.conv_layer2 = self._make_conv_layer(8, 32)\n",
    "        self.conv_layer3 = self._make_conv_layer(32, 64)\n",
    "        self.conv_layer4 = nn.Conv3d(64, 128, kernel_size=(1, 3, 3))\n",
    "        self.conv_layer5 = nn.Conv3d(128, 128, kernel_size=(1,3,3), padding=0)\n",
    "\n",
    "        self.fc1 = nn.Linear(86528, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, cnn_output_size)\n",
    "        \n",
    "        self.fc4 = nn.Linear(cnn_output_size + in_features, 100)\n",
    "        self.fc5 = nn.Linear(100, out_quantiles)\n",
    "\n",
    "    def _make_conv_layer(self, in_c, out_c):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv3d(in_c, out_c, kernel_size=(2,3,3), padding=0),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv3d(out_c, out_c, kernel_size=(2, 3, 3), padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool3d((2,2,2)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = self.conv_layer1(x)\n",
    "        x = self.conv_layer2(x)\n",
    "        x = self.conv_layer3(x)\n",
    "        x = self.conv_layer4(x)\n",
    "        x = self.conv_layer5(x)\n",
    "\n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # concatenate the in_features\n",
    "        x = torch.cat((x, y), dim=1)\n",
    "        x = self.fc4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_one, clip_two = torch.tensor(70, dtype=torch.float32).to(DEVICE), torch.tensor(1000, dtype=torch.float32).to(DEVICE)\n",
    "def score(y_true, y_pred):\n",
    "    sigma = y_pred[:, 2] - y_pred[:, 0]\n",
    "    fvc_pred = y_pred[:, 1]\n",
    "\n",
    "    sigma_clip = torch.max(sigma, clip_one)\n",
    "    delta = torch.abs(y_true - fvc_pred)\n",
    "    delta = torch.min(delta, clip_two)\n",
    "    sq2 = torch.sqrt(torch.tensor(2, dtype=torch.float32))\n",
    "    metric = (delta / sigma_clip) * sq2 + torch.log(sigma_clip * sq2)\n",
    "    return torch.mean(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_loss(preds, target, quantiles, _lambda):\n",
    "    assert not target.requires_grad\n",
    "    assert preds.size(0) == target.size(0)\n",
    "    losses = []\n",
    "    for i, q in enumerate(quantiles):\n",
    "        errors = target - preds[:, i]\n",
    "        losses.append(torch.max((q - 1) * errors, q * errors).unsqueeze(1))\n",
    "    loss = torch.mean(torch.sum(torch.cat(losses, dim=1), dim=1))\n",
    "    return loss\n",
    "    # return _lambda * loss + (1 - _lambda) * score(target, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_data_loader, optimizer, train_loss):\n",
    "    model.train()\n",
    "    for i, data in enumerate(train_data_loader):\n",
    "        images = data['imgarray']\n",
    "        features = data['features']\n",
    "        targets = data['target']\n",
    "\n",
    "        images = images.to(DEVICE).float()\n",
    "        features = features.to(DEVICE).float()\n",
    "        targets = targets.to(DEVICE).float()\n",
    "\n",
    "        model.zero_grad()\n",
    "        out = model(images, features)\n",
    "        loss = quantile_loss(out, targets, QUANTILES, 0.6)\n",
    "        train_loss.update(loss, features.size(0))\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_one_epoch(model, valid_data_loader, valid_loss, lr_scheduler):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(valid_data_loader):\n",
    "            images = data['imgarray']\n",
    "            features = data['features']\n",
    "            targets = data['target']\n",
    "\n",
    "            images = images.to(DEVICE).float()\n",
    "            features = features.to(DEVICE).float()\n",
    "            targets = targets.to(DEVICE).float()\n",
    "            \n",
    "            out = model(images, features)\n",
    "            loss = quantile_loss(out, targets, QUANTILES, 0.6)\n",
    "            valid_loss.update(loss, features.size(0))\n",
    "    \n",
    "    if lr_scheduler is not None:\n",
    "        lr_scheduler.step(valid_loss.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\ntorch.Size([10, 10])\ntorch.Size([10, 9])\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-cbef050ada7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAverageMeter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0meval_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-2c3051ebc0a2>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, train_data_loader, optimizer, train_loss)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'imgarray'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 974\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    975\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    939\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 941\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    942\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# unserialize the data after having released the lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mqsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/multiprocessing/reductions.py\u001b[0m in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrebuild_storage_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m     \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage_from_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mdetach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;34m'''Get the fd.  This should only be called once.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_resource_sharer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/reduction.py\u001b[0m in \u001b[0;36mrecv_handle\u001b[0;34m(conn)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;34m'''Receive a handle over a local connection.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromfd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileno\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAF_UNIX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSOCK_STREAM\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mrecvfds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mDupFd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/reduction.py\u001b[0m in \u001b[0;36mrecvfds\u001b[0;34m(sock, size)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mbytes_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mancdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maddr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecvmsg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCMSG_SPACE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mancdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for fold, (train_index, test_index) in enumerate(kf.split(ALL_TRAIN_PATIENTS)):\n",
    "    model = PulmonaryModel(10, len(FV))\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    train_ids = ALL_TRAIN_PATIENTS[train_index]\n",
    "    test_ids = ALL_TRAIN_PATIENTS[test_index]\n",
    "\n",
    "    df_train = combined_df[combined_df['Patient'].isin(train_ids)].reset_index(drop=True)\n",
    "    df_valid = combined_df[combined_df['Patient'].isin(test_ids)].reset_index(drop=True)\n",
    "\n",
    "    train_dataset = PulmonaryDataset(df_train, FV)\n",
    "    valid_dataset = PulmonaryDataset(df_valid, FV)\n",
    "\n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=10,\n",
    "        shuffle=True,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    valid_data_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=4,\n",
    "        shuffle=False,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=50, factor=0.7, verbose=True)\n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    \n",
    "    # tq = tqdm(range(NUM_EPOCHS), desc=f\"Fold {fold}\")\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_loss = AverageMeter()\n",
    "        valid_loss = AverageMeter()\n",
    "\n",
    "        train_one_epoch(model, train_data_loader, optimizer, train_loss)\n",
    "        eval_one_epoch(model, valid_data_loader, valid_loss, lr_scheduler)\n",
    "\n",
    "        if epoch % PRINT_EVERY == 0:\n",
    "            print(f\"Epoch {epoch}/{NUM_EPOCHS}, Loss {train_loss.avg}\")\n",
    "            print(f\"Fold {fold}, Valid Loss {valid_loss.avg} \\n\")\n",
    "        \n",
    "        # tq.set_postfix(val_loss=valid_loss.avg.item())\n",
    "\n",
    "        if valid_loss.avg < best_valid_loss:\n",
    "            best_valid_loss = valid_loss.avg\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, os.path.join(CONFIG.CFG.DATA.MODELS_OUT, f\"model_fold_{fold}.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PulmonaryDataset(train_df, FV, test=False)\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, data in enumerate(train_data_loader):\n",
    "    imgarray = data['imgarray'].to(DEVICE).float()\n",
    "    out = model(imgarray)\n",
    "    print(out)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for key in array_from_id:\n",
    "    print(array_from_id[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}