{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "if os.path.abspath(os.pardir) not in sys.path:\n",
    "    sys.path.insert(0, os.path.abspath(os.pardir))\n",
    "import CONFIG\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import random\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<torch._C.Generator at 0x7f53902475a0>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = CONFIG.CFG.DATA.BASE\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Patient</th>\n      <th>Weeks</th>\n      <th>FVC</th>\n      <th>Percent</th>\n      <th>Age</th>\n      <th>Sex</th>\n      <th>SmokingStatus</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ID00007637202177411956430</td>\n      <td>-4</td>\n      <td>2315</td>\n      <td>58.253649</td>\n      <td>79</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ID00007637202177411956430</td>\n      <td>5</td>\n      <td>2214</td>\n      <td>55.712129</td>\n      <td>79</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ID00007637202177411956430</td>\n      <td>7</td>\n      <td>2061</td>\n      <td>51.862104</td>\n      <td>79</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ID00007637202177411956430</td>\n      <td>9</td>\n      <td>2144</td>\n      <td>53.950679</td>\n      <td>79</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ID00007637202177411956430</td>\n      <td>11</td>\n      <td>2069</td>\n      <td>52.063412</td>\n      <td>79</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                     Patient  Weeks   FVC    Percent  Age   Sex SmokingStatus\n0  ID00007637202177411956430     -4  2315  58.253649   79  Male     Ex-smoker\n1  ID00007637202177411956430      5  2214  55.712129   79  Male     Ex-smoker\n2  ID00007637202177411956430      7  2061  51.862104   79  Male     Ex-smoker\n3  ID00007637202177411956430      9  2144  53.950679   79  Male     Ex-smoker\n4  ID00007637202177411956430     11  2069  52.063412   79  Male     Ex-smoker"
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "train_df = pd.read_csv(os.path.join(ROOT, \"train.csv\"))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_input = train_df[['Weeks', 'FVC', 'Age']].values\n",
    "lstm_input_scaled = torch.tensor(min_max_scaler.fit_transform(lstm_input)).float()\n",
    "train_df[['Weeks', 'FVC', 'Age']] = lstm_input_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Patient</th>\n      <th>Weeks</th>\n      <th>FVC</th>\n      <th>Percent</th>\n      <th>Age</th>\n      <th>Sex</th>\n      <th>SmokingStatus</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ID00007637202177411956430</td>\n      <td>0.007246</td>\n      <td>0.267050</td>\n      <td>58.253649</td>\n      <td>0.769231</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ID00007637202177411956430</td>\n      <td>0.072464</td>\n      <td>0.248923</td>\n      <td>55.712129</td>\n      <td>0.769231</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ID00007637202177411956430</td>\n      <td>0.086957</td>\n      <td>0.221464</td>\n      <td>51.862104</td>\n      <td>0.769231</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ID00007637202177411956430</td>\n      <td>0.101449</td>\n      <td>0.236360</td>\n      <td>53.950679</td>\n      <td>0.769231</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ID00007637202177411956430</td>\n      <td>0.115942</td>\n      <td>0.222900</td>\n      <td>52.063412</td>\n      <td>0.769231</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                     Patient     Weeks       FVC  ...       Age   Sex SmokingStatus\n0  ID00007637202177411956430  0.007246  0.267050  ...  0.769231  Male     Ex-smoker\n1  ID00007637202177411956430  0.072464  0.248923  ...  0.769231  Male     Ex-smoker\n2  ID00007637202177411956430  0.086957  0.221464  ...  0.769231  Male     Ex-smoker\n3  ID00007637202177411956430  0.101449  0.236360  ...  0.769231  Male     Ex-smoker\n4  ID00007637202177411956430  0.115942  0.222900  ...  0.769231  Male     Ex-smoker\n\n[5 rows x 7 columns]"
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OSICLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(OSICLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.lc = nn.Linear(hidden_size, 50)\n",
    "        self.lc2 = nn.Linear(50, output_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        out, _ = self.lstm(X)\n",
    "        out = self.lc(out)\n",
    "        out = self.lc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OSICLSTM(3, 100, 1, 3)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_PATIENTS = train_df[\"Patient\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "OSICLSTM(\n  (lstm): LSTM(3, 100, batch_first=True)\n  (lc): Linear(in_features=100, out_features=50, bias=True)\n  (lc2): Linear(in_features=50, out_features=3, bias=True)\n)"
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[[ 0.0341, -0.0711, -0.0313]]], grad_fn=<AddBackward0>)\n"
    }
   ],
   "source": [
    "for patient in ALL_PATIENTS:\n",
    "    patient_data = train_df[train_df[\"Patient\"] == patient][['Weeks', 'FVC', 'Age']].values\n",
    "    patient_data = torch.tensor(patient_data).float()\n",
    "\n",
    "    for data in patient_data:\n",
    "        out = model(data.view(1,1,-1))\n",
    "        print(out)\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 0, loss=13.032611177600065\nEpoch 1, loss=1.0277080979115134\nEpoch 2, loss=0.8733080312089947\nEpoch 3, loss=1.1937330269760658\nEpoch 4, loss=1.2651335714726486\nEpoch 5, loss=1.083214549108812\nEpoch 6, loss=1.1848274484831207\nEpoch 7, loss=1.0241240544296444\nEpoch 8, loss=0.9775634313525636\nEpoch 9, loss=1.1894017512223094\nEpoch 10, loss=1.5839617902624616\nEpoch 11, loss=0.9990007880627347\nEpoch 12, loss=0.9857747280800395\nEpoch 13, loss=0.9891022588691287\nEpoch 14, loss=1.008210702130991\nEpoch 15, loss=1.0206364006589859\nEpoch 16, loss=0.999121806293107\nEpoch 17, loss=0.9548726909931926\nEpoch 18, loss=0.9058535892814024\nEpoch 19, loss=0.8819846086518507\nEpoch 20, loss=0.8728879955248323\nEpoch 21, loss=0.8556869997496632\nEpoch 22, loss=0.8365606623328133\nEpoch 23, loss=0.8276396507607051\nEpoch 24, loss=0.8248577994488526\nEpoch 25, loss=0.8023507359456441\nEpoch 26, loss=0.8166683995779025\nEpoch 27, loss=0.8938315016370992\nEpoch 28, loss=0.9268804073472645\nEpoch 29, loss=0.8174480644516505\nEpoch 30, loss=0.7827898213701389\nEpoch 31, loss=0.7808330986536519\nEpoch 32, loss=0.7376220851979269\nEpoch 33, loss=0.8453168411432755\nEpoch 34, loss=0.768013284691592\nEpoch 35, loss=1.1875028702037123\nEpoch 36, loss=0.8613707389415604\nEpoch 37, loss=0.7728884389524399\nEpoch 38, loss=0.7043932498494825\nEpoch 39, loss=0.6963653061661325\nEpoch 40, loss=0.7035558894518775\nEpoch 41, loss=0.6765416843069036\nEpoch 42, loss=0.7867502425714302\nEpoch 43, loss=0.8653570789064227\nEpoch 44, loss=0.9882375725254688\nEpoch 45, loss=1.9489376803049538\nEpoch 46, loss=0.6570530788827863\nEpoch 47, loss=0.6335873745279481\nEpoch 48, loss=0.6325624735910864\nEpoch 49, loss=0.6314596853055172\nEpoch 50, loss=0.6319251218275288\nEpoch 51, loss=0.6392101544306464\nEpoch 52, loss=0.654700920484743\nEpoch 53, loss=0.6774698782113341\nEpoch 54, loss=0.913070600110581\nEpoch 55, loss=0.662614586136852\nEpoch 56, loss=0.6831456116518666\nEpoch 57, loss=0.6548852521677802\nEpoch 58, loss=1.312438998269734\nEpoch 59, loss=1.21078357689436\nEpoch 60, loss=0.6645939505796837\nEpoch 61, loss=0.6320822969364711\nEpoch 62, loss=0.6341394229299943\nEpoch 63, loss=0.6397590303302452\nEpoch 64, loss=0.6454230375468166\nEpoch 65, loss=0.7126234581313781\nEpoch 66, loss=0.7039071799136836\nEpoch 67, loss=1.1534770352205723\nEpoch 68, loss=0.8189067506362454\nEpoch 69, loss=0.6478731377311044\nEpoch 70, loss=0.6344227504933565\nEpoch 71, loss=0.650944220975881\nEpoch 72, loss=0.6294134667588749\nEpoch 73, loss=0.6857140694563307\nEpoch 74, loss=0.7345226847704442\nEpoch 75, loss=1.2348068385262057\nEpoch 76, loss=0.6896110293283227\nEpoch 77, loss=0.6730255814002573\nEpoch 78, loss=0.651092521728615\nEpoch 79, loss=0.6503629460528126\nEpoch 80, loss=0.6635677764510577\nEpoch 81, loss=0.6615849061592657\nEpoch 82, loss=0.6524838433067943\nEpoch 83, loss=0.6407464732784692\nEpoch 84, loss=0.6433799547036216\nEpoch 85, loss=0.6740503812023007\nEpoch 86, loss=0.6692216392391335\nEpoch 87, loss=0.7835345353287669\nEpoch 88, loss=1.1109036663463772\nEpoch 89, loss=0.6751544805422393\nEpoch 90, loss=0.6432388392567794\nEpoch 91, loss=0.6580834654192841\nEpoch 92, loss=0.6890796696987087\nEpoch 93, loss=0.7025361909444097\nEpoch 94, loss=0.6916747175355928\nEpoch 95, loss=0.6854228111140633\nEpoch 96, loss=0.6324670476931603\nEpoch 97, loss=0.6556729541548276\nEpoch 98, loss=0.6407525091035606\nEpoch 99, loss=0.6255956393131276\nEpoch 100, loss=0.6758585730583683\nEpoch 101, loss=0.7934775187780394\nEpoch 102, loss=1.0710582684954664\nEpoch 103, loss=0.6600809128651669\nEpoch 104, loss=0.6777168259515979\nEpoch 105, loss=0.6446363329901303\nEpoch 106, loss=0.6383141177442115\nEpoch 107, loss=0.6521751898598079\nEpoch 108, loss=0.6764725598173997\nEpoch 109, loss=0.6716883292230967\nEpoch 110, loss=0.650613595697094\nEpoch 111, loss=0.6200052212701909\nEpoch 112, loss=0.6123239804059614\nEpoch 113, loss=0.6136732607908796\nEpoch 114, loss=0.6205481643416708\nEpoch 115, loss=0.9289729679229681\nEpoch 116, loss=1.0812198161558872\nEpoch 117, loss=0.6351576612538687\nEpoch 118, loss=0.607416524642296\nEpoch 119, loss=0.6125215416845036\nEpoch 120, loss=0.6158104115169231\nEpoch 121, loss=0.6167677400321925\nEpoch 122, loss=0.6199884839907326\nEpoch 123, loss=0.6232830342963315\nEpoch 124, loss=0.6209162820756576\nEpoch 125, loss=0.6220228775740376\nEpoch 126, loss=0.6850711949202021\nEpoch 127, loss=0.7393679755808101\nEpoch 128, loss=0.6497897403287823\nEpoch 129, loss=0.6143620332832402\nEpoch 130, loss=0.6235952757367448\nEpoch 131, loss=0.5989725874771418\nEpoch 132, loss=0.6113742904038117\nEpoch 133, loss=1.0000759921042124\nEpoch 134, loss=0.8467785898017397\nEpoch 135, loss=0.6351947771573068\nEpoch 136, loss=0.5977051740914954\nEpoch 137, loss=0.6009823086075047\nEpoch 138, loss=0.6182677014754461\nEpoch 139, loss=0.6293899939492272\nEpoch 140, loss=0.6054976977111061\nEpoch 141, loss=0.63648118671376\nEpoch 142, loss=0.7024443591281375\nEpoch 143, loss=0.6185026122724555\nEpoch 144, loss=0.6188601365699943\nEpoch 145, loss=0.5873011705542677\nEpoch 146, loss=0.5844842328680299\nEpoch 147, loss=0.5928742809008536\nEpoch 148, loss=0.7189879149865187\nEpoch 149, loss=0.7471006309468059\nEpoch 150, loss=0.6252007832240349\nEpoch 151, loss=0.6070487811030294\nEpoch 152, loss=0.6689250761832207\nEpoch 153, loss=0.6416847885435332\nEpoch 154, loss=0.6723899742107302\nEpoch 155, loss=0.7167855314822926\nEpoch 156, loss=0.6064698564212524\nEpoch 157, loss=0.6412705935721255\nEpoch 158, loss=0.6558029183479797\nEpoch 159, loss=0.6283674167428749\nEpoch 160, loss=0.5906147276532772\nEpoch 161, loss=0.5907772090482276\nEpoch 162, loss=0.6357289947640732\nEpoch 163, loss=1.4037573023741234\nEpoch 164, loss=0.9730311582194564\nEpoch 165, loss=0.6069827537747389\nEpoch 166, loss=0.5771481482641025\nEpoch 167, loss=0.5746652792390342\nEpoch 168, loss=0.5743226646974291\nEpoch 169, loss=0.5738336839532331\nEpoch 170, loss=0.5735810531497195\nEpoch 171, loss=0.5781769204427132\nEpoch 172, loss=0.5950582600587673\nEpoch 173, loss=0.6282071290691147\nEpoch 174, loss=0.6245764029015002\nEpoch 175, loss=0.6211567188374278\nEpoch 176, loss=0.6091947711649526\nEpoch 177, loss=0.6421518321974854\nEpoch 178, loss=0.6572981468706139\nEpoch 179, loss=0.5889101305273223\nEpoch 180, loss=0.6492120135050141\nEpoch 181, loss=0.6088039691518075\nEpoch 182, loss=0.6059860769744644\nEpoch 183, loss=0.6034781983487383\nEpoch 184, loss=0.707440888696878\nEpoch 185, loss=0.6329171552910946\nEpoch 186, loss=0.5837271038848629\nEpoch 187, loss=0.5907627410322297\nEpoch 188, loss=0.7986381235817656\nEpoch 189, loss=1.015613586178768\nEpoch 190, loss=0.6399976972652542\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-4f515d824fdc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m#     out = model(data.view(1, 1, -1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m#     print(out)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpatient_loss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mpatient_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    total_loss = 0\n",
    "    for patient in ALL_PATIENTS:\n",
    "        patient_loss = 0\n",
    "        model.zero_grad()\n",
    "\n",
    "        patient_data = train_df[train_df[\"Patient\"] == patient][['Weeks', 'FVC', 'Age']].values\n",
    "        patient_data = torch.tensor(patient_data).float()\n",
    "\n",
    "        loss = 0\n",
    "        for i in range(patient_data.size()[0] - 1):\n",
    "            out = model(patient_data[i].view(1,1,-1))\n",
    "            loss += loss_function(out.view(3), patient_data[i+1])\n",
    "            patient_loss += loss.item()\n",
    "        # for data in patient_data:\n",
    "        #     out = model(data.view(1, 1, -1))\n",
    "        #     print(out)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += patient_loss/patient_data.shape[0]\n",
    "    print(f\"Epoch {epoch}, loss={total_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([-12])"
     },
     "metadata": {},
     "execution_count": 96
    }
   ],
   "source": [
    "sample_u = torch.tensor([[-12, 3020, 73]])\n",
    "sample = torch.tensor(min_max_scaler.transform(sample_u)).float()\n",
    "sample_u[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[ -14.31420559 2945.78741264   73.12457055]]\n[[ -12.         2945.78741264   73.        ]] \n\n[[ -14.38764644 2874.82694507   73.13517064]]\n[[ -11.         2874.82694507   73.        ]] \n\n[[ -13.16220379 2808.05978775   73.1353845 ]]\n[[ -10.         2808.05978775   73.        ]] \n\n[[ -11.92862207 2744.85890365   73.13483357]]\n[[  -9.         2744.85890365   73.        ]] \n\n[[ -10.68775672 2684.68925214   73.13386887]]\n[[  -8.         2684.68925214   73.        ]] \n\n[[  -9.44036448 2627.08802807   73.13275075]]\n[[  -7.         2627.08802807   73.        ]] \n\n[[  -8.1870088  2571.65270555   73.13167447]]\n[[  -6.         2571.65270555   73.        ]] \n\n[[  -6.92818731 2518.03074229   73.13079345]]\n[[  -5.         2518.03074229   73.        ]] \n\n[[  -5.66428661 2465.91077852   73.13021231]]\n[[  -4.         2465.91077852   73.        ]] \n\n[[  -4.39559871 2415.01715708   73.13001239]]\n[[  -3.         2415.01715708   73.        ]] \n\n[[  -3.12236214 2365.10494161   73.13024718]]\n[[-2.00000000e+00  2.36510494e+03  7.30000000e+01]] \n\n[[-1.84472084e+00  2.31595676e+03  7.31309469e+01]]\n[[-1.00000000e+00  2.31595676e+03  7.30000000e+01]] \n\n[[-5.62798202e-01  2.26737999e+03  7.31321324e+01]]\n[[   0.         2267.37998879   73.        ]] \n\n[[7.23339975e-01 2.21920458e+03 7.31338038e+01]]\n[[1.00000000e+00 2.21920458e+03 7.30000000e+01]] \n\n[[2.01366079e+00 2.17128224e+03 7.31359563e+01]]\n[[2.00000000e+00 2.17128224e+03 7.30000000e+01]] \n\n[[   3.30812722 2123.48528183   73.13857847]]\n[[   3.         2123.48528183   73.        ]] \n\n[[   4.60675985 2075.70567316   73.14164692]]\n[[   4.         2075.70567316   73.        ]] \n\n[[   5.90952575 2027.85416114   73.14513844]]\n[[   5.         2027.85416114   73.        ]] \n\n[[   7.21644756 1979.86116725   73.14902514]]\n[[   6.         1979.86116725   73.        ]] \n\n[[   8.52749237 1931.67446607   73.15327913]]\n[[   7.         1931.67446607   73.        ]] \n\n[[   9.84264579 1883.25993258   73.15786552]]\n[[   8.         1883.25993258   73.        ]] \n\n[[  11.16186669 1834.59963244   73.16275412]]\n[[   9.         1834.59963244   73.        ]] \n\n[[  12.48511395 1785.69115782   73.16791469]]\n[[  10.         1785.69115782   73.        ]] \n\n[[  13.81234232 1736.54604977   73.17331469]]\n[[  11.         1736.54604977   73.        ]] \n\n[[  15.14349011 1687.18805468   73.17892623]]\n[[  12.         1687.18805468   73.        ]] \n\n[[  16.47851825 1637.65063334   73.18471444]]\n[[  13.         1637.65063334   73.        ]] \n\n[[  17.8173959  1587.97413796   73.19065142]]\n[[  14.         1587.97413796   73.        ]] \n\n[[  19.16011688 1538.20365345   73.19671392]]\n[[  15.         1538.20365345   73.        ]] \n\n[[  20.50671616 1488.38501197   73.20286942]]\n[[  16.         1488.38501197   73.        ]] \n\n[[  21.85728627 1438.56172085   73.20909697]]\n[[  17.         1438.56172085   73.        ]] \n\n[[  23.21196294 1388.77189052   73.21537334]]\n[[  18.         1388.77189052   73.        ]] \n\n[[  24.5709703  1339.04316974   73.22167993]]\n[[  19.         1339.04316974   73.        ]] \n\n[[  25.93462914 1289.39050376   73.22800046]]\n[[  20.         1289.39050376   73.        ]] \n\n[[  27.30337954 1239.8118999    73.23432797]]\n[[  21.        1239.8118999   73.       ]] \n\n[[  28.67778698 1190.28410995   73.24066246]]\n[[  22.         1190.28410995   73.        ]] \n\n[[  30.0586226  1140.75864482   73.24701554]]\n[[  23.         1140.75864482   73.        ]] \n\n[[  31.44687963 1091.15695882   73.25341046]]\n[[  24.         1091.15695882   73.        ]] \n\n[[  32.84386384 1041.36513579   73.25988907]]\n[[  25.         1041.36513579   73.        ]] \n\n[[ 34.25129437 991.2270807   73.26651877]]\n[[ 26.        991.2270807  73.       ]] \n\n[[ 35.6714291  940.53522038  73.27338791]]\n[[ 27.         940.53522038  73.        ]] \n\n[[ 37.10729086 889.0185473   73.2806313 ]]\n[[ 28.        889.0185473  73.       ]] \n\n[[ 38.56292248 836.32601368  73.28842795]]\n[[ 29.         836.32601368  73.        ]] \n\n[[ 40.04384321 782.00344944  73.29703122]]\n[[ 30.         782.00344944  73.        ]] \n\n[[ 41.55768627 725.45885587  73.30678284]]\n[[ 31.         725.45885587  73.        ]] \n\n[[ 43.11518997 665.90993118  73.31816864]]\n[[ 32.         665.90993118  73.        ]] \n\n[[ 44.73181403 602.30635512  73.33187205]]\n[[ 33.         602.30635512  73.        ]] \n\n[[ 46.43048686 533.20541108  73.3488996 ]]\n[[ 34.         533.20541108  73.        ]] \n\n[[ 48.24635988 456.5654093   73.37075996]]\n[[ 35.        456.5654093  73.       ]] \n\n[[ 50.23570335 369.39663184  73.39983815]]\n[[ 36.         369.39663184  73.        ]] \n\n[[ 52.49344707 267.12150896  73.44006038]]\n[[ 37.         267.12150896  73.        ]] \n\n[[ 55.19056094 142.33980787  73.4983725 ]]\n[[ 38.         142.33980787  73.        ]] \n\n[[ 58.6614297  -17.77765608  73.58801532]]\n[[ 39.         -17.77765608  73.        ]] \n\n[[  63.63240504 -237.60960507   73.73701167]]\n[[  40.         -237.60960507   73.        ]] \n\n[[  71.90776622 -569.67553973   74.01271862]]\n[[  41.         -569.67553973   73.        ]] \n\n[[   88.75995922 -1149.41147709    74.60277402]]\n[[   42.         -1149.41147709    73.        ]] \n\n[[  131.60629773 -2434.52623248    76.05585808]]\n[[   43.         -2434.52623248    73.        ]] \n\n[[  234.56215191 -6710.49943829    78.5584231 ]]\n[[   44.         -6710.49943829    73.        ]] \n\n[[   315.43356895 -31391.23960304     72.23388636]]\n[[    45.         -31391.23960304     73.        ]] \n\n[[-9.60362689e+01 -2.11149145e+05 -1.00504699e+03]]\n[[ 4.60000000e+01 -2.11149145e+05  7.30000000e+01]] \n\n[[ 1.92733349e+03 -2.18720946e+05 -1.20893650e+02]]\n[[ 4.70000000e+01 -2.18720946e+05  7.30000000e+01]] \n\n[[ 1.93890044e+03 -2.19039545e+05 -1.19118209e+02]]\n[[ 4.80000000e+01 -2.19039545e+05  7.30000000e+01]] \n\n[[ 1.93828333e+03 -2.19026452e+05 -1.19220825e+02]]\n[[ 4.90000000e+01 -2.19026452e+05  7.30000000e+01]] \n\n[[ 1.93710821e+03 -2.18997969e+05 -1.19410809e+02]]\n[[ 5.00000000e+01 -2.18997969e+05  7.30000000e+01]] \n\n[[ 1.93590861e+03 -2.18968892e+05 -1.19603470e+02]]\n[[ 5.10000000e+01 -2.18968892e+05  7.30000000e+01]] \n\n[[ 1.93470915e+03 -2.18939878e+05 -1.19795071e+02]]\n[[ 5.20000000e+01 -2.18939878e+05  7.30000000e+01]] \n\n[[ 1.93351126e+03 -2.18910992e+05 -1.19985371e+02]]\n[[ 5.30000000e+01 -2.18910992e+05  7.30000000e+01]] \n\n[[ 1.93231535e+03 -2.18882212e+05 -1.20174182e+02]]\n[[ 5.40000000e+01 -2.18882212e+05  7.30000000e+01]] \n\n[[ 1.93112220e+03 -2.18853602e+05 -1.20361637e+02]]\n[[ 5.50000000e+01 -2.18853602e+05  7.30000000e+01]] \n\n[[ 1.92993128e+03 -2.18825098e+05 -1.20547808e+02]]\n[[ 5.60000000e+01 -2.18825098e+05  7.30000000e+01]] \n\n[[ 1.92874274e+03 -2.18796765e+05 -1.20732510e+02]]\n[[ 5.70000000e+01 -2.18796765e+05  7.30000000e+01]] \n\n[[ 1.92755775e+03 -2.18768516e+05 -1.20915668e+02]]\n[[ 5.80000000e+01 -2.18768516e+05  7.30000000e+01]] \n\n[[ 1.92637605e+03 -2.18740480e+05 -1.21097636e+02]]\n[[ 5.9000000e+01 -2.1874048e+05  7.3000000e+01]] \n\n[[ 1.92519764e+03 -2.18712593e+05 -1.21278098e+02]]\n[[ 6.00000000e+01 -2.18712593e+05  7.30000000e+01]] \n\n[[ 1.92402305e+03 -2.18684812e+05 -1.21457072e+02]]\n[[ 6.10000000e+01 -2.18684812e+05  7.30000000e+01]] \n\n[[ 1.92285267e+03 -2.18657265e+05 -1.21634614e+02]]\n[[ 6.20000000e+01 -2.18657265e+05  7.30000000e+01]] \n\n[[ 1.92168676e+03 -2.18629867e+05 -1.21810688e+02]]\n[[ 6.30000000e+01 -2.18629867e+05  7.30000000e+01]] \n\n[[ 1.92052533e+03 -2.18602638e+05 -1.21985329e+02]]\n[[ 6.40000000e+01 -2.18602638e+05  7.30000000e+01]] \n\n[[ 1.91936824e+03 -2.18575601e+05 -1.22158556e+02]]\n[[ 6.50000000e+01 -2.18575601e+05  7.30000000e+01]] \n\n[[ 1.91821668e+03 -2.18548798e+05 -1.22330297e+02]]\n[[ 6.60000000e+01 -2.18548798e+05  7.30000000e+01]] \n\n[[ 1.91707025e+03 -2.18522122e+05 -1.22500493e+02]]\n[[ 6.70000000e+01 -2.18522122e+05  7.30000000e+01]] \n\n[[ 1.91592974e+03 -2.18495702e+05 -1.22669313e+02]]\n[[ 6.80000000e+01 -2.18495702e+05  7.30000000e+01]] \n\n[[ 1.91479450e+03 -2.18469451e+05 -1.22836386e+02]]\n[[ 6.90000000e+01 -2.18469451e+05  7.30000000e+01]] \n\n[[ 1.91366584e+03 -2.18443435e+05 -1.23002379e+02]]\n[[ 7.00000000e+01 -2.18443435e+05  7.30000000e+01]] \n\n[[ 1.91254323e+03 -2.18417609e+05 -1.23166551e+02]]\n[[ 7.10000000e+01 -2.18417609e+05  7.30000000e+01]] \n\n[[ 1.91142694e+03 -2.18392039e+05 -1.23329346e+02]]\n[[ 7.20000000e+01 -2.18392039e+05  7.30000000e+01]] \n\n[[ 1.91031788e+03 -2.18366681e+05 -1.23490504e+02]]\n[[ 7.30000000e+01 -2.18366681e+05  7.30000000e+01]] \n\n[[ 1.90921541e+03 -2.18341536e+05 -1.23650342e+02]]\n[[ 7.40000000e+01 -2.18341536e+05  7.30000000e+01]] \n\n[[ 1.90812018e+03 -2.18316603e+05 -1.23808656e+02]]\n[[ 7.50000000e+01 -2.18316603e+05  7.30000000e+01]] \n\n[[ 1.90703231e+03 -2.18291926e+05 -1.23965351e+02]]\n[[ 7.60000000e+01 -2.18291926e+05  7.30000000e+01]] \n\n[[ 1.90595261e+03 -2.18267503e+05 -1.24120670e+02]]\n[[ 7.70000000e+01 -2.18267503e+05  7.30000000e+01]] \n\n[[ 1.90488054e+03 -2.18243314e+05 -1.24274427e+02]]\n[[ 7.80000000e+01 -2.18243314e+05  7.30000000e+01]] \n\n[[ 1.90381636e+03 -2.18219402e+05 -1.24426715e+02]]\n[[ 7.90000000e+01 -2.18219402e+05  7.30000000e+01]] \n\n[[ 1.90276048e+03 -2.18195681e+05 -1.24577348e+02]]\n[[ 8.00000000e+01 -2.18195681e+05  7.30000000e+01]] \n\n[[ 1.90171328e+03 -2.18172278e+05 -1.24726642e+02]]\n[[ 8.10000000e+01 -2.18172278e+05  7.30000000e+01]] \n\n[[ 1.90067438e+03 -2.18149131e+05 -1.24874448e+02]]\n[[ 8.20000000e+01 -2.18149131e+05  7.30000000e+01]] \n\n[[ 1.89964468e+03 -2.18126196e+05 -1.25020674e+02]]\n[[ 8.30000000e+01 -2.18126196e+05  7.30000000e+01]] \n\n[[ 1.89862407e+03 -2.18103559e+05 -1.25165393e+02]]\n[[ 8.40000000e+01 -2.18103559e+05  7.30000000e+01]] \n\n[[ 1.89761267e+03 -2.18081177e+05 -1.25308643e+02]]\n[[ 8.50000000e+01 -2.18081177e+05  7.30000000e+01]] \n\n[[ 1.89661022e+03 -2.18059114e+05 -1.25450387e+02]]\n[[ 8.60000000e+01 -2.18059114e+05  7.30000000e+01]] \n\n[[ 1.89561777e+03 -2.18037285e+05 -1.25590642e+02]]\n[[ 8.70000000e+01 -2.18037285e+05  7.30000000e+01]] \n\n[[ 1.89463519e+03 -2.18015795e+05 -1.25729504e+02]]\n[[ 8.80000000e+01 -2.18015795e+05  7.30000000e+01]] \n\n[[ 1.89366222e+03 -2.17994561e+05 -1.25866914e+02]]\n[[ 8.90000000e+01 -2.17994561e+05  7.30000000e+01]] \n\n[[ 1.89269925e+03 -2.17973561e+05 -1.26002726e+02]]\n[[ 9.00000000e+01 -2.17973561e+05  7.30000000e+01]] \n\n[[ 1.89174642e+03 -2.17952879e+05 -1.26137217e+02]]\n[[ 9.10000000e+01 -2.17952879e+05  7.30000000e+01]] \n\n[[ 1.89080385e+03 -2.17932495e+05 -1.26270220e+02]]\n[[ 9.20000000e+01 -2.17932495e+05  7.30000000e+01]] \n\n[[ 1.88987220e+03 -2.17912366e+05 -1.26401735e+02]]\n[[ 9.30000000e+01 -2.17912366e+05  7.30000000e+01]] \n\n[[ 1.88895043e+03 -2.17892599e+05 -1.26531800e+02]]\n[[ 9.40000000e+01 -2.17892599e+05  7.30000000e+01]] \n\n[[ 1.88803971e+03 -2.17873086e+05 -1.26660470e+02]]\n[[ 9.50000000e+01 -2.17873086e+05  7.30000000e+01]] \n\n[[ 1.88713978e+03 -2.17853892e+05 -1.26787709e+02]]\n[[ 9.60000000e+01 -2.17853892e+05  7.30000000e+01]] \n\n[[ 1.88625051e+03 -2.17834996e+05 -1.26913589e+02]]\n[[ 9.70000000e+01 -2.17834996e+05  7.30000000e+01]] \n\n[[ 1.88537243e+03 -2.17816355e+05 -1.27037945e+02]]\n[[ 9.80000000e+01 -2.17816355e+05  7.30000000e+01]] \n\n[[ 1.88450540e+03 -2.17798033e+05 -1.27161036e+02]]\n[[ 9.90000000e+01 -2.17798033e+05  7.30000000e+01]] \n\n[[ 1.88364943e+03 -2.17780029e+05 -1.27282696e+02]]\n[[ 1.00000000e+02 -2.17780029e+05  7.30000000e+01]] \n\n[[ 1.88280477e+03 -2.17762345e+05 -1.27402867e+02]]\n[[ 1.01000000e+02 -2.17762345e+05  7.30000000e+01]] \n\n[[ 1.88197104e+03 -2.17744937e+05 -1.27522053e+02]]\n[[ 1.02000000e+02 -2.17744937e+05  7.30000000e+01]] \n\n[[ 1.88114915e+03 -2.17727826e+05 -1.27639472e+02]]\n[[ 1.03000000e+02 -2.17727826e+05  7.30000000e+01]] \n\n[[ 1.88033819e+03 -2.17711013e+05 -1.27755776e+02]]\n[[ 1.04000000e+02 -2.17711013e+05  7.30000000e+01]] \n\n[[ 1.87953934e+03 -2.17694519e+05 -1.27870796e+02]]\n[[ 1.05000000e+02 -2.17694519e+05  7.30000000e+01]] \n\n[[ 1.87875154e+03 -2.17678364e+05 -1.27984459e+02]]\n[[ 1.06000000e+02 -2.17678364e+05  7.30000000e+01]] \n\n[[ 1.87797532e+03 -2.17662465e+05 -1.28096671e+02]]\n[[ 1.07000000e+02 -2.17662465e+05  7.30000000e+01]] \n\n"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for i in range(120):\n",
    "        out = model(sample.view(1,1,-1))\n",
    "        out = min_max_scaler.inverse_transform(out.squeeze(dim=0))\n",
    "        print(out)\n",
    "        prev_zero = sample_u[:,0]\n",
    "        prev_three = sample_u[:,2]\n",
    "        out[:, 0] = prev_zero + i\n",
    "        out[:, 2] = prev_three\n",
    "        print(out, \"\\n\")\n",
    "        sample = torch.tensor(min_max_scaler.transform(out)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(sample)\n",
    "    out = model(sample.view(1, 1, -1))\n",
    "    print(out)\n",
    "    sample = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    input = torch.randn(2, 5, 4)\n",
    "    print(input)\n",
    "    print()\n",
    "    out = model(input)\n",
    "    print(out)\n",
    "    print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}