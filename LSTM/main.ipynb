{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "if os.path.abspath(os.pardir) not in sys.path:\n",
    "    sys.path.insert(0, os.path.abspath(os.pardir))\n",
    "import CONFIG\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import random\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = CONFIG.CFG.DATA.BASE\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Patient</th>\n      <th>Weeks</th>\n      <th>FVC</th>\n      <th>Percent</th>\n      <th>Age</th>\n      <th>Sex</th>\n      <th>SmokingStatus</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ID00007637202177411956430</td>\n      <td>-4</td>\n      <td>2315</td>\n      <td>58.253649</td>\n      <td>79</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ID00007637202177411956430</td>\n      <td>5</td>\n      <td>2214</td>\n      <td>55.712129</td>\n      <td>79</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ID00007637202177411956430</td>\n      <td>7</td>\n      <td>2061</td>\n      <td>51.862104</td>\n      <td>79</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ID00007637202177411956430</td>\n      <td>9</td>\n      <td>2144</td>\n      <td>53.950679</td>\n      <td>79</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ID00007637202177411956430</td>\n      <td>11</td>\n      <td>2069</td>\n      <td>52.063412</td>\n      <td>79</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                     Patient  Weeks   FVC    Percent  Age   Sex SmokingStatus\n0  ID00007637202177411956430     -4  2315  58.253649   79  Male     Ex-smoker\n1  ID00007637202177411956430      5  2214  55.712129   79  Male     Ex-smoker\n2  ID00007637202177411956430      7  2061  51.862104   79  Male     Ex-smoker\n3  ID00007637202177411956430      9  2144  53.950679   79  Male     Ex-smoker\n4  ID00007637202177411956430     11  2069  52.063412   79  Male     Ex-smoker"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "train_df = pd.read_csv(os.path.join(ROOT, \"train.csv\"))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_input = train_df[['Weeks', 'FVC', 'Percent', 'Age']].values\n",
    "lstm_input_scaled = torch.tensor(min_max_scaler.fit_transform(lstm_input)).float()\n",
    "train_df[['Weeks', 'FVC', 'Percent', 'Age']] = lstm_input_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Patient</th>\n      <th>Weeks</th>\n      <th>FVC</th>\n      <th>Percent</th>\n      <th>Age</th>\n      <th>Sex</th>\n      <th>SmokingStatus</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ID00007637202177411956430</td>\n      <td>0.007246</td>\n      <td>0.267050</td>\n      <td>0.236393</td>\n      <td>0.769231</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ID00007637202177411956430</td>\n      <td>0.072464</td>\n      <td>0.248923</td>\n      <td>0.215941</td>\n      <td>0.769231</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ID00007637202177411956430</td>\n      <td>0.086957</td>\n      <td>0.221464</td>\n      <td>0.184960</td>\n      <td>0.769231</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ID00007637202177411956430</td>\n      <td>0.101449</td>\n      <td>0.236360</td>\n      <td>0.201767</td>\n      <td>0.769231</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ID00007637202177411956430</td>\n      <td>0.115942</td>\n      <td>0.222900</td>\n      <td>0.186580</td>\n      <td>0.769231</td>\n      <td>Male</td>\n      <td>Ex-smoker</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                     Patient     Weeks       FVC  ...       Age   Sex SmokingStatus\n0  ID00007637202177411956430  0.007246  0.267050  ...  0.769231  Male     Ex-smoker\n1  ID00007637202177411956430  0.072464  0.248923  ...  0.769231  Male     Ex-smoker\n2  ID00007637202177411956430  0.086957  0.221464  ...  0.769231  Male     Ex-smoker\n3  ID00007637202177411956430  0.101449  0.236360  ...  0.769231  Male     Ex-smoker\n4  ID00007637202177411956430  0.115942  0.222900  ...  0.769231  Male     Ex-smoker\n\n[5 rows x 7 columns]"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<torch._C.Generator at 0x7fbeb99e46a8>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OSICLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(OSICLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.lc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        out, _ = self.lstm(X)\n",
    "        out = self.lc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OSICLSTM(4, 100, 1, 4)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "6"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "len(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([400, 4])\ntorch.Size([400, 100])\ntorch.Size([400])\ntorch.Size([400])\ntorch.Size([4, 100])\ntorch.Size([4])\n"
    }
   ],
   "source": [
    "for i in range(len(list(model.parameters()))):\n",
    "    print(list(model.parameters())[i].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "OSICLSTM(\n  (lstm): LSTM(4, 100, batch_first=True)\n  (lc): Linear(in_features=100, out_features=4, bias=True)\n)\n"
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_PATIENTS = train_df[\"Patient\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "OSICLSTM(\n  (lstm): LSTM(4, 100, batch_first=True)\n  (lc): Linear(in_features=100, out_features=4, bias=True)\n)"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[[0.0356, 0.1050, 0.0795, 0.0066]]], grad_fn=<AddBackward0>)\n"
    }
   ],
   "source": [
    "for patient in ALL_PATIENTS:\n",
    "    patient_data = train_df[train_df[\"Patient\"] == patient][['Weeks', 'FVC', 'Percent', 'Age']].values\n",
    "    patient_data = torch.tensor(patient_data).float()\n",
    "\n",
    "    for data in patient_data:\n",
    "        out = model(data.view(1,1,-1))\n",
    "        print(out)\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 0, loss=10.578166382103568\nEpoch 1, loss=1.5046061009539213\nEpoch 2, loss=0.957016112834201\nEpoch 3, loss=0.9199114482127925\nEpoch 4, loss=0.9353469789592571\nEpoch 5, loss=0.9530735072787914\nEpoch 6, loss=0.9601929409023906\nEpoch 7, loss=0.9747644018967867\nEpoch 8, loss=0.9714935864186964\nEpoch 9, loss=0.9686685320556521\nEpoch 10, loss=0.9671902290449976\nEpoch 11, loss=0.9711561967032164\nEpoch 12, loss=0.990335028531835\nEpoch 13, loss=0.9853241236027809\nEpoch 14, loss=0.9256156483030857\nEpoch 15, loss=0.9088940953749489\nEpoch 16, loss=0.9159720649784098\nEpoch 17, loss=0.9150007020322749\nEpoch 18, loss=0.9050621580945494\nEpoch 19, loss=0.9019594548205898\nEpoch 20, loss=0.9044298249824816\nEpoch 21, loss=0.9051997109260853\nEpoch 22, loss=0.9042383937033599\nEpoch 23, loss=0.9044177300667796\nEpoch 24, loss=0.9060268095016895\nEpoch 25, loss=0.9077113932861678\nEpoch 26, loss=0.9083101831431877\nEpoch 27, loss=0.9074147396293282\nEpoch 28, loss=0.9053198047854336\nEpoch 29, loss=0.9026506164278113\nEpoch 30, loss=0.8998795903732899\nEpoch 31, loss=0.8972281949328031\nEpoch 32, loss=0.8948287951217909\nEpoch 33, loss=0.8927815531690478\nEpoch 34, loss=0.89113482104487\nEpoch 35, loss=0.8898962254677542\nEpoch 36, loss=0.8890744242010286\nEpoch 37, loss=0.8887076433161677\nEpoch 38, loss=0.8888491106603839\nEpoch 39, loss=0.8895075700493398\nEpoch 40, loss=0.8905769792644929\nEpoch 41, loss=0.8918031041047096\nEpoch 42, loss=0.8928021937457626\nEpoch 43, loss=0.8931494875805933\nEpoch 44, loss=0.8925054432504225\nEpoch 45, loss=0.8907211142344202\nEpoch 46, loss=0.8878673257766743\nEpoch 47, loss=0.8844707849274458\nEpoch 48, loss=0.8815506821358007\nEpoch 49, loss=0.8795941066029976\nEpoch 50, loss=0.8779805925906643\nEpoch 51, loss=0.8758213353797244\nEpoch 52, loss=0.8728739014422436\nEpoch 53, loss=0.8694815054469279\nEpoch 54, loss=0.8658844700409809\nEpoch 55, loss=0.861831589161202\nEpoch 56, loss=0.8571111326029005\nEpoch 57, loss=0.851904536026548\nEpoch 58, loss=0.8466325727983346\nEpoch 59, loss=0.8418952575144227\nEpoch 60, loss=0.8381384114573589\nEpoch 61, loss=0.8351891027523852\nEpoch 62, loss=0.8327513140168314\nEpoch 63, loss=0.8309218495483991\nEpoch 64, loss=0.8297146303948192\nEpoch 65, loss=0.8285694486176531\nEpoch 66, loss=0.8270829126230518\nEpoch 67, loss=0.8259549849408748\nEpoch 68, loss=0.8255106735665724\nEpoch 69, loss=0.8243890617050833\nEpoch 70, loss=0.8222884907404897\nEpoch 71, loss=0.8214672144276501\nEpoch 72, loss=0.8214450580600495\nEpoch 73, loss=0.8190031211409668\nEpoch 74, loss=0.8172132484937372\nEpoch 75, loss=0.8172824858347493\nEpoch 76, loss=0.8160549168505922\nEpoch 77, loss=0.813390182910204\nEpoch 78, loss=0.8132211002099614\nEpoch 79, loss=0.8128349630854375\nEpoch 80, loss=0.8099793762038152\nEpoch 81, loss=0.8097587384882642\nEpoch 82, loss=0.8098793377770729\nEpoch 83, loss=0.8072179953590547\nEpoch 84, loss=0.8064402436574611\nEpoch 85, loss=0.8073843312783798\nEpoch 86, loss=0.8053738673764664\nEpoch 87, loss=0.8032814664646113\nEpoch 88, loss=0.8053480502544025\nEpoch 89, loss=0.8047316027441903\nEpoch 90, loss=0.8006042327790314\nEpoch 91, loss=0.8034716540831532\nEpoch 92, loss=0.80380338609389\nEpoch 93, loss=0.7992468712448653\nEpoch 94, loss=0.8027569483147667\nEpoch 95, loss=0.7992702728726327\nEpoch 96, loss=0.7987453941592596\nEpoch 97, loss=0.805013172565566\nEpoch 98, loss=0.7987942465117425\nEpoch 99, loss=0.8026415129052241\nEpoch 100, loss=0.7962915784297062\nEpoch 101, loss=0.798368086417392\nEpoch 102, loss=0.8004975031737573\nEpoch 103, loss=0.7952770143042505\nEpoch 104, loss=0.8029437006902388\nEpoch 105, loss=0.7939774231557084\nEpoch 106, loss=0.8008380578704458\nEpoch 107, loss=0.7938629222283244\nEpoch 108, loss=0.8026905442957668\nEpoch 109, loss=0.7984601996520508\nEpoch 110, loss=0.8046547235433793\nEpoch 111, loss=0.7903170858201337\nEpoch 112, loss=0.797355804952366\nEpoch 113, loss=0.7927469766721008\nEpoch 114, loss=0.7989558541034795\nEpoch 115, loss=0.7908305195719162\nEpoch 116, loss=0.7969813062108052\nEpoch 117, loss=0.7894352994529129\nEpoch 118, loss=0.7968443892753024\nEpoch 119, loss=0.7880401548521199\nEpoch 120, loss=0.7996613050844082\nEpoch 121, loss=0.788447552275081\nEpoch 122, loss=0.8007456649624853\nEpoch 123, loss=0.7852883905015561\nEpoch 124, loss=0.793285815625329\nEpoch 125, loss=0.7811987703540064\nEpoch 126, loss=0.7858212033253577\nEpoch 127, loss=0.7810607761480672\nEpoch 128, loss=0.7856930376929578\nEpoch 129, loss=0.7817984374879414\nEpoch 130, loss=0.7896010395472643\nEpoch 131, loss=0.7812967120271279\nEpoch 132, loss=0.7892707068583574\nEpoch 133, loss=0.7775158078235984\nEpoch 134, loss=0.7805481189595124\nEpoch 135, loss=0.7761900566722649\nEpoch 136, loss=0.776846675019494\nEpoch 137, loss=0.7802853535628104\nEpoch 138, loss=0.781879091080793\nEpoch 139, loss=0.7758720945413116\nEpoch 140, loss=0.7763385604138685\nEpoch 141, loss=0.7742826217468385\nEpoch 142, loss=0.7742298289013166\nEpoch 143, loss=0.7744825141388523\nEpoch 144, loss=0.7775082832202076\nEpoch 145, loss=0.7720178580115326\nEpoch 146, loss=0.771041810091567\nEpoch 147, loss=0.7712351447875687\nEpoch 148, loss=0.7721817331757799\nEpoch 149, loss=0.77066589345419\nEpoch 150, loss=0.7718919204157983\nEpoch 151, loss=0.7715189514347809\nEpoch 152, loss=0.7737959913561125\nEpoch 153, loss=0.7698756266737504\nEpoch 154, loss=0.7745903430972161\nEpoch 155, loss=0.7706835540704612\nEpoch 156, loss=0.7707564771381351\nEpoch 157, loss=0.7659366766618702\nEpoch 158, loss=0.7714000482558753\nEpoch 159, loss=0.7705576819387248\nEpoch 160, loss=0.7743395151398084\nEpoch 161, loss=0.767190362945298\nEpoch 162, loss=0.7703294226997708\nEpoch 163, loss=0.7681673053469823\nEpoch 164, loss=0.7718847291689998\nEpoch 165, loss=0.7694415173088791\nEpoch 166, loss=0.7726849991606358\nEpoch 167, loss=0.7703106663041435\nEpoch 168, loss=0.7714300343259094\nEpoch 169, loss=0.7675580006567699\nEpoch 170, loss=0.7675340456326516\nEpoch 171, loss=0.7647371749088013\nEpoch 172, loss=0.766334328771795\nEpoch 173, loss=0.7643861501326962\nEpoch 174, loss=0.7667693337813262\nEpoch 175, loss=0.7649298663320141\nEpoch 176, loss=0.7649128790773956\nEpoch 177, loss=0.7610334044103287\nEpoch 178, loss=0.7600403370947197\nEpoch 179, loss=0.7580961295732276\nEpoch 180, loss=0.7589935628169309\nEpoch 181, loss=0.7596792291950945\nEpoch 182, loss=0.7627178482668132\nEpoch 183, loss=0.7659537316884483\nEpoch 184, loss=0.7669766060339847\nEpoch 185, loss=0.7617675852979742\nEpoch 186, loss=0.7561456670321255\nEpoch 187, loss=0.7558247458559053\nEpoch 188, loss=0.7549374434487228\nEpoch 189, loss=0.756585583553084\nEpoch 190, loss=0.7583848927826297\nEpoch 191, loss=0.7611542531852872\nEpoch 192, loss=0.7652510234575002\nEpoch 193, loss=0.7698939676325116\nEpoch 194, loss=0.7714106064931967\nEpoch 195, loss=0.7655700322993159\nEpoch 196, loss=0.7549710603535129\nEpoch 197, loss=0.7577932375035451\nEpoch 198, loss=0.7580274583491625\nEpoch 199, loss=0.7617318915434113\nEpoch 200, loss=0.7546547643048767\nEpoch 201, loss=0.7575192760208721\nEpoch 202, loss=0.7696676845299972\nEpoch 203, loss=0.7623896149193511\nEpoch 204, loss=0.7541354240972035\nEpoch 205, loss=0.7710156067206025\nEpoch 206, loss=0.7813675884860938\nEpoch 207, loss=0.747772594723794\nEpoch 208, loss=0.7575423505595524\nEpoch 209, loss=0.7555919792019868\nEpoch 210, loss=0.7666872552871306\nEpoch 211, loss=0.7703649937997428\nEpoch 212, loss=0.7648689790639823\nEpoch 213, loss=0.7660074777780947\nEpoch 214, loss=0.7522933073893346\nEpoch 215, loss=0.7457418308551322\nEpoch 216, loss=0.755241554132922\nEpoch 217, loss=0.7729985211428807\nEpoch 218, loss=0.7478907578778708\nEpoch 219, loss=0.7486751389932997\nEpoch 220, loss=0.750712170081328\nEpoch 221, loss=0.7754417177393044\nEpoch 222, loss=0.7518149646976389\nEpoch 223, loss=0.7414427085410572\nEpoch 224, loss=0.7522496081220326\nEpoch 225, loss=0.7666076252057976\nEpoch 226, loss=0.7541551354519874\nEpoch 227, loss=0.7490378916324225\nEpoch 228, loss=0.7557853270008524\nEpoch 229, loss=0.7564628918746027\nEpoch 230, loss=0.7443606055193633\nEpoch 231, loss=0.7488196275006335\nEpoch 232, loss=0.7486502908107887\nEpoch 233, loss=0.7547102864346283\nEpoch 234, loss=0.7437226939901691\nEpoch 235, loss=0.7623341671180041\nEpoch 236, loss=0.7486104407333621\nEpoch 237, loss=0.7599178245702639\nEpoch 238, loss=0.7456439966477864\nEpoch 239, loss=0.743398248366854\nEpoch 240, loss=0.745776323624708\nEpoch 241, loss=0.7615092020155698\nEpoch 242, loss=0.7443512102074884\nEpoch 243, loss=0.7548801302031726\nEpoch 244, loss=0.7470870414677006\nEpoch 245, loss=0.7516537268419712\nEpoch 246, loss=0.7443073912558736\nEpoch 247, loss=0.7584172443081303\nEpoch 248, loss=0.7416206615225747\nEpoch 249, loss=0.7470833997602025\nEpoch 250, loss=0.7494371573026049\nEpoch 251, loss=0.7509496576401754\nEpoch 252, loss=0.7521606364400356\nEpoch 253, loss=0.7533736473444518\nEpoch 254, loss=0.7404921863351165\nEpoch 255, loss=0.7483679416968222\nEpoch 256, loss=0.7383496203060456\nEpoch 257, loss=0.7465799944419943\nEpoch 258, loss=0.7386609625782828\nEpoch 259, loss=0.7426944337183248\nEpoch 260, loss=0.7374513826878883\nEpoch 261, loss=0.7639080911994044\nEpoch 262, loss=0.738893431716355\nEpoch 263, loss=0.7394858757278668\nEpoch 264, loss=0.7355175947024054\nEpoch 265, loss=0.7450605558770385\nEpoch 266, loss=0.7432842207146679\nEpoch 267, loss=0.763793811145631\nEpoch 268, loss=0.7384390867575837\nEpoch 269, loss=0.7507605054493965\nEpoch 270, loss=0.7378076651821032\nEpoch 271, loss=0.7405461369360895\nEpoch 272, loss=0.7374897215219541\nEpoch 273, loss=0.7382187568233926\nEpoch 274, loss=0.7461640740961986\nEpoch 275, loss=0.7531310906624853\nEpoch 276, loss=0.736153853809852\nEpoch 277, loss=0.7328923168359425\nEpoch 278, loss=0.7354259997499651\nEpoch 279, loss=0.7403253728777413\nEpoch 280, loss=0.7403277095903421\nEpoch 281, loss=0.7454939031457439\nEpoch 282, loss=0.7390251683841309\nEpoch 283, loss=0.7499870754190976\nEpoch 284, loss=0.734647477583975\nEpoch 285, loss=0.7494810101181993\nEpoch 286, loss=0.7380706284668247\nEpoch 287, loss=0.7434677574387573\nEpoch 288, loss=0.7307736418251347\nEpoch 289, loss=0.7343565699439305\nEpoch 290, loss=0.7326200639173756\nEpoch 291, loss=0.749402285420239\nEpoch 292, loss=0.7362586060533096\nEpoch 293, loss=0.7501712952227481\nEpoch 294, loss=0.7355019267097554\nEpoch 295, loss=0.7500580810776201\nEpoch 296, loss=0.7370075236660589\nEpoch 297, loss=0.7458227027652831\nEpoch 298, loss=0.7311486072524608\nEpoch 299, loss=0.7402752032608542\nEpoch 300, loss=0.7321008124926384\nEpoch 301, loss=0.7457691102557908\nEpoch 302, loss=0.7303661726732482\nEpoch 303, loss=0.7524621621261787\nEpoch 304, loss=0.7350159046586551\nEpoch 305, loss=0.7468964149060876\nEpoch 306, loss=0.7317242296509956\nEpoch 307, loss=0.7358132457770303\nEpoch 308, loss=0.7336998614677952\nEpoch 309, loss=0.7494060457043167\nEpoch 310, loss=0.7329727358896362\nEpoch 311, loss=0.7474510935535149\nEpoch 312, loss=0.7323911886994443\nEpoch 313, loss=0.7519718531161672\nEpoch 314, loss=0.7331726335659335\nEpoch 315, loss=0.742501938345999\nEpoch 316, loss=0.7301931044470636\nEpoch 317, loss=0.7345380825610862\nEpoch 318, loss=0.7305806447366453\nEpoch 319, loss=0.7442813380758861\nEpoch 320, loss=0.7316777329120032\nEpoch 321, loss=0.7432534400637826\nEpoch 322, loss=0.7315605468855256\nEpoch 323, loss=0.7451703422281191\nEpoch 324, loss=0.7329222257853217\nEpoch 325, loss=0.7415997942431495\nEpoch 326, loss=0.7301493717364292\nEpoch 327, loss=0.7364000495556677\nEpoch 328, loss=0.7305683603642663\nEpoch 329, loss=0.7441901290173794\nEpoch 330, loss=0.7323057437904766\nEpoch 331, loss=0.738162087721667\nEpoch 332, loss=0.7307389650065864\nEpoch 333, loss=0.7412742172359771\nEpoch 334, loss=0.732925452114861\nEpoch 335, loss=0.7391812466991363\nEpoch 336, loss=0.7308826262264813\nEpoch 337, loss=0.7353260206308229\nEpoch 338, loss=0.731818351420103\nEpoch 339, loss=0.7386613373150005\nEpoch 340, loss=0.7292088660298334\nEpoch 341, loss=0.7405313075066319\nEpoch 342, loss=0.7348111848079381\nEpoch 343, loss=0.7392933184483306\nEpoch 344, loss=0.7304463748926092\nEpoch 345, loss=0.7325613919123005\nEpoch 346, loss=0.7306371712101126\nEpoch 347, loss=0.7469047419593575\nEpoch 348, loss=0.7351524179237603\nEpoch 349, loss=0.7385294479680841\nEpoch 350, loss=0.7300925992911378\nEpoch 351, loss=0.7291691612220107\nEpoch 352, loss=0.7308267153998972\nEpoch 353, loss=0.7317276101133787\nEpoch 354, loss=0.7345643572703022\nEpoch 355, loss=0.7355021738320049\nEpoch 356, loss=0.7328203203784536\nEpoch 357, loss=0.7312818164789728\nEpoch 358, loss=0.7353009993306927\nEpoch 359, loss=0.7299916321217242\nEpoch 360, loss=0.7395873154948348\nEpoch 361, loss=0.733996931973789\nEpoch 362, loss=0.7362643155570208\nEpoch 363, loss=0.732330370625413\nEpoch 364, loss=0.7331908807348814\nEpoch 365, loss=0.7294638068731014\nEpoch 366, loss=0.7289868184690909\nEpoch 367, loss=0.730360577587077\nEpoch 368, loss=0.7326119817805884\nEpoch 369, loss=0.7329450641881212\nEpoch 370, loss=0.7283577088367837\nEpoch 371, loss=0.740819306309206\nEpoch 372, loss=0.7341292198100832\nEpoch 373, loss=0.734004426683776\nEpoch 374, loss=0.727289140432379\nEpoch 375, loss=0.7312896836462022\nEpoch 376, loss=0.7361223450544895\nEpoch 377, loss=0.7431902674039653\nEpoch 378, loss=0.7279978388776569\nEpoch 379, loss=0.7382463742839755\nEpoch 380, loss=0.7283415845368807\nEpoch 381, loss=0.7329604506282994\nEpoch 382, loss=0.7253206485892795\nEpoch 383, loss=0.7282727278318659\nEpoch 384, loss=0.7285954115426619\nEpoch 385, loss=0.7349346966412598\nEpoch 386, loss=0.7276979213082477\nEpoch 387, loss=0.7274346849922586\nEpoch 388, loss=0.7231117135344816\nEpoch 389, loss=0.7274621699032943\nEpoch 390, loss=0.7233777859471048\nEpoch 391, loss=0.7325536821989258\nEpoch 392, loss=0.7365484399314023\nEpoch 393, loss=0.7350007150999126\nEpoch 394, loss=0.7256505464294823\nEpoch 395, loss=0.7223145947659741\nEpoch 396, loss=0.7192181157678958\nEpoch 397, loss=0.7533929198834726\nEpoch 398, loss=0.7358803857172406\nEpoch 399, loss=0.7308081396983\nEpoch 400, loss=0.7297596366281232\nEpoch 401, loss=0.7329736455938515\nEpoch 402, loss=0.7276167529948604\nEpoch 403, loss=0.7216745856613196\nEpoch 404, loss=0.7165315872506604\nEpoch 405, loss=0.7226136181605902\nEpoch 406, loss=0.7229581938829601\nEpoch 407, loss=0.7330520104802651\nEpoch 408, loss=0.7333254865654565\nEpoch 409, loss=0.7390692202825359\nEpoch 410, loss=0.727705241376902\nEpoch 411, loss=0.7202178060437376\nEpoch 412, loss=0.7150856088298185\nEpoch 413, loss=0.7163101609743991\nEpoch 414, loss=0.7180249275418636\nEpoch 415, loss=0.7290412400216396\nEpoch 416, loss=0.7235724393305505\nEpoch 417, loss=0.7262916189545067\nEpoch 418, loss=0.7223332634486389\nEpoch 419, loss=0.7309711429112774\nEpoch 420, loss=0.7281987676195494\nEpoch 421, loss=0.7183406181115757\nEpoch 422, loss=0.7162757169644917\nEpoch 423, loss=0.7139249968847355\nEpoch 424, loss=0.7281233649314433\nEpoch 425, loss=0.7242623343023757\nEpoch 426, loss=0.7278541650444909\nEpoch 427, loss=0.7210416370629807\nEpoch 428, loss=0.7148160827982506\nEpoch 429, loss=0.7253811601865917\nEpoch 430, loss=0.7279633859405319\nEpoch 431, loss=0.7217694318352502\nEpoch 432, loss=0.717152314044013\nEpoch 433, loss=0.7194619750476937\nEpoch 434, loss=0.7199336418174713\nEpoch 435, loss=0.7851154571197272\nEpoch 436, loss=0.7323255487612413\nEpoch 437, loss=0.7245179439348985\nEpoch 438, loss=0.7183191475308188\nEpoch 439, loss=0.70853002514457\nEpoch 440, loss=0.7115748373412821\nEpoch 441, loss=0.727064289731213\nEpoch 442, loss=0.7199814891090501\nEpoch 443, loss=0.7197486962060322\nEpoch 444, loss=0.7217787083222058\nEpoch 445, loss=0.7188982970986177\nEpoch 446, loss=0.7125612414459812\nEpoch 447, loss=0.7120285248823282\nEpoch 448, loss=0.7070081748142868\nEpoch 449, loss=0.7119942796854454\nEpoch 450, loss=0.7119752669667768\nEpoch 451, loss=0.7326763268254843\nEpoch 452, loss=0.7441411631028763\nEpoch 453, loss=0.7277446887719132\nEpoch 454, loss=0.7139073224692737\nEpoch 455, loss=0.7086916783788973\nEpoch 456, loss=0.7138127115913487\nEpoch 457, loss=0.7048944909287954\nEpoch 458, loss=0.7124051735640404\nEpoch 459, loss=0.7125669491293549\nEpoch 460, loss=0.7246733214420698\nEpoch 461, loss=0.7240588285403878\nEpoch 462, loss=0.7279268747835427\nEpoch 463, loss=0.7124692801929381\nEpoch 464, loss=0.6998027718653176\nEpoch 465, loss=0.7054409287972085\nEpoch 466, loss=0.7077039160424714\nEpoch 467, loss=0.7335124802427393\nEpoch 468, loss=0.7189039159691071\nEpoch 469, loss=0.726911833937029\nEpoch 470, loss=0.7104520195301367\nEpoch 471, loss=0.704471395385194\nEpoch 472, loss=0.7100113946281438\nEpoch 473, loss=0.7136016813680104\nEpoch 474, loss=0.7067145505165432\nEpoch 475, loss=0.7180240859153798\nEpoch 476, loss=0.7259031982393315\nEpoch 477, loss=0.7261858469785996\nEpoch 478, loss=0.7301406555600026\nEpoch 479, loss=0.7232212464503852\nEpoch 480, loss=0.7102130763414563\nEpoch 481, loss=0.7090861723834656\nEpoch 482, loss=0.7123298701982989\nEpoch 483, loss=0.7051670629756531\nEpoch 484, loss=0.7156914089800582\nEpoch 485, loss=0.7206751576752372\nEpoch 486, loss=0.7209843212305107\nEpoch 487, loss=0.7101366824541898\nEpoch 488, loss=0.7234842182696306\nEpoch 489, loss=0.7163421202983508\nEpoch 490, loss=0.7327877561150296\nEpoch 491, loss=0.7134082973110898\nEpoch 492, loss=0.705847744619107\nEpoch 493, loss=0.7118040152065398\nEpoch 494, loss=0.7089134206586082\nEpoch 495, loss=0.7057198340273034\nEpoch 496, loss=0.7214850323797333\nEpoch 497, loss=0.7079703357344114\nEpoch 498, loss=0.7172567385995958\nEpoch 499, loss=0.7278818977705201\nEpoch 500, loss=0.7390246547083735\nEpoch 501, loss=0.7169871917503\nEpoch 502, loss=0.7102498881674837\nEpoch 503, loss=0.7115456079864556\nEpoch 504, loss=0.7080795027575675\nEpoch 505, loss=0.7064836013246437\nEpoch 506, loss=0.7047022567998629\nEpoch 507, loss=0.7109657911913783\nEpoch 508, loss=0.7084029714329106\nEpoch 509, loss=0.7155430080748822\nEpoch 510, loss=0.7181885629683193\nEpoch 511, loss=0.7173051822867754\nEpoch 512, loss=0.7128042726494964\nEpoch 513, loss=0.7106765106981698\nEpoch 514, loss=0.700031104124231\nEpoch 515, loss=0.7278038869202073\nEpoch 516, loss=0.7333776631175761\nEpoch 517, loss=0.7227528805577048\nEpoch 518, loss=0.712150458495716\nEpoch 519, loss=0.7106301013481752\nEpoch 520, loss=0.7076672842449645\nEpoch 521, loss=0.7076992803495143\nEpoch 522, loss=0.7099929646351183\nEpoch 523, loss=0.7058101859682052\nEpoch 524, loss=0.710501432082801\nEpoch 525, loss=0.7268122334859163\nEpoch 526, loss=0.726452138600392\nEpoch 527, loss=0.7023994377279333\nEpoch 528, loss=0.702738234342242\nEpoch 529, loss=0.7095702676943569\nEpoch 530, loss=0.7337382864866571\nEpoch 531, loss=0.7406640947838261\nEpoch 532, loss=0.7153806832425988\nEpoch 533, loss=0.7099749101731052\nEpoch 534, loss=0.7077981924966872\nEpoch 535, loss=0.7036909878147309\nEpoch 536, loss=0.7009453822206031\nEpoch 537, loss=0.7047084285062463\nEpoch 538, loss=0.7047801991595435\nEpoch 539, loss=0.7140724756314583\nEpoch 540, loss=0.719874314094516\nEpoch 541, loss=0.7248033574703432\nEpoch 542, loss=0.7375529536349101\nEpoch 543, loss=0.7115774285055577\nEpoch 544, loss=0.7015686967323082\nEpoch 545, loss=0.7038329252199632\nEpoch 546, loss=0.7056047144307669\nEpoch 547, loss=0.7106557215248066\nEpoch 548, loss=0.7188709066665244\nEpoch 549, loss=0.7101147790937523\nEpoch 550, loss=0.7114497698338828\nEpoch 551, loss=0.7239806775018062\nEpoch 552, loss=0.7056904315352048\nEpoch 553, loss=0.7106325183339386\nEpoch 554, loss=0.7170254863949768\nEpoch 555, loss=0.7216174335446175\nEpoch 556, loss=0.7101487896943197\nEpoch 557, loss=0.7061833853285427\nEpoch 558, loss=0.7179885848312074\nEpoch 559, loss=0.7244183711926894\nEpoch 560, loss=0.7213814015055228\nEpoch 561, loss=0.7099035956190903\nEpoch 562, loss=0.7060106567449597\nEpoch 563, loss=0.7063145623388117\nEpoch 564, loss=0.704685108988796\nEpoch 565, loss=0.7123726651620855\nEpoch 566, loss=0.7085919767981244\nEpoch 567, loss=0.7077653001066504\nEpoch 568, loss=0.7123483644544837\nEpoch 569, loss=0.726031298695852\nEpoch 570, loss=0.7377850573082566\nEpoch 571, loss=0.7271374347262414\nEpoch 572, loss=0.7263673811538829\nEpoch 573, loss=0.7136435176765676\nEpoch 574, loss=0.7066216470324714\nEpoch 575, loss=0.7097639061282768\nEpoch 576, loss=0.710393656947575\nEpoch 577, loss=0.7025243905253118\nEpoch 578, loss=0.7062222659682922\nEpoch 579, loss=0.7139651929129085\nEpoch 580, loss=0.7124272281622038\nEpoch 581, loss=0.7102126041545551\nEpoch 582, loss=0.7169665876158906\nEpoch 583, loss=0.7092616816407822\nEpoch 584, loss=0.7122190515558121\nEpoch 585, loss=0.7226480253368697\nEpoch 586, loss=0.7159676815722462\nEpoch 587, loss=0.7216104358809778\nEpoch 588, loss=0.7127501654759306\nEpoch 589, loss=0.7071306562614881\nEpoch 590, loss=0.7106447829633545\nEpoch 591, loss=0.7132512057065712\nEpoch 592, loss=0.7088524838642368\nEpoch 593, loss=0.7115278482879495\nEpoch 594, loss=0.7137742969026447\nEpoch 595, loss=0.708440035208701\nEpoch 596, loss=0.7134778742389791\nEpoch 597, loss=0.7088742523964728\nEpoch 598, loss=0.7092884887587606\nEpoch 599, loss=0.7183131622004342\nEpoch 600, loss=0.7111203493554822\nEpoch 601, loss=0.7062913742561374\nEpoch 602, loss=0.7143482181820378\nEpoch 603, loss=0.706950150867523\nEpoch 604, loss=0.7076050039564653\nEpoch 605, loss=0.7239249951453781\nEpoch 606, loss=0.726622080439475\nEpoch 607, loss=0.7102268807005641\nEpoch 608, loss=0.7048408144801791\nEpoch 609, loss=0.6968824616343948\nEpoch 610, loss=0.6978506570690781\nEpoch 611, loss=0.7121740914205732\nEpoch 612, loss=0.7114097149159075\nEpoch 613, loss=0.7073472475092608\nEpoch 614, loss=0.704845248408551\nEpoch 615, loss=0.702013301065212\nEpoch 616, loss=0.7096857989226385\nEpoch 617, loss=0.7171032579107958\nEpoch 618, loss=0.7233760854350597\nEpoch 619, loss=0.7114918383722264\nEpoch 620, loss=0.7142462572938149\nEpoch 621, loss=0.7083245197448681\nEpoch 622, loss=0.7011941044235694\nEpoch 623, loss=0.7039045733773507\nEpoch 624, loss=0.7223482565451464\nEpoch 625, loss=0.7153658001072718\nEpoch 626, loss=0.7183703933715192\nEpoch 627, loss=0.7150100866095263\nEpoch 628, loss=0.7159572039659151\nEpoch 629, loss=0.7193000527311436\nEpoch 630, loss=0.7039340364492379\nEpoch 631, loss=0.6944169312755917\nEpoch 632, loss=0.6995957150208091\nEpoch 633, loss=0.7141749479075276\nEpoch 634, loss=0.7068199835411622\nEpoch 635, loss=0.7082921223491578\nEpoch 636, loss=0.700672697637017\nEpoch 637, loss=0.7095379073596041\nEpoch 638, loss=0.7106007465135842\nEpoch 639, loss=0.7133244383466855\nEpoch 640, loss=0.7061284910712219\nEpoch 641, loss=0.7161766936490711\nEpoch 642, loss=0.6969212861060275\nEpoch 643, loss=0.7052905773573707\nEpoch 644, loss=0.6982610764564496\nEpoch 645, loss=0.7112204827021523\nEpoch 646, loss=0.7186969175655431\nEpoch 647, loss=0.7115120546624168\nEpoch 648, loss=0.7114566081832259\nEpoch 649, loss=0.7107448515342493\nEpoch 650, loss=0.7003076876581729\nEpoch 651, loss=0.6993967440398057\nEpoch 652, loss=0.6983675717666344\nEpoch 653, loss=0.7014575944759751\nEpoch 654, loss=0.6997170545908685\nEpoch 655, loss=0.6958744511089366\nEpoch 656, loss=0.7125207540414407\nEpoch 657, loss=0.7034756331535954\nEpoch 658, loss=0.7166078544243378\nEpoch 659, loss=0.7069436257884057\nEpoch 660, loss=0.7004345561031576\nEpoch 661, loss=0.6967312490827816\nEpoch 662, loss=0.6912318243570322\nEpoch 663, loss=0.6978018814753898\nEpoch 664, loss=0.692675841566001\nEpoch 665, loss=0.699015408055023\nEpoch 666, loss=0.7180007632051543\nEpoch 667, loss=0.7290705320099159\nEpoch 668, loss=0.7052440057007776\nEpoch 669, loss=0.7100189314154932\nEpoch 670, loss=0.6995536612175346\nEpoch 671, loss=0.695934705622988\nEpoch 672, loss=0.6988738819051135\nEpoch 673, loss=0.701246092693912\nEpoch 674, loss=0.6991413720799264\nEpoch 675, loss=0.7090838433126622\nEpoch 676, loss=0.6975455465389937\nEpoch 677, loss=0.708701736672939\nEpoch 678, loss=0.6944445878633888\nEpoch 679, loss=0.6944228498075464\nEpoch 680, loss=0.6972858176762744\nEpoch 681, loss=0.6968500093665777\nEpoch 682, loss=0.7105970854945406\nEpoch 683, loss=0.7138344233844279\nEpoch 684, loss=0.6913792074084323\nEpoch 685, loss=0.6991680755553068\nEpoch 686, loss=0.6942928414136756\nEpoch 687, loss=0.6976800225525703\nEpoch 688, loss=0.6903668844295859\nEpoch 689, loss=0.7164912942623203\nEpoch 690, loss=0.7029154070190825\nEpoch 691, loss=0.6942215147731604\nEpoch 692, loss=0.7019686305186867\nEpoch 693, loss=0.6989090233438064\nEpoch 694, loss=0.7192967390889428\nEpoch 695, loss=0.7072275270540588\nEpoch 696, loss=0.7001875006230357\nEpoch 697, loss=0.7134312941701677\nEpoch 698, loss=0.6961600131362345\nEpoch 699, loss=0.6903821842899381\nEpoch 700, loss=0.6888531498785592\nEpoch 701, loss=0.7072208512516089\nEpoch 702, loss=0.6999894437470501\nEpoch 703, loss=0.6970046715968722\nEpoch 704, loss=0.6907480975360692\nEpoch 705, loss=0.696190844859715\nEpoch 706, loss=0.693835794061254\nEpoch 707, loss=0.6958192655108005\nEpoch 708, loss=0.7053904578423282\nEpoch 709, loss=0.6927425071264228\nEpoch 710, loss=0.6953608894621054\nEpoch 711, loss=0.6986896376352368\nEpoch 712, loss=0.7413338792025144\nEpoch 713, loss=0.6969030149338705\nEpoch 714, loss=0.7189130483686088\nEpoch 715, loss=0.7193411399741146\nEpoch 716, loss=0.7108041950146561\nEpoch 717, loss=0.6939184749933789\nEpoch 718, loss=0.6920003326282661\nEpoch 719, loss=0.6964209120184469\nEpoch 720, loss=0.6864478416453943\nEpoch 721, loss=0.6918200370862376\nEpoch 722, loss=0.6912768610716274\nEpoch 723, loss=0.691050533901477\nEpoch 724, loss=0.6883543957083555\nEpoch 725, loss=0.7207401890450784\nEpoch 726, loss=0.7096673047120712\nEpoch 727, loss=0.7012602742427076\nEpoch 728, loss=0.7122955537152419\nEpoch 729, loss=0.6994147791386631\nEpoch 730, loss=0.71285615179582\nEpoch 731, loss=0.6941217611112175\nEpoch 732, loss=0.6959852945928446\nEpoch 733, loss=0.6882551918941492\nEpoch 734, loss=0.7021585370511348\nEpoch 735, loss=0.6878116638066744\nEpoch 736, loss=0.6962255977090355\nEpoch 737, loss=0.6994533965187473\nEpoch 738, loss=0.6924990149767536\nEpoch 739, loss=0.712419301623696\nEpoch 740, loss=0.6958561728840468\nEpoch 741, loss=0.7038047228841607\nEpoch 742, loss=0.7011853964565851\nEpoch 743, loss=0.6995919123229305\nEpoch 744, loss=0.6916898092491045\nEpoch 745, loss=0.7294956148304907\nEpoch 746, loss=0.6969729982555105\nEpoch 747, loss=0.6933291806114139\nEpoch 748, loss=0.6840141878442154\nEpoch 749, loss=0.7030607811167195\nEpoch 750, loss=0.6924695758802558\nEpoch 751, loss=0.6960960142478362\nEpoch 752, loss=0.6878548335792346\nEpoch 753, loss=0.6835782872742813\nEpoch 754, loss=0.6933843272263185\nEpoch 755, loss=0.7135891167940231\nEpoch 756, loss=0.7154401931766118\nEpoch 757, loss=0.7172265775198448\nEpoch 758, loss=0.7115513062177362\nEpoch 759, loss=0.6898248252018966\nEpoch 760, loss=0.687806596325503\nEpoch 761, loss=0.6836155066396861\nEpoch 762, loss=0.7151180159684406\nEpoch 763, loss=0.6957718540747727\nEpoch 764, loss=0.7089346727920728\nEpoch 765, loss=0.6919708872110866\nEpoch 766, loss=0.6987249940726874\nEpoch 767, loss=0.68674259437336\nEpoch 768, loss=0.6862663166805315\nEpoch 769, loss=0.6869458982540029\nEpoch 770, loss=0.6904767240790595\nEpoch 771, loss=0.7025115988884214\nEpoch 772, loss=0.6954420032794321\nEpoch 773, loss=0.7170251275994358\nEpoch 774, loss=0.7172561318219489\nEpoch 775, loss=0.7043419156407309\nEpoch 776, loss=0.7353159990889445\nEpoch 777, loss=0.6966473472424711\nEpoch 778, loss=0.6883903127827349\nEpoch 779, loss=0.6874134157720906\nEpoch 780, loss=0.686122785331776\nEpoch 781, loss=0.6986271668056303\nEpoch 782, loss=0.7046457348420745\nEpoch 783, loss=0.7198101648617743\nEpoch 784, loss=0.6928751913062806\nEpoch 785, loss=0.6922091849866168\nEpoch 786, loss=0.7081751729401562\nEpoch 787, loss=0.695443441577176\nEpoch 788, loss=0.7172118254976984\nEpoch 789, loss=0.6988095703753696\nEpoch 790, loss=0.7066607027517653\nEpoch 791, loss=0.6909090939435025\nEpoch 792, loss=0.6965992476172641\nEpoch 793, loss=0.6931662200054766\nEpoch 794, loss=0.7033499558365701\nEpoch 795, loss=0.68733861303293\nEpoch 796, loss=0.6816027934994099\nEpoch 797, loss=0.685740932920489\nEpoch 798, loss=0.685712408928785\nEpoch 799, loss=0.701442070955205\nEpoch 800, loss=0.7007715473735445\nEpoch 801, loss=0.724931885268746\nEpoch 802, loss=0.7032111031998012\nEpoch 803, loss=0.7002171016094708\nEpoch 804, loss=0.7130132576906089\nEpoch 805, loss=0.696907678759619\nEpoch 806, loss=0.6974778414446214\nEpoch 807, loss=0.6795297172937369\nEpoch 808, loss=0.6886846268360012\nEpoch 809, loss=0.6892229245804692\nEpoch 810, loss=0.6944970914947937\nEpoch 811, loss=0.6843037942070983\nEpoch 812, loss=0.7036753428604345\nEpoch 813, loss=0.6972050799426701\nEpoch 814, loss=0.7047123515903425\nEpoch 815, loss=0.6969230770265475\nEpoch 816, loss=0.6942846445410862\nEpoch 817, loss=0.6884973671037581\nEpoch 818, loss=0.7096512649440444\nEpoch 819, loss=0.6918526745704178\nEpoch 820, loss=0.6918645303241234\nEpoch 821, loss=0.68542571646786\nEpoch 822, loss=0.6978390744020455\nEpoch 823, loss=0.695030807347613\nEpoch 824, loss=0.6996099903918799\nEpoch 825, loss=0.6886511544021927\nEpoch 826, loss=0.6971292749192669\nEpoch 827, loss=0.6942535866540845\nEpoch 828, loss=0.6997442358553961\nEpoch 829, loss=0.689541434344714\nEpoch 830, loss=0.7197360642023893\nEpoch 831, loss=0.7009349839710903\nEpoch 832, loss=0.6976304783126828\nEpoch 833, loss=0.692655506231165\nEpoch 834, loss=0.7011832214428029\nEpoch 835, loss=0.6983380010769901\nEpoch 836, loss=0.684885929535467\nEpoch 837, loss=0.6882214717279814\nEpoch 838, loss=0.6909059798614822\nEpoch 839, loss=0.69562187270403\nEpoch 840, loss=0.719597585143665\nEpoch 841, loss=0.6976028535895051\nEpoch 842, loss=0.6862167982364032\nEpoch 843, loss=0.6834188878969947\nEpoch 844, loss=0.6976805920111167\nEpoch 845, loss=0.6966469803704289\nEpoch 846, loss=0.69929896085541\nEpoch 847, loss=0.6985419850221067\nEpoch 848, loss=0.7081868566959684\nEpoch 849, loss=0.6936131163719627\nEpoch 850, loss=0.6885611345902738\nEpoch 851, loss=0.6868665295811457\nEpoch 852, loss=0.6859821070943224\nEpoch 853, loss=0.6922185173345862\nEpoch 854, loss=0.6949055118557341\nEpoch 855, loss=0.7061526264711113\nEpoch 856, loss=0.7168320288736949\nEpoch 857, loss=0.6989959984986248\nEpoch 858, loss=0.711207942149272\nEpoch 859, loss=0.689634239369623\nEpoch 860, loss=0.6889227718735921\nEpoch 861, loss=0.6935750723658768\nEpoch 862, loss=0.7044995370868263\nEpoch 863, loss=0.6901843372193894\nEpoch 864, loss=0.6875382792590621\nEpoch 865, loss=0.6805122657515126\nEpoch 866, loss=0.6876452773798685\nEpoch 867, loss=0.6820847484033082\nEpoch 868, loss=0.6865905154816906\nEpoch 869, loss=0.6903700183606948\nEpoch 870, loss=0.7263365732843101\nEpoch 871, loss=0.7013434429531898\nEpoch 872, loss=0.7011815773562995\nEpoch 873, loss=0.6828241422896144\nEpoch 874, loss=0.6820252967148652\nEpoch 875, loss=0.6740289538452355\nEpoch 876, loss=0.6834976758872146\nEpoch 877, loss=0.6801719909251356\nEpoch 878, loss=0.6970949419878608\nEpoch 879, loss=0.7012310690987459\nEpoch 880, loss=0.7102260009297185\nEpoch 881, loss=0.6915683150467472\nEpoch 882, loss=0.69863660704563\nEpoch 883, loss=0.705105738515425\nEpoch 884, loss=0.6853430158969854\nEpoch 885, loss=0.6883840097711231\nEpoch 886, loss=0.6776299816316484\nEpoch 887, loss=0.7011887971709112\nEpoch 888, loss=0.6915032357286456\nEpoch 889, loss=0.7007442094658669\nEpoch 890, loss=0.7046132985499307\nEpoch 891, loss=0.6889599900865073\nEpoch 892, loss=0.6839723559935891\nEpoch 893, loss=0.6776783273718775\nEpoch 894, loss=0.6919023591517492\nEpoch 895, loss=0.7063288637621797\nEpoch 896, loss=0.7260391777532508\nEpoch 897, loss=0.6878051566490275\nEpoch 898, loss=0.6876398343807332\nEpoch 899, loss=0.6760698708259274\nEpoch 900, loss=0.6847460541134723\nEpoch 901, loss=0.6729537776244054\nEpoch 902, loss=0.67989149362375\nEpoch 903, loss=0.6756886456353209\nEpoch 904, loss=0.6877914727036253\nEpoch 905, loss=0.6792435347368717\nEpoch 906, loss=0.6900851570659892\nEpoch 907, loss=0.7129392070342514\nEpoch 908, loss=0.7304402445955783\nEpoch 909, loss=0.6767801293862986\nEpoch 910, loss=0.6795187270860162\nEpoch 911, loss=0.6749429351517819\nEpoch 912, loss=0.6954435055132495\nEpoch 913, loss=0.6780649784896651\nEpoch 914, loss=0.677732670454047\nEpoch 915, loss=0.7010396415485217\nEpoch 916, loss=0.7149769931418247\nEpoch 917, loss=0.6831016363501125\nEpoch 918, loss=0.6764226398067749\nEpoch 919, loss=0.691033382589352\nEpoch 920, loss=0.6936089898744819\nEpoch 921, loss=0.7212674646145772\nEpoch 922, loss=0.6945317490521895\nEpoch 923, loss=0.6763729666465473\nEpoch 924, loss=0.6745515324816685\nEpoch 925, loss=0.6885735368101031\nEpoch 926, loss=0.6912259240117864\nEpoch 927, loss=0.6875644495288812\nEpoch 928, loss=0.6976350325145808\nEpoch 929, loss=0.6983897706995931\nEpoch 930, loss=0.6894518705065238\nEpoch 931, loss=0.6847308837817561\nEpoch 932, loss=0.6767171252116018\nEpoch 933, loss=0.6956964276991194\nEpoch 934, loss=0.7037762740655342\nEpoch 935, loss=0.7117777955191267\nEpoch 936, loss=0.6887597115693009\nEpoch 937, loss=0.687909389189271\nEpoch 938, loss=0.6773193807144403\nEpoch 939, loss=0.6925153528200698\nEpoch 940, loss=0.6849193989488791\nEpoch 941, loss=0.6933323786726083\nEpoch 942, loss=0.6954946550352638\nEpoch 943, loss=0.696567227454555\nEpoch 944, loss=0.6790704678236658\nEpoch 945, loss=0.6915741976331526\nEpoch 946, loss=0.6802769952730118\nEpoch 947, loss=0.679303119241502\nEpoch 948, loss=0.6774777066503875\nEpoch 949, loss=0.6824791920639685\nEpoch 950, loss=0.6768825771066264\nEpoch 951, loss=0.6918214736037307\nEpoch 952, loss=0.7009757655822564\nEpoch 953, loss=0.6932855609396549\nEpoch 954, loss=0.6904399024390176\nEpoch 955, loss=0.678321943273814\nEpoch 956, loss=0.6899896580454996\nEpoch 957, loss=0.692627993796998\nEpoch 958, loss=0.7097271495525382\nEpoch 959, loss=0.6945306138440719\nEpoch 960, loss=0.6797222131438823\nEpoch 961, loss=0.6793859598827056\nEpoch 962, loss=0.702685651362915\nEpoch 963, loss=0.6886918071731839\nEpoch 964, loss=0.6803744573270806\nEpoch 965, loss=0.6801388305962012\nEpoch 966, loss=0.6952658211310002\nEpoch 967, loss=0.7013550501634173\nEpoch 968, loss=0.7008233611790647\nEpoch 969, loss=0.6786882377952755\nEpoch 970, loss=0.6783090214272969\nEpoch 971, loss=0.6749753838638995\nEpoch 972, loss=0.682110597537111\nEpoch 973, loss=0.71407988294092\nEpoch 974, loss=0.716760403026961\nEpoch 975, loss=0.7042203074611122\nEpoch 976, loss=0.6828461349795979\nEpoch 977, loss=0.6760525966096156\nEpoch 978, loss=0.6711539723490102\nEpoch 979, loss=0.6877530656265899\nEpoch 980, loss=0.684158209651027\nEpoch 981, loss=0.7005726688868572\nEpoch 982, loss=0.6905587937059692\nEpoch 983, loss=0.6981227475190028\nEpoch 984, loss=0.6811869449090068\nEpoch 985, loss=0.6936912513777935\nEpoch 986, loss=0.677940108544961\nEpoch 987, loss=0.6864983470426967\nEpoch 988, loss=0.675839846717333\nEpoch 989, loss=0.682070719265064\nEpoch 990, loss=0.6780678286935841\nEpoch 991, loss=0.6860133119513803\nEpoch 992, loss=0.6830500605939107\nEpoch 993, loss=0.7001120314654525\nEpoch 994, loss=0.6774721486304899\nEpoch 995, loss=0.6896531557623207\nEpoch 996, loss=0.6791935034552897\nEpoch 997, loss=0.695109327281426\nEpoch 998, loss=0.6838321037571008\nEpoch 999, loss=0.6767765467849405\n"
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    total_loss = 0\n",
    "    for patient in ALL_PATIENTS:\n",
    "        patient_loss = 0\n",
    "        model.zero_grad()\n",
    "\n",
    "        patient_data = train_df[train_df[\"Patient\"] == patient][['Weeks', 'FVC', 'Percent', 'Age']].values\n",
    "        patient_data = torch.tensor(patient_data).float()\n",
    "\n",
    "        loss = 0\n",
    "        for i in range(patient_data.size()[0] - 1):\n",
    "            out = model(patient_data[i].view(1,1,-1))\n",
    "            loss += loss_function(out.view(4), patient_data[i+1])\n",
    "            patient_loss += loss.item()\n",
    "        # for data in patient_data:\n",
    "        #     out = model(data.view(1, 1, -1))\n",
    "        #     print(out)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += patient_loss/patient_data.shape[0]\n",
    "    print(f\"Epoch {epoch}, loss={total_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[-0.0507,  0.3936,  0.3309,  0.6154]])"
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "sample = torch.tensor([[-12, 3020,   70,   73]])\n",
    "sample = torch.tensor(min_max_scaler.transform(sample)).float()\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[  12.72775212 3351.45580626   79.05865674   72.65787369]]\n[[  17.55453753 3323.5759058    78.1780903    72.70940733]]\n[[  23.72204721 3317.9380523    77.80429176   72.75171006]]\n[[  31.15028667 3333.3524363    77.96112257   72.79025859]]\n[[  40.49702799 3357.50930429   78.53850451   72.77853805]]\n"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for i in range(15):\n",
    "        out = model(sample.view(1,1,-1))\n",
    "        print(min_max_scaler.inverse_transform(out.squeeze(dim=0)))\n",
    "        sample = out.squeeze(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(sample)\n",
    "    out = model(sample.view(1, 1, -1))\n",
    "    print(out)\n",
    "    sample = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    input = torch.randn(2, 5, 4)\n",
    "    print(input)\n",
    "    print()\n",
    "    out = model(input)\n",
    "    print(out)\n",
    "    print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}